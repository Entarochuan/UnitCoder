{"id": "ef00f77b-4456-387f-b9d5-23c46476a811_0", "content": "def load_data_fer(height, width):\n    imagesDir = 'C:\\\\Users\\\\david\\\\Desktop\\\\FER\\\\train'\n    train = tf.keras.preprocessing.image_dataset_from_directory(imagesDir,\n        labels='inferred', validation_split=0.2, subset='training',\n        color_mode='grayscale', seed=123, label_mode='categorical',\n        image_size=(height, width))\n    test = tf.keras.preprocessing.image_dataset_from_directory(imagesDir,\n        validation_split=0.2, subset='validation', color_mode='grayscale',\n        seed=123, label_mode='categorical', image_size=(height, width))\n    return train, test\n", "import_code": ["import tensorflow as tf\n", "from tensorflow import keras\n"]}
{"id": "96008757-d6c7-3259-8bf8-354fa1291893_0", "content": "def insert_row(row_num, df, row_value):\n    \"\"\"\n    This function inserts a row into a DataFrame at a specific position.\n    \"\"\"\n    df1 = df[0:row_num].copy()\n    df2 = df[row_num:].copy()\n    df1.loc[row_num] = row_value\n    df = pd.concat([df1, df2])\n    df.index = [*range(df.shape[0])]\n    return df\n", "import_code": ["import pandas as pd\n"]}
{"id": "e5bbbacc-eb58-3e88-964b-1a82b05055a6_1", "content": "def make_char_alphabet(text):\n    \"\"\"\n    Create a dictionary mapping characters to integers.\n    The characters are sorted and then mapped to their index in the sorted list.\n    This creates a unique integer representation for each character.\n    \"\"\"\n    chars = sorted(list(set(text)))\n    return dict((char, chars.index(char)) for char in chars)\n", "import_code": []}
{"id": "e5bbbacc-eb58-3e88-964b-1a82b05055a6_2", "content": "def make_training_data(text, char2int):\n    \"\"\"\n    Create training data for the model.\n    The text is split into sequences of characters with a specified sequence length and step size.\n    The sequences are vectorized to create input (x) and output (y) for the model.\n    \"\"\"\n    seqlen = cfg.getint('args', 'seqlen')\n    sequences = []\n    targets = []\n    step = cfg.getint('args', 'step')\n    for i in range(0, len(text) - seqlen, step):\n        sequences.append(text[i:i + seqlen])\n        targets.append(text[i + seqlen])\n    items = len(sequences) * seqlen * len(char2int)\n    item_size_in_bytes = np.dtype(np.bool).itemsize\n    print('train tensor shape:', (len(sequences), seqlen, len(char2int)))\n    print('allocating:', hurry.filesize.size(items * item_size_in_bytes))\n    x = np.zeros((len(sequences), seqlen, len(char2int)), dtype=np.bool)\n    print('train tensor size in bytes:', hurry.filesize.size(x.nbytes))\n    y = np.zeros((len(sequences), len(char2int)), dtype=np.bool)\n    for n, sequence in enumerate(sequences):\n        for time_step, char in enumerate(sequence):\n            x[n, time_step, char2int[char]] = 1\n        y[n, char2int[targets[n]]] = 1\n    return x, y\n", "import_code": ["import numpy as np\n"]}
{"id": "e5bbbacc-eb58-3e88-964b-1a82b05055a6_3", "content": "def get_model(num_features):\n    \"\"\"\n    Define the LSTM model.\n    The model takes a sequence of characters as input and outputs a probability distribution over the\n    possible characters. The model is compiled with categorical cross-entropy loss and RMSprop optimizer.\n    \"\"\"\n    model = keras.models.Sequential()\n    model.add(layers.LSTM(128, input_shape=(None, num_features)))\n    model.add(layers.Dense(num_features, activation='softmax'))\n    optimizer = keras.optimizers.RMSprop(lr=0.01)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n    return model\n", "import_code": ["from keras import backend as bke\n", "import keras, random, sys, configparser\n", "from keras import layers\n", "from keras import layers\n"]}
{"id": "e5bbbacc-eb58-3e88-964b-1a82b05055a6_4", "content": "def sample_char(preds, temperature=1.0):\n    \"\"\"\n    Sample a character from the predicted probability distribution.\n    The distribution is first reweighted according to the temperature parameter, then a character is\n    sampled from the reweighted distribution.\n    \"\"\"\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n", "import_code": ["import numpy as np\n"]}
{"id": "e5bbbacc-eb58-3e88-964b-1a82b05055a6_5", "content": "def pick_init_seq(text):\n    \"\"\"\n    Pick a random substring from the corpus as the initial sequence.\n    \"\"\"\n    seqlen = cfg.getint('args', 'seqlen')\n    seed_start = random.randint(0, len(text) - seqlen - 1)\n    seed = text[seed_start:seed_start + seqlen]\n    return seed\n", "import_code": ["import random as rn\n", "import keras, random, sys, configparser\n"]}
{"id": "e5bbbacc-eb58-3e88-964b-1a82b05055a6_6", "content": "def sample_init_seq(model, init_char, chars, char2int, temp=0.01):\n    \"\"\"\n    Sample an initial sequence of fixed length given a character.\n    \"\"\"\n    seqlen = cfg.getint('args', 'seqlen')\n    text = init_char\n    for i in range(seqlen - len(text)):\n        vectorized = np.zeros((1, len(text), len(chars)))\n        for t, char in enumerate(text):\n            vectorized[0, t, char2int[char]] = 1.0\n        preds = model.predict(vectorized)[0]\n        next_index = sample_char(preds, temp)\n        next_char = chars[next_index]\n        text = text + next_char\n    return text\n", "import_code": ["import numpy as np\n"]}
{"id": "fae1edc5-6774-353f-b90b-fe4320ffeeb3_0", "content": "def search(tag, count=10, likes=0, lang=None):\n    if not lang:\n        return api.search(q=tag + ' filter:native_video -filter:retweets',\n            rpp=count, count=count, min_faves=likes, include_entities=True)\n    else:\n        return api.search(q=tag + ' filter:native_video -filter:retweets',\n            rpp=count, count=count, min_faves=likes, lang=lang,\n            include_entities=True)\n", "import_code": []}
{"id": "fae1edc5-6774-353f-b90b-fe4320ffeeb3_1", "content": "def get_media(tweet):\n    try:\n        variants = tweet.extended_entities['media'][0]['video_info']['variants'\n            ]\n        return get_best_video(variants)\n    except:\n        return False\n", "import_code": []}
{"id": "fae1edc5-6774-353f-b90b-fe4320ffeeb3_2", "content": "def get_best_video(variants):\n    highest = variants[0]\n    for v in variants:\n        if v['content_type'] == 'video/mp4':\n            if highest['bitrate'] < v['bitrate']:\n                highest = v\n    return highest['url']\n", "import_code": []}
{"id": "fae1edc5-6774-353f-b90b-fe4320ffeeb3_4", "content": "def scrape(tag, path, count=10, likes=0, lang=None, zip=False):\n    if not os.path.exists(path):\n        os.mkdir(path)\n    for i in search(tag, count=count, likes=likes, lang=lang):\n        url = get_media(i)\n        if url:\n            save_media(url, path + i.user.screen_name)\n    if zip:\n        return get_zip_data(path)\n    return True\n", "import_code": ["import os\n"]}
{"id": "fae1edc5-6774-353f-b90b-fe4320ffeeb3_5", "content": "def get_zip_data(path):\n    base_path = pathlib.Path('./' + path)\n    data = io.BytesIO()\n    with zipfile.ZipFile(data, mode='w') as z:\n        for f_name in base_path.iterdir():\n            z.write(f_name)\n            os.unlink(f_name)\n    os.rmdir(path)\n    data.seek(0)\n    return data\n", "import_code": ["import pathlib\n", "import io\n", "import zipfile\n", "import os\n"]}
{"id": "85cdf67b-2ce3-35c3-8a4c-c8835720fa22_0", "content": "def prob_value(p):\n    q = int(10 * p)\n    l = [1] * q + [0] * (10 - q)\n    item = random.sample(l, 1)[0]\n    return item\n", "import_code": ["import random\n"]}
{"id": "85cdf67b-2ce3-35c3-8a4c-c8835720fa22_1", "content": "def generate_DAG(job_id):\n    short_or_long = random.randint(1, 10)\n    n = 0\n    if short_or_long <= 8:\n        n = max(2, int(random.gauss(4, 5)))\n    else:\n        n = max(10, int(random.gauss(20, 10)))\n    num_of_resource = n + random.randint(0, 3)\n    constraint = []\n    resource = []\n    for i in range(num_of_resource):\n        resource.append(job_id + '_resource' + str(i))\n    into_degree = [0] * n\n    out_degree = [0] * n\n    edges = []\n    for i in range(n - 1):\n        for j in range(i + 1, n):\n            if i == 0 and j == n - 1:\n                continue\n            prob = prob_value(0.4)\n            if prob:\n                if out_degree[i] < 2 and into_degree[j] < 2:\n                    edges.append((i, j))\n                    into_degree[j] += 1\n                    out_degree[i] += 1\n    for node, id in enumerate(into_degree):\n        if node != 0:\n            if id == 0:\n                edges.append((0, node))\n                out_degree[0] += 1\n                into_degree[node] += 1\n    for node, od in enumerate(out_degree):\n        if node != n - 1:\n            if od == 0:\n                edges.append((node, n - 1))\n                out_degree[node] += 1\n                into_degree[n - 1] += 1\n    task_list = []\n    for i in range(n):\n        if i == 0 or i == n - 1:\n            continue\n        task_list.append({'name': job_id + '_task' + str(i), 'resource': [],\n            'time': max(1, random.gauss(3, 2))})\n        local_resource = []\n        num_of_local_resource = random.randint(2, min(num_of_resource, i + 1))\n        for j in range(num_of_local_resource):\n            r = random.randint(0, num_of_local_resource - 1)\n            if resource[r] not in local_resource:\n                local_resource.append(resource[r])\n                task_list[i - 1]['resource'].append({'name': resource[r],\n                    'size': max(random.gauss(400, 400), 50)})\n    for e in edges:\n        if e[0] == 0 or e[1] == n - 1:\n            continue\n        constraint.append([task_list[e[0] - 1]['name'], task_list[e[1] - 1]\n            ['name']])\n        task_list[e[1] - 1]['resource'].append({'name': task_list[e[0] - 1]\n            ['name'], 'size': random.randint(50, 300)})\n    return task_list, constraint, resource\n", "import_code": ["import random\n"]}
{"id": "85cdf67b-2ce3-35c3-8a4c-c8835720fa22_3", "content": "def generate_job_list(n):\n    job = {}\n    job['job'] = []\n    constraint = {}\n    constraint['constraint'] = []\n    data = []\n    for i in range(n):\n        job_id = 'job' + str(i)\n        task_list, const, resource = generate_DAG(job_id)\n        job['job'].append({'name': job_id, 'task': task_list})\n        for c in const:\n            constraint['constraint'].append({'start': c[0], 'end': c[1]})\n        data = data + resource\n    job_json = json.dumps(job, sort_keys=True, indent=4, separators=(',', ': ')\n        )\n    constraint_json = json.dumps(constraint, sort_keys=True, indent=4,\n        separators=(',', ': '))\n    with open('job_list.json', 'w') as j:\n        j.write(job_json)\n    with open('constraint.json', 'w') as c:\n        c.write(constraint_json)\n    return data\n", "import_code": ["import json\n"]}
{"id": "01bc6fdc-df6c-35e9-8cb0-f4c700491b9b_1", "content": "def download_zip(url):\n    print('downloading...' + url)\n    if not os.path.exists('.cache'):\n        os.makedirs('.cache')\n    dz = '.cache/' + url.split('/')[-1]\n    urllib.request.urlretrieve(url, dz)\n    return dz\n", "import_code": ["import os\n"]}
{"id": "03868119-6da6-35d0-85dc-bda4e82c0438_1", "content": "def read_words(filepath):\n    emotion_words = pd.read_csv(filepath)\n    emotion_words.columns = ['Num', 'Disgust', 'Shame', 'Sadness', 'Anger',\n        'Fear', 'Joy', 'Guilt', 'Surprise']\n    return emotion_words\n", "import_code": ["import pandas as pd\n"]}
{"id": "a8630958-4d04-3cb1-b033-04a559b1e1c4_0", "content": "def formatRepoName(repo_name):\n    tokens = [x.capitalize() for x in repo_name.split('-')]\n    return '-'.join(tokens)\n", "import_code": []}
{"id": "e16b2e8a-f73b-355c-b8c3-8b4ede805406_0", "content": "def cleanup_str(st, numbers=False):\n    st = str(st)\n    if numbers == True:\n        keep = set(string.letters + string.digits + ' ')\n    else:\n        keep = set(string.letters + ' ')\n    st = ''.join(x if x in keep else ' ' for x in st)\n    st = re.sub(' +', ' ', st)\n    return st.strip().lower()\n", "import_code": ["import re\n", "import string\n"]}
{"id": "e16b2e8a-f73b-355c-b8c3-8b4ede805406_1", "content": "def cleanup_col(col, numbers=False):\n    col = map(lambda x: cleanup_str(x, numbers=numbers), col)\n    return col\n", "import_code": []}
{"id": "e16b2e8a-f73b-355c-b8c3-8b4ede805406_2", "content": "def cleanup_frame(dat, collist=['user1_string', 'user2_string'], numbers=False\n    ):\n    for col in collist:\n        dat[col] = cleanup_col(dat[col], numbers=False)\n    return dat\n", "import_code": []}
{"id": "e16b2e8a-f73b-355c-b8c3-8b4ede805406_3", "content": "def reduce_strings(stringlist, maxlength, return_arrays=True):\n    splitsreduce = [x[0:maxlength] for x in [x.split(' ') for x in stringlist]]\n    if return_arrays:\n        return splitsreduce\n    shortstrings = [' '.join(x) for x in splitsreduce]\n    return shortstrings\n", "import_code": []}
{"id": "e16b2e8a-f73b-355c-b8c3-8b4ede805406_4", "content": "def vec2pad(doc, max_length):\n    doclength, embdim = np.shape(doc)\n    if doclength < max_length:\n        s = np.zeros([max_length - doclength, embdim])\n        doc = np.concatenate((doc, s), axis=0)\n        return doc\n    elif doclength == max_length:\n        return doc\n    else:\n        print('document is longer that the set max_length')\n        return doc\n", "import_code": ["import numpy as np\n", "from keras.utils import np_utils\n", "import numpy as np\n"]}
{"id": "e16b2e8a-f73b-355c-b8c3-8b4ede805406_6", "content": "def genwordvecs(docs, emb_size, try_load=False, minc=1):\n    vmodel_name = 'embedding_dim_{}_c_{}'.format(emb_size, minc)\n    if try_load == True:\n        try:\n            vmodel = gensim.models.Word2Vec.load('vmodels/' + vmodel_name)\n            print('model loaded from disk')\n            return vmodel\n        except IOError:\n            print('error loading model..')\n            print('training word embeddings')\n    vmodel = gensim.models.Word2Vec(docs, min_count=minc, size=emb_size,\n        workers=4)\n    vmodel.save('vmodels/' + vmodel_name)\n    return vmodel\n", "import_code": ["import gensim\n"]}
{"id": "e16b2e8a-f73b-355c-b8c3-8b4ede805406_7", "content": "def w2v_transform(string_arrays, model, max_length=None):\n    v2w_arrays = map(lambda x: model[[y for y in x if y in model]],\n        string_arrays)\n    if max_length != None:\n        v2w_arrays = map(lambda x: vec2pad(x, max_length), v2w_arrays)\n    return np.array(v2w_arrays)\n", "import_code": ["import numpy as np\n", "from keras.utils import np_utils\n", "import numpy as np\n"]}
{"id": "9afc9b8a-69e9-3cd2-a132-e85414de9341_1", "content": "def convert_cwas_to_csv(subject_dir, out_dir=None, filename=None):\n    \"\"\"\n    Convert CWA files to CSV format and time-sync them.\n    \"\"\"\n    out_dir = out_dir or subject_dir\n    back_cwa = find_back_cwa_file(subject_dir)\n    thigh_cwa = find_thigh_cwa_file(subject_dir)\n    print('Found back_cwa: ', back_cwa)\n    print('Found thigh_cwa:', thigh_cwa)\n    synched_csv = timesync_from_cwa(back_cwa, thigh_cwa, out_dir=out_dir,\n        clean_up=True, out_file=filename)\n    return synched_csv\n", "import_code": []}
{"id": "9afc9b8a-69e9-3cd2-a132-e85414de9341_2", "content": "def find_thigh_cwa_file(subject_dir, strict=True):\n    \"\"\"\n    Look for a thigh cwa file.\n    \"\"\"\n    return _find_file(subject_dir, '*_T.cwa', strict=strict)\n", "import_code": []}
{"id": "9afc9b8a-69e9-3cd2-a132-e85414de9341_3", "content": "def find_back_cwa_file(subject_dir, strict=True):\n    \"\"\"\n    Look for a back cwa file.\n    \"\"\"\n    return _find_file(subject_dir, '*_B.cwa', strict=strict)\n", "import_code": []}
{"id": "9afc9b8a-69e9-3cd2-a132-e85414de9341_4", "content": "def _find_file(subject_dir, file_rgx, strict=True):\n    \"\"\"\n    Searches for a file that matches a pattern in the given directory\n\n    Inputs:\n      - subject_dir\n        A string containing a path to the directory that will be searched\n      - file_rgx\n        A string pattern to search for; \"*\" & \"_\" wildcards are allowed\n      - strict=True\n        Enforces that exactly one file is returned if set to true\n    Output:\n      The filepath to the matched files\n    Throws:\n      An exception if strict is set to true and the number of \n      matches is not exactly one\n    \"\"\"\n    match_string = os.path.join(subject_dir, file_rgx)\n    try:\n        matches = glob.glob(match_string)\n        match, = matches\n        return match\n    except ValueError as e:\n        if not strict:\n            return None\n        if 'not enough values to unpack' in str(e):\n            raise Exception('No file found matching %s ' % match_string)\n        if 'too many values to unpack' in str(e):\n            raise Exception('%s files matches %s' % (len(matches),\n                match_string))\n        raise e\n", "import_code": ["import glob\n", "import os\n"]}
{"id": "9afc9b8a-69e9-3cd2-a132-e85414de9341_6", "content": "def timesync_from_cwa(master_cwa, slave_cwa, out_dir=None, out_file=None,\n    clean_up=True):\n    \"\"\"\n    Time-sync CWA files by aligning the slave output to the master output.\n    \"\"\"\n    timesync_script = TIMESYNC_LOCATION\n    out_dir = out_dir or os.path.dirname(master_cwa)\n    assert os.path.exists(out_dir), 'Output dir \"%s\" does not exist!' % out_dir\n    master_basename_without_extension = os.path.splitext(os.path.basename(\n        master_cwa))[0]\n    slave_basename_without_extension = os.path.splitext(os.path.basename(\n        slave_cwa))[0]\n    master_wav = os.path.join(out_dir, master_basename_without_extension +\n        '.wav')\n    slave_wav = os.path.join(out_dir, slave_basename_without_extension + '.wav'\n        )\n    timesync_output_file = (out_file or master_basename_without_extension +\n        '_' + slave_basename_without_extension + '_timesync_output.csv')\n    timesync_output_path = os.path.join(out_dir, timesync_output_file)\n    intermediary_files = [master_wav, slave_wav]\n    try:\n        print('Converting master and slave CWA files to intermediary WAV files'\n            )\n        run_omconvert(master_cwa, output_wav_path=master_wav)\n        run_omconvert(slave_cwa, output_wav_path=slave_wav)\n        if not os.path.exists(timesync_script):\n            print(\n                'Did not find a compiled version of Timesync. Building Timesync from source. This may take a while.'\n                )\n            timesync_directory = os.path.dirname(timesync_script)\n            make_call = ['make', '-C', timesync_directory]\n            subprocess.call(make_call)\n        print('Running Timesync')\n        subprocess.call([timesync_script, master_wav, slave_wav, '-csv',\n            timesync_output_path])\n        return timesync_output_path\n    except Exception as e:\n        print('Encountered exception during conversion:', e)\n        raise e\n    finally:\n        if clean_up:\n            print('Removing intermediary files', intermediary_files)\n            for f in intermediary_files:\n                subprocess.call(['rm', f])\n", "import_code": ["import subprocess\n", "import os\n"]}
{"id": "4d6767ca-cc6d-3fb6-b2b4-d7b774fe2c04_0", "content": "def get_courses_link_list(xml_content, record_count):\n    root = etree.fromstring(xml_content)\n    url_list = []\n    for element in root.getchildren():\n        for child in element.getchildren():\n            url_list.append(child.text)\n    return url_list[:record_count]\n", "import_code": ["from lxml import etree\n"]}
{"id": "4d6767ca-cc6d-3fb6-b2b4-d7b774fe2c04_1", "content": "def fetch_content(link):\n    response = requests.get(link).content\n    return response\n", "import_code": ["import requests\n"]}
{"id": "4d6767ca-cc6d-3fb6-b2b4-d7b774fe2c04_2", "content": "def get_course_inform(html_content, course):\n    soup = BeautifulSoup(html_content, 'html.parser')\n    course_inform = dict(course_title=soup.find('h2').get_text(), language=\n        soup.find('div', class_='rc-Language').get_text(), start_date=soup.\n        find('div', class_='startdate rc-StartDateString caption-text').\n        get_text(), continuation=len(soup.findAll('div', class_='week')),\n        rating=None)\n    rating = soup.find('div', class_='ratings-text headline-2-text')\n    if rating:\n        course_inform['rating'] = rating.getText()\n    return course_inform\n", "import_code": ["from bs4 import BeautifulSoup\n"]}
{"id": "4d6767ca-cc6d-3fb6-b2b4-d7b774fe2c04_3", "content": "def output_courses_info_to_xls(course_list, ws):\n    ws.title = 'Coursera courses info'\n    ws['A1'] = 'course_title'\n    ws['B1'] = 'language'\n    ws['C1'] = 'start_date'\n    ws['D1'] = 'continouation'\n    ws['E1'] = 'rating'\n    for course in course_list:\n        ws.append([course['course_title'], course['language'], course[\n            'start_date'], course['continuation'], course['rating']])\n    return ws\n", "import_code": []}
{"id": "bd3eef84-2474-3bfc-b0b7-17e41cd14a70_0", "content": "def load_workbook_range(range_string, ws):\n    col_start, col_end = re.findall('[A-Z]+', range_string)\n    data_rows = []\n    for row in ws[range_string]:\n        data_rows.append([cell.value for cell in row])\n    return pd.DataFrame(data_rows)\n", "import_code": ["import pandas as pd\n", "import re\n"]}
{"id": "93f5abe0-a8df-3863-851d-a91c75976cd9_0", "content": "def transform_etl(data: any) ->any:\n    df = pd.DataFrame(data)\n    df.rename(columns={'text': 'message'}, inplace=True)\n    df.drop(columns=['author'], inplace=True)\n    return df.to_json(orient='records')\n", "import_code": ["import pandas as pd\n"]}
{"id": "093c851a-ddec-31c7-ae50-ba0c66956357_0", "content": "def get_stats(stock_data):\n    return {'last': np.mean(stock_data.tail(1)), 'short_mean': np.mean(\n        stock_data.tail(30)), 'long_mean': np.mean(stock_data.tail(90)),\n        'short_rolling': stock_data.rolling(window=30).mean(),\n        'long_rolling': stock_data.rolling(window=90).mean()}\n", "import_code": ["import numpy as np\n"]}
{"id": "093c851a-ddec-31c7-ae50-ba0c66956357_1", "content": "def clean_data(stock_data, col):\n    weekdays = pd.date_range(start=START_DATE, end=END_DATE)\n    clean_data = stock_data[col].reindex(weekdays)\n    return clean_data.fillna(method='ffill')\n", "import_code": ["import pandas as pd\n", "from pandas_datareader import data\n"]}
{"id": "422d440f-27fc-37a7-ac6c-d795d82f50ec_0", "content": "def train_test_split(dataset, labels):\n    train_test_split_ratio = 0.7\n    combined = np.array(zip(dataset, labels))\n    shuffle_indices = np.random.permutation(np.arange(len(combined)))\n    shuffled_data = combined[shuffle_indices]\n    indices_of_1 = np.where(shuffled_data[:, 1] == 1)[0]\n    indices_of_0 = np.where(shuffled_data[:, 1] == 0)[0]\n    train_data = np.concatenate((shuffled_data[indices_of_0[0:int(0.7 * len\n        (indices_of_0))]], shuffled_data[indices_of_1[0:int(0.7 * len(\n        indices_of_1))]]), axis=0)\n    test_data = np.concatenate((shuffled_data[indices_of_0[int(0.7 * len(\n        indices_of_0)):]], shuffled_data[indices_of_1[int(0.7 * len(\n        indices_of_1)):]]), axis=0)\n    return train_data, test_data\n", "import_code": ["import numpy as np\n"]}
{"id": "788772b9-b869-38cd-9b3d-d9fd9ffb19cb_5", "content": "@app.route('/models/<model_name>')\ndef modelInfo(model_name):\n    return str(pickle.loads(app.r.get(model_name + '_object'))), 200\n", "import_code": ["import pickle\n"]}
{"id": "d5d9e84e-dcd7-3442-a372-92859624fc0a_0", "content": "def load_data(messages_filepath, categories_filepath):\n    \"\"\"This function is used to extract messages and\n    categories data with given paths. Indexes of input should match.\n    Input:\n        messages_filepath (str): path to csv file of messages\n        categories_filepath (str): path to csv file of categories\n    Output:\n        df (pd.DataFrame): dataframe that includes messages and categories\n        \n    \"\"\"\n    messages = pd.read_csv(messages_filepath)\n    categories = pd.read_csv(categories_filepath)\n    df = pd.merge(messages, categories)\n    categories = df['categories'].str.split(pat=';', expand=True)\n    row = categories.iloc[0]\n    category_colnames = row.apply(lambda x: x[0:-2])\n    categories.columns = category_colnames\n    for column in categories:\n        categories[column] = categories[column].str[-1]\n        categories[column] = pd.to_numeric(categories[column])\n    df.drop('categories', axis=1, inplace=True)\n    df = pd.concat([df, categories], axis=1)\n    return df\n", "import_code": ["import pandas as pd\n"]}
{"id": "d5d9e84e-dcd7-3442-a372-92859624fc0a_1", "content": "def clean_data(df):\n    \"\"\"This function is use to clean merged messages and \n    categories dataframe. Duplicate values are removed\n    Input:\n        df (pd.DataFrame): data to be cleaned\n    Output:\n        df (pd.DataFrame): cleaned dataframe\n    \"\"\"\n    df.drop_duplicates(inplace=True)\n    return df\n", "import_code": []}
{"id": "cd3bfa51-7a55-3c5b-bddf-a23a5e9d2fde_0", "content": "def find_anagram(dictionary):\n    anagrams = collections.defaultdict(list)\n    for s in dictionary:\n        temp = [0] * 26\n        for c in s:\n            temp[ord(c) - ord('a')] += 1\n        anagrams[tuple(temp)].append(s)\n    return [group for group in anagrams.values() if len(group) >= 2]\n", "import_code": ["import collections\n"]}
{"id": "e8cb45c1-f588-3a80-965b-a99cc6c344ac_0", "content": "def _to_pd_period(freq: str, date_like: tp.Union[pd.Period, datetime, date,\n    str]) ->pd.Period:\n    if isinstance(date_like, pd.Period):\n        return date_like\n    return pd.Period(date_like, freq=freq)\n", "import_code": ["from datetime import datetime, date\n", "import pandas as pd\n", "from datetime import datetime, date\n", "import typing as tp\n"]}
{"id": "39b2406a-69b6-38dc-b4e9-60d3dc75ff25_0", "content": "def random_pass(choice):\n    global password\n    password_char = (string.ascii_lowercase + string.ascii_uppercase +\n        string.punctuation + string.digits)\n    password = list(''.join(random.choice(password_char) for i in range(\n        choice)))\n    return list(password)\n", "import_code": ["import random\n", "import string\n"]}
{"id": "90a7bdef-f1fe-3d68-8530-adf650f5b4ac_0", "content": "def getFileList(path):\n    files = []\n    for f in os.listdir(path):\n        file = os.path.join(path, f)\n        files.append(file)\n    return files\n", "import_code": ["import os\n"]}
{"id": "90a7bdef-f1fe-3d68-8530-adf650f5b4ac_1", "content": "def getFileDT(files):\n    dates = []\n    for f in files:\n        dtm = os.path.getmtime(f)\n        dtc = os.path.getctime(f)\n        dt = dtm\n        if dtc < dtm:\n            dt = dtc\n        data = datetime.datetime.fromtimestamp(dt)\n        dates.append('{}-{}h{}m{}'.format(data.date(), data.hour, data.\n            minute, data.second))\n    return dates\n", "import_code": ["import datetime\n", "import os\n"]}
{"id": "76160db9-7d79-3a64-82a8-d59a79ebcf47_0", "content": "def generate_nuclear_masks_cli_str(sources, destination, source_dir):\n    return shlex.join(['pipenv', 'run', 'python', __file__, \n        '--destination=%s' % destination, '--source_dir=%s' % source_dir, *\n        [str(source) for source in sources]])\n", "import_code": ["import shlex\n"]}
{"id": "99eb5e6a-bf10-3b47-8059-fed28b937808_1", "content": "def files_in_folder(folder, format='jpeg'):\n    \"\"\"\n    return a list of file names in folder\n\n    Input:   path of folder\n    Output:  [file_names]\n    \"\"\"\n    try:\n        os.stat(folder)\n    except:\n        print('{} is not a valid path!'.format(folder))\n        return\n    imgs = [p[2] for p in walk(folder)][0]\n    imgs = list(filter(lambda x: x.endswith(format), imgs))\n    return imgs\n", "import_code": ["from os import walk\n", "import os, sys\n", "from os import walk\n"]}
{"id": "99eb5e6a-bf10-3b47-8059-fed28b937808_2", "content": "def dirs_in_dir(parent_path):\n    \"\"\"\n    get list of directories in parent folder\n    Args:\n        dir_path: [abs_path for each subdirectory]\n\n    Return:\n    \"\"\"\n    dirs = []\n    for dir_path in os.listdir(parent_path):\n        dir_path = os.path.join(parent_path, dir_path)\n        if not os.path.isdir(dir_path):\n            continue\n        dirs.append(dir_path)\n    return dirs\n", "import_code": ["import os, sys\n", "from os import walk\n"]}
{"id": "99eb5e6a-bf10-3b47-8059-fed28b937808_3", "content": "def get_fpath(f):\n    \"\"\"\n    get the absolute path of a file\n    Args:\n        f:\n\n    Return:\n    \"\"\"\n    return os.path.dirname(os.path.abspath(f))\n", "import_code": ["import os, sys\n", "from os import walk\n"]}
{"id": "1811437e-4442-38c2-86c6-0e254370c32c_0", "content": "def get_bit_in_byte(b, index, startFromMsb=False):\n    if index > 7:\n        raise IndexError\n    max_index = 7\n    if startFromMsb:\n        target_index = max_index - index\n    else:\n        target_index = index\n    return int(1 << target_index & b != 0)\n", "import_code": []}
{"id": "1811437e-4442-38c2-86c6-0e254370c32c_1", "content": "def get_bit_in_byte_array(byte_array, index):\n    byte_index = index // 8\n    bit_index_in_byte = index % 8\n    target_byte = byte_array[byte_index]\n    bit = get_bit_in_byte(target_byte, bit_index_in_byte, startFromMsb=True)\n    return bit\n", "import_code": []}
{"id": "1811437e-4442-38c2-86c6-0e254370c32c_2", "content": "def base64encode(byte_array):\n    output_chars = []\n    num_bytes = len(byte_array)\n    num_bits = 8 * num_bytes\n    code_index = 0\n    bits_counted = 0\n    for index in range(num_bits):\n        bit = get_bit_in_byte_array(byte_array, index)\n        if bit:\n            code_index += int(pow(2, 5 - bits_counted))\n        bits_counted += 1\n        if bits_counted % 6 == 0:\n            output_chars.append(CODE[code_index])\n            code_index = 0\n            bits_counted = 0\n    return bytes(output_chars)\n", "import_code": []}
{"id": "1811437e-4442-38c2-86c6-0e254370c32c_3", "content": "def int_to_bits(num):\n    result = [get_bit_in_byte(num, i, startFromMsb=True) for i in range(2, 8)]\n    return result\n", "import_code": []}
{"id": "1811437e-4442-38c2-86c6-0e254370c32c_4", "content": "def char_block_to_byte_block(block):\n    byte_array = [0] * 3\n    bits = [i for six_bits in [int_to_bits(CHAR_TO_INT[c]) for c in block] for\n        i in six_bits]\n    for i, bit in enumerate(bits):\n        if bit == 0:\n            continue\n        byte_index = i // 8\n        bit_index = i % 8\n        from_msb_index = 7 - bit_index\n        byte_array[byte_index] = byte_array[byte_index] | 1 << from_msb_index\n    return bytes(byte_array)\n", "import_code": []}
{"id": "1811437e-4442-38c2-86c6-0e254370c32c_5", "content": "def base64decode(byte_array):\n    assert len(byte_array) % 4 == 0\n    index = 4\n    blocks = []\n    while index <= len(byte_array):\n        blocks.append(byte_array[index - 4:index])\n        index += 4\n    assert len(blocks) == len(byte_array) // 4\n    byte_blocks = [char_block_to_byte_block(block) for block in blocks]\n    return bytes([b for byte_block in byte_blocks for b in byte_block])\n", "import_code": []}
{"id": "ec8e1fcd-bf5a-3404-8f88-da95ce8c307e_2", "content": "def checksum(data):\n    return hashlib.md5(data).hexdigest()\n", "import_code": ["import hashlib\n"]}
{"id": "170685e6-d640-3662-9795-f73a350806af_4", "content": "def ingest_csv_form_data(filename: str):\n    \"\"\"Read and print the contents of a CSV file.\"\"\"\n    with open(filename) as csv_file:\n        csv_reader = csv.reader(csv_file, delimiter=',')\n        line_count = 0\n        for row in csv_reader:\n            if line_count == 0:\n                print(f\"Column names are {', '.join(row)}\")\n                line_count += 1\n            else:\n                print(f\"Column values are {', '.join(row)}\")\n                line_count += 1\n        print(f'Processed {line_count} lines.')\n        return csv_reader\n", "import_code": ["import csv\n"]}
{"id": "2fc1ff11-5eb2-35d1-8c9d-35820e5d91b1_0", "content": "def last_n_crix_values(n):\n    with urllib.request.urlopen('http://thecrix.de/data/crix_hf.json') as url:\n        data = json.loads(url.read().decode())[-n:]\n    return data\n", "import_code": ["import json\n", "import urllib.request, json\n"]}
{"id": "b4fb1ceb-993b-3b2e-8f0c-3290434cce55_0", "content": "def api_request(api_base_url='http://localhost:8080/', path='', method=\n    'get', data=None, params={}, verify=True, cert=list()):\n    \"\"\"\n    Wrapper function for requests\n\n    :param api_base_url: Base URL for requests\n    :param path: Path to request\n    :param method: HTTP method\n    :param data: Data for post (ignored for GETs)\n    :param params: Dict of key, value query params\n    :param verify: True/False/CA_File_Name to perform SSL Verification of CA Chain\n    :param cert: list of cert and key to use for client authentication\n\n    \"\"\"\n    method = method.lower()\n    headers = {'Accept': 'application/json', 'Content-type': 'application/json'\n        }\n    methods = {'get': requests.get, 'post': requests.post}\n    if path[0] != '/':\n        path = '/{0}'.format(path)\n    if params:\n        path += '?{0}'.format(urllib.urlencode(params))\n    url = '{0}{1}'.format(api_base_url, path)\n    resp = methods[method](url, data=json.dumps(data), headers=headers,\n        verify=verify, cert=cert)\n    return resp\n", "import_code": ["import urllib\n", "import requests\n", "import simplejson as json\n", "import json\n"]}
{"id": "b4fb1ceb-993b-3b2e-8f0c-3290434cce55_1", "content": "def _make_api_request(api_url=None, path=None, verify=False, cert=list(),\n    params={}):\n    resp = api_request(api_url, path, params=params, verify=verify, cert=cert)\n    data = json.loads(resp.content)\n    return data\n", "import_code": ["import simplejson as json\n", "import json\n"]}
{"id": "e73f7bea-47d1-3e55-a3f1-f019124e3fa1_0", "content": "def scan_index(path, word):\n    \"\"\"\n    Returns the posting list for the word\n    \"\"\"\n    inputbuffer = ''\n    with open(path, 'r') as inputfile:\n        mapping = {}\n        for line in inputfile:\n            i_word = line.split('.')[0]\n            if word == i_word:\n                doc_freq_map = line.split('.')[1:]\n                for mapset in doc_freq_map:\n                    if len(mapset) > 2:\n                        terms = mapset.split('_')\n                        mapping[terms[0]] = terms[1]\n                return mapping\n        return None\n", "import_code": []}
{"id": "e73f7bea-47d1-3e55-a3f1-f019124e3fa1_2", "content": "def process(postings):\n    lst = {}\n    words = postings.keys()\n    for word in words:\n        for doc in postings[word].keys():\n            if doc not in lst.keys():\n                lst[doc] = 1\n            else:\n                lst[doc] += 1\n    req = len(words)\n    lstfinal = []\n    for elem in lst:\n        if lst[elem] == req:\n            lstfinal.append(elem)\n    rank(lstfinal, words, postings)\n    return\n", "import_code": []}
{"id": "e73f7bea-47d1-3e55-a3f1-f019124e3fa1_3", "content": "def query_func(index_path):\n    global stopwords\n    alphabet = range(0, 26)\n    print('Ready')\n    while True:\n        query = input('Query: ')\n        if query == 'q':\n            return\n        else:\n            querystart = time.time()\n            query = query.lower()\n            tokens = query.split(' ')\n            plist = {}\n            for word in tokens:\n                if word in stopwords:\n                    continue\n                search_word = ps.stem(word)\n                if len(search_word) < 2:\n                    continue\n                t1 = ord(search_word[0]) - 97\n                t2 = ord(search_word[1]) - 97\n                if t1 not in alphabet:\n                    t1 = 26\n                if t2 not in alphabet:\n                    t2 = 26\n                search_path = str(t1) + '_' + str(t2)\n                temp = scan_index(index_path + '/' + search_path, search_word)\n                if temp:\n                    plist[search_word] = temp\n            process(plist)\n            queryend = time.time()\n            print('Time Taken:', queryend - querystart)\n", "import_code": ["import time\n"]}
{"id": "e73f7bea-47d1-3e55-a3f1-f019124e3fa1_4", "content": "def Merge(dict1, dict2):\n    res = {**dict1, **dict2}\n    return res\n", "import_code": []}
{"id": "2c50dee5-dcc6-363f-a69c-f9eba8b22067_0", "content": "def run_aspell(input, flags=[]):\n    if not isinstance(input, str):\n        input = '\\n'.join(input)\n    return subprocess.run(['aspell', 'list', '--mode=none'] + flags, input=\n        input, capture_output=True, universal_newlines=True).stdout.split()\n", "import_code": ["import subprocess\n"]}
{"id": "451fa4ff-6900-3d87-9d87-33e6bec59dd0_0", "content": "def stats_text_en(it, n):\n    try:\n        wq = {}\n        for i in it.split():\n            j = re.sub(\"[^a-zA-Z\\\\']\", '', i)\n            if j not in wq:\n                wq[j] = 1\n            else:\n                wq[j] += 1\n        k = Counter(wq).most_common(n)\n        return k\n    except TypeError:\n        print('From module:\\tThe type of input is not a string')\n    except ValueError:\n        print('From module:\\tThe value of input is not correct')\n    except AttributeError:\n        print(\n            'From module:\\tThe type of input does not have this type of attribute'\n            )\n", "import_code": ["import re\n", "from collections import Counter\n"]}
{"id": "451fa4ff-6900-3d87-9d87-33e6bec59dd0_1", "content": "def stats_text_cn(it, count):\n    its = re.sub(\"[\u3002\uff01\u3001\uff1f\uff5e\uff0c !*-.\\\\n\\\\',a-zA-Z]\", '', it)\n    k = Counter(its).most_common(count)\n    return k\n", "import_code": ["import re\n", "from collections import Counter\n"]}
{"id": "451fa4ff-6900-3d87-9d87-33e6bec59dd0_2", "content": "def stats_text(it, n):\n    se = stats_text_en(it, n)\n    sc = stats_text_cn(it, n)\n    return se + sc\n", "import_code": []}
{"id": "e80dd771-76ea-322f-9098-03de24e3de68_0", "content": "def get_date_under(date):\n    \"\"\"\n    Convert date str format from dashes to underscores\n\n    :param date: string using format (YYYY-MM-DD)\n    :returns: string using format (YYYY_MM_DD)\n    \"\"\"\n    return date.replace('-', '_')\n", "import_code": []}
{"id": "e80dd771-76ea-322f-9098-03de24e3de68_1", "content": "def get_actual_date(date):\n    \"\"\"\n    Convert date str to actual python date object\n\n    :param date: string using format (YYYY-MM-DD)\n    :returns: python date object\n    \"\"\"\n    return datetime.strptime(date, '%Y-%m-%d')\n", "import_code": ["from datetime import datetime, timedelta\n"]}
{"id": "e80dd771-76ea-322f-9098-03de24e3de68_2", "content": "def get_date_str(date):\n    \"\"\"\n    Convert date object to date string (YYYY-MM-DD)\n\n    :param date: python date object\n    :returns: string using format (YYYY-MM-DD)\n    \"\"\"\n    return date.strftime('%Y-%m-%d')\n", "import_code": []}
{"id": "e80dd771-76ea-322f-9098-03de24e3de68_3", "content": "def get_date_range(start_date, end_date):\n    \"\"\"\n    Create a list of all dates included in a specified time period\n\n    :param start_date: date string (YYYY-MM-DD)\n    :param end_date: date string (YYYY-MM-DD)\n    :returns: list of dates\n    \"\"\"\n    ta = get_actual_date(start_date)\n    tb = get_actual_date(end_date)\n    if tb < ta:\n        raise ValueError(\n            'Time period is negative (end date comes before start date)')\n    delta = tb - ta\n    dates = [ta]\n    for i in range(1, delta.days):\n        tc = ta + timedelta(days=i)\n        dates.append(tc)\n    if tb > ta:\n        dates.append(tb)\n    return dates\n", "import_code": ["from datetime import datetime, timedelta\n"]}
{"id": "940e1d0f-78b5-304a-8ed0-c7308ecac1c2_0", "content": "def generate_random_string(length):\n    letters = string.ascii_lowercase\n    rand_string = ''.join(random.choice(letters) for _ in range(length))\n    return rand_string\n", "import_code": ["import random\n", "import string\n"]}
{"id": "f785ffd3-b44d-3e57-a3e9-9b7a133a290d_0", "content": "def matrix_authenticate(session, username, password):\n    url = 'https://app.matrixbooking.com/api/v1/user/login'\n    session.post(url, json={'username': username, 'password': password})\n    return session\n", "import_code": []}
{"id": "f785ffd3-b44d-3e57-a3e9-9b7a133a290d_1", "content": "def make_booking_params(time_from, time_to, status=None, pageSize=None,\n    pageNum=0):\n    params = {'f': time_from, 't': time_to, 'bc': 'ROOM', 'status': status,\n        'include': ['audit', 'locations'], 'pageSize': pageSize, 'pageNum':\n        pageNum}\n    return params\n", "import_code": []}
{"id": "f785ffd3-b44d-3e57-a3e9-9b7a133a290d_2", "content": "def get_payload(session, url, parameters):\n    resp = session.get(url=url, cookies=session.cookies, params=parameters)\n    print(f'GET {resp.url}')\n    print(f'response status code: {resp.status_code}')\n    return resp.json()\n", "import_code": []}
{"id": "f785ffd3-b44d-3e57-a3e9-9b7a133a290d_3", "content": "def get_locations_from_api(username, password):\n    ses = requests.session()\n    matrix_authenticate(ses, username, password)\n    url = 'https://app.matrixbooking.com/api/v1/booking'\n    params = make_booking_params(time_from=date.today(), time_to='eod',\n        status=['CONFIRMED', 'TENTATIVE', 'CANCELLED'])\n    data = get_payload(ses, url, params)\n    return pd.io.json.json_normalize(data['locations'])\n", "import_code": ["from datetime import datetime, date, timedelta\n", "import pandas as pd\n", "import requests\n"]}
{"id": "8623520d-c0ac-38c3-97c0-c9b05b511972_0", "content": "def int_decode(label):\n    if label == 'true':\n        label = 0\n    elif label == 'half-true':\n        label = 1\n    elif label == 'barely-true':\n        label = 2\n    elif label == 'mostly-true':\n        label = 3\n    elif label == 'false':\n        label = 4\n    elif label == 'pants-fire':\n        label = 5\n    return label\n", "import_code": []}
{"id": "ff73510f-86d6-39da-adc2-a97f51034071_3", "content": "def checktime(base, sound_time):\n    \"\"\"\n    This function checks if a sound event happened within the last 6 hours.\n    It takes two time strings as input, the base time and the sound time.\n    \"\"\"\n    if len(sound_time) == 16:\n        sound_time += ':00'\n    t1 = time.strptime(base, '%Y-%m-%d %H:%M:%S')\n    t2 = time.strptime(sound_time, '%Y-%m-%d %H:%M:%S')\n    interval = (time.mktime(t1) - time.mktime(t2)) / 3600.0\n    if interval <= 6.0:\n        return True\n    else:\n        return False\n", "import_code": ["import datetime\n", "import time\n"]}
{"id": "ff73510f-86d6-39da-adc2-a97f51034071_4", "content": "def sub_time(time1, time2):\n    \"\"\"\n    This function returns the interval between two times in hours.\n    \"\"\"\n    t1 = time.strptime(time1, '%Y-%m-%d %H:%M:%S')\n    t2 = time.strptime(time2, '%Y-%m-%d %H:%M:%S')\n    interval = (time.mktime(t1) - time.mktime(t2)) / 3600.0\n    return interval\n", "import_code": ["import datetime\n", "import time\n"]}
{"id": "ff73510f-86d6-39da-adc2-a97f51034071_5", "content": "def dt2str(news_time):\n    \"\"\"\n    This function converts a datetime object to a string in the format '%Y-%m-%d %H:%M:%S'.\n    \"\"\"\n    return news_time.strftime('%Y-%m-%d %H:%M:%S')\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_0", "content": "def fb_post_type_str(type_id, default=None):\n    return _FB_POST_TYPES.get(type_id, default or type_id)\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_1", "content": "def links_from_str(input_str):\n    return re.findall(REGEX_URL, input_str)\n", "import_code": ["import re\n"]}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_2", "content": "def wordlist_regex(words):\n    matches = '|'.join(words)\n    return re.compile('(' + matches + ')', flags=re.IGNORECASE)\n", "import_code": ["import re\n"]}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_3", "content": "def make_anon_name(name):\n    return ''.join([n[0] for n in name.split()]).upper()\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_5", "content": "def day_range(year, month, start_day, end_day=None):\n    end_day = end_day or start_day\n    return datetime(year, month, start_day, tzinfo=timezone.utc), datetime(year\n        , month, end_day, 23, 59, 59, tzinfo=timezone.utc)\n", "import_code": ["from django.utils import timezone\n", "from datetime import datetime, timedelta\n"]}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_6", "content": "def padded_date_range(start_date, end_date=None):\n    end_date = end_date or start_date\n    return datetime(start_date.year, start_date.month, start_date.day,\n        tzinfo=timezone.utc), datetime(end_date.year, end_date.month,\n        end_date.day, 23, 59, 59, tzinfo=timezone.utc)\n", "import_code": ["from datetime import datetime, timedelta\n", "from django.utils import timezone\n"]}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_7", "content": "def isodatestr_to_date(datestr):\n    return isodatestr_to_datetime(datestr).date()\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_8", "content": "def isodatestr_to_datetime(datestr):\n    return datetime.strptime(datestr, '%Y-%m-%d').replace(tzinfo=timezone.utc)\n", "import_code": ["from datetime import datetime, timedelta\n", "from django.utils import timezone\n"]}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_9", "content": "def date_to_timestamp(dateobj, depth=None):\n    if depth:\n        return int(time.mktime(dateobj.timetuple()[:depth]))\n    else:\n        return int(time.mktime(dateobj.timetuple()))\n", "import_code": ["from datetime import datetime, timedelta\n", "import time\n", "from django.utils import timezone\n"]}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_10", "content": "def ordinal_to_timestamp(ordinal):\n    return (ordinal - 719163) * 24 * 60 * 60\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_11", "content": "def recent_time_frame(days=14):\n    end = timezone.now().date() + timedelta(days=1)\n    start = end - timedelta(days=days)\n    return start, end\n", "import_code": ["from django.utils import timezone\n", "from datetime import datetime, timedelta\n"]}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_12", "content": "def timestamp_to_datetime(timestamp):\n    return datetime.fromtimestamp(float(timestamp), timezone.utc)\n", "import_code": ["from datetime import datetime, timedelta\n", "from django.utils import timezone\n"]}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_14", "content": "def list_of_properties(objects, property_name):\n    return [getattr(obj, property_name) for obj in objects if getattr(obj,\n        property_name) != None]\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_15", "content": "def dict_of_objects(objects, property_name):\n    return {unicode(getattr(obj, property_name)): obj for obj in objects}\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_16", "content": "def quoted_list_str(items):\n    return ','.join([(\"'%s'\" % item) for item in items])\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_17", "content": "def property_list_str(objects, property_name):\n    return ', '.join(list_of_properties(objects, property_name))\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_18", "content": "def get_choice_id(choicename, choices, default=0):\n    for choice in choices:\n        if choice[1] == choicename:\n            return choice[0]\n    return default\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_19", "content": "def get_choice_name(choice_id, choices):\n    for choice in choices:\n        if choice[0] == choice_id:\n            return choice[1]\n    return None\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_20", "content": "def json_error(message):\n    return '{\"type\":\"error\", \"message\":%s}' % message\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_21", "content": "def can_access_user(user, username):\n    return user.is_staff or user.username == username\n", "import_code": []}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_22", "content": "def from_jstimestamp(jstimestamp):\n    \"\"\"Converts Javascript timestamp to datetime object.\"\"\"\n    return datetime.utcfromtimestamp(jstimestamp / 1000.0).replace(tzinfo=\n        timezone.utc)\n", "import_code": ["from datetime import datetime, timedelta\n", "from django.utils import timezone\n"]}
{"id": "d318837e-b03c-3d03-8890-a886193f69b2_23", "content": "def truncate_html(message, length=12):\n    return Truncator(message).words(length, html=True, truncate=' ...')\n", "import_code": ["from django.utils.text import Truncator\n"]}
{"id": "2f2ff102-e989-3502-9f2d-b4511a766f64_0", "content": "def load_json(time, brand, activity):\n    \"\"\"\n    This function loads a JSON file from the specified path.\n    The path is determined by the time, brand, and activity parameters.\n    \"\"\"\n    file_path = '../data/' + brand + '/' + activity + '/' + time + '.json'\n    with open(file_path, encoding='UTF-8') as json_data:\n        print('--- load file from', file_path, '---')\n        result_json = json.load(json_data)\n    return result_json\n", "import_code": ["import json\n"]}
{"id": "b0fe6150-ca63-3cda-b2ae-9a7c39f15c77_0", "content": "def modeManual(arg1, arg2):\n    startTime = DT.datetime.strptime(arg1, '%Y-%m-%d %H:%M:%S')\n    endTime = DT.datetime.strptime(arg2, '%Y-%m-%d %H:%M:%S')\n    startInt = startTime\n    while startInt + timeInt <= endTime:\n        fp = open(logFile, 'a')\n        fp.write('[' + str(DT.datetime.utcnow()) + '] ' + str(startInt) +\n            ' - ' + str(startInt + timeInt) + ' >> START\\n')\n        fp.close()\n        p = subprocess.Popen([\n            '/opt/spark-1.3.0-bin-hadoop2.4/bin/spark-submit', '--master',\n            'local[4]', prgName, '%s' % str(startInt), '%s' % str(startInt +\n            timeInt)])\n        p.wait()\n        fp = open(logFile, 'a')\n        if p.returncode == 0:\n            fp.write('[' + str(DT.datetime.utcnow()) + '] ' + str(startInt) +\n                ' - ' + str(startInt + timeInt) + ' >> COMPLETE\\n')\n        else:\n            fp.write('[' + str(DT.datetime.utcnow()) + '] ' + str(startInt) +\n                ' - ' + str(startInt + timeInt) + ' >> FAILED\\n')\n        fp.close()\n        startInt += timeInt\n    return\n", "import_code": ["import datetime as DT\n", "from datetime import timedelta\n", "import subprocess\n"]}
{"id": "f13bda36-357a-3320-928e-bc4780412883_0", "content": "def _all_to_lower(tokens):\n    return tuple(map(lambda x: x.lower(), tokens))\n", "import_code": []}
{"id": "f13bda36-357a-3320-928e-bc4780412883_1", "content": "def load_list(path):\n    \"\"\"\n    Loads a list of data from a file.\n\n    :param path: The path to the file.\n    :returns: A dictionary where the keys are tuples of lowercase tokens and the values are tuples of original tokens.\n\n    \"\"\"\n    data = {}\n    with codecs.open(path, encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            tokens = nltk.word_tokenize(line)\n            k = _all_to_lower(tokens)\n            v = tuple(tokens)\n            if k and v:\n                data[k] = v\n    return data\n", "import_code": ["import codecs\n", "import nltk\n"]}
{"id": "f13bda36-357a-3320-928e-bc4780412883_2", "content": "def load_simple_list(path):\n    \"\"\"\n    Loads a simple list of data from a file.\n\n    :param path: The path to the file.\n    :returns: A list of lines from the file.\n\n    \"\"\"\n    data = []\n    with codecs.open(path, encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            data.append(line)\n    return data\n", "import_code": ["import codecs\n"]}
{"id": "f13bda36-357a-3320-928e-bc4780412883_3", "content": "def load_map(path, delimiter):\n    \"\"\"\n    Loads a map from a file.\n\n    :param path: The path to the file.\n    :param delimiter: The delimiter to split the lines.\n    :returns: A dictionary where the keys are tuples of lowercase tokens and the values are tuples of tokens split by the delimiter.\n\n    \"\"\"\n    data = {}\n    with codecs.open(path, encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            try:\n                k, v = line.split(delimiter)\n            except ValueError:\n                pass\n            else:\n                k = _all_to_lower(nltk.word_tokenize(k))\n                v = tuple(nltk.word_tokenize(v))\n                if k and v:\n                    data[k] = v\n    return data\n", "import_code": ["import codecs\n", "import nltk\n"]}
{"id": "ee656f9c-fe6f-30e0-a94e-f3cbc8828950_0", "content": "def n_sided_shape(n):\n    shapes = ['circle', 'semi-circle', 'triangle', 'square', 'pentagon',\n        'hexagon', 'heptagon', 'octagon', 'nonagon', 'decagon']\n    return shapes[n - 1]\n", "import_code": []}
{"id": "ee656f9c-fe6f-30e0-a94e-f3cbc8828950_1", "content": "def n_sided_shape(n):\n    shapes = {(1): 'circle', (2): 'semi-circle', (3): 'triangle', (4):\n        'square', (5): 'pentagon', (6): 'hexagon', (7): 'heptagon', (8):\n        'octagon', (9): 'nonagon', (10): 'decagon'}\n    return shapes[n]\n", "import_code": []}
{"id": "e51c4990-2508-3418-ab49-d6761baf9caf_0", "content": "def animalrecognition(path):\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = cv2.resize(img, (int(img_size), int(img_size)))\n    img = img.reshape(-1, img_size, img_size, 1).astype('float32')\n    img /= 255\n    x = model.predict(img)\n    y = x > 0.5\n    if y.any():\n        x = np.argmax(x, axis=1)\n        recognize_animal = animal[x[0]]\n    else:\n        recognize_animal = 'Not Found'\n    return recognize_animal\n", "import_code": ["import cv2\n", "import numpy as np\n"]}
{"id": "ca2023bd-f0c9-3ab2-9991-4b5842363f9f_1", "content": "def functionCount(emulationID, emulationName, emulationLifetimeID,\n    startTimesec, duration, distributionGranularity, distributionArg,\n    resType, HOMEPATH):\n    \"\"\"\n    The actual distribution function which counts start time, duration and stress value for each run of distribution.\n    \"\"\"\n    startLoad = int(distributionArg['startload'])\n    stopLoad = int(distributionArg['stopload'])\n    triggerType = 'time'\n    upperBoundary = int(distributionGranularity) - 1\n    duration = float(duration)\n    stressValues = []\n    runStartTimeList = []\n    runDurations = []\n    runDuration = int(duration) / distributionGranularity\n    runDuration = float(runDuration)\n    if int(distributionGranularity) == 1:\n        stressValues.append(startLoad)\n        runStartTimeList.append(startTimesec)\n        runDurations.append(runDuration)\n        return stressValues, runStartTimeList, runDurations, triggerType\n    else:\n        stressValues = map(int, linspace(startLoad, stopLoad,\n            distributionGranularity))\n        runStartTimeList = map(int, linspace(startTimesec, startTimesec +\n            duration - runDuration, distributionGranularity))\n        runDurations = [runDuration] * distributionGranularity\n        if resType.lower() == 'mem':\n            [mallocSplit(i, stressValues, runStartTimeList, runDurations) for\n                i, load in enumerate(stressValues) if load > MALLOC_LIMIT]\n        return stressValues, runStartTimeList, runDurations, triggerType\n", "import_code": ["from numpy import linspace\n"]}
{"id": "ca2023bd-f0c9-3ab2-9991-4b5842363f9f_3", "content": "def argNames(Rtype=None):\n    \"\"\"\n    We specify how many arguments distribution instance require to run properly\n    Rtype = <CPU,MEM,IO,NET>\n    IMPORTANT: All argument variable names must be in lower case\n    \"\"\"\n    if Rtype == None:\n        argNames = ['cpu', 'mem', 'io', 'net']\n        return argNames\n    if Rtype.lower() == 'cpu':\n        argNames = [('duration', {'upperBound': 100000, 'lowerBound': 0,\n            'argHelp': \"\"\"Time Distribution lasts for.\nUnits: seconds\"\"\"}),\n            ('granularity', {'upperBound': 100000, 'lowerBound': 0,\n            'argHelp': 'Number of runs to create'}), ('minJobTime', {\n            'upperBound': 10000000, 'lowerBound': 2, 'argHelp':\n            \"\"\"Minimum time a single job's duration can be (any jobs under will be deleted).\nUnits: seconds (Min 2)\"\"\"\n            }), ('startload', {'upperBound': 100, 'lowerBound': 0,\n            'argHelp': \"\"\"Value for distribution to start at.\nUnits: %\"\"\"}),\n            ('stopload', {'upperBound': 100, 'lowerBound': 0, 'argHelp':\n            \"\"\"Value for distribution to stop at.\nUnits: %\"\"\"})]\n        return OrderedDict(argNames)\n    if Rtype.lower() == 'mem':\n        memReading = psutil.phymem_usage()\n        allMemory = memReading.total / 1048576\n        argNames = [('duration', {'upperBound': 100000, 'lowerBound': 1,\n            'argHelp': \"\"\"Time Distribution lasts for.\nUnits: seconds\"\"\"}),\n            ('granularity', {'upperBound': 100000, 'lowerBound': 1,\n            'argHelp': 'Number of runs to create'}), ('minJobTime', {\n            'upperBound': 10000000, 'lowerBound': 2, 'argHelp':\n            \"\"\"Minimum time a single job's duration can be (any jobs under will be deleted).\nUnits: seconds (Min 2)\"\"\"\n            }), ('startload', {'upperBound': allMemory, 'lowerBound': 50,\n            'argHelp':\n            \"\"\"Value for distribution to begin at.\nUnits: MB or %\ne.g. '10' (for 10MB) or '10%'\"\"\"\n            }), ('stopload', {'upperBound': allMemory, 'lowerBound': 50,\n            'argHelp':\n            \"\"\"Value for distribution to stop at.\nUnits: MB or %\ne.g. '10' (for 10MB) or '10%'\"\"\"\n            })]\n        return OrderedDict(argNames)\n    if Rtype.lower() == 'io':\n        argNames = [('duration', {'upperBound': 100000, 'lowerBound': 1,\n            'argHelp': \"\"\"Time Distribution lasts for.\nUnits: seconds\"\"\"}),\n            ('granularity', {'upperBound': 100000, 'lowerBound': 1,\n            'argHelp': 'Number of runs to create'}), ('minJobTime', {\n            'upperBound': 10000000, 'lowerBound': 2, 'argHelp':\n            \"\"\"Minimum time a single job's duration can be (any jobs under will be deleted).\nUnits: seconds (Min 2)\"\"\"\n            }), ('startload', {'upperBound': 999999, 'lowerBound': 1,\n            'argHelp':\n            'Number of files to write to when distribution begins.'}), (\n            'stopload', {'upperBound': 999999, 'lowerBound': 1, 'argHelp':\n            'Number of files to write to when distribution ends.'})]\n        return OrderedDict(argNames)\n    if Rtype.lower() == 'net':\n        argNames = [('duration', {'upperBound': 100000, 'lowerBound': 1,\n            'argHelp': \"\"\"Time Distribution lasts for.\nUnits: seconds\"\"\"}),\n            ('granularity', {'upperBound': 100000, 'lowerBound': 1,\n            'argHelp': 'Number of runs to create'}), ('minJobTime', {\n            'upperBound': 10000000, 'lowerBound': 2, 'argHelp':\n            \"\"\"Minimum time a single job's duration can be (any jobs under will be deleted).\nUnits: seconds (Min 2)\"\"\"\n            }), ('startload', {'upperBound': 999999, 'lowerBound': 1,\n            'argHelp':\n            \"\"\"Value for distribution to begin at.\nUnits: MB/s throughput\"\"\"\n            }), ('stopload', {'upperBound': 999999, 'lowerBound': 1,\n            'argHelp':\n            \"\"\"Value for distribution to end at.\nUnits: MB/s throughput\"\"\"})]\n        return OrderedDict(argNames)\n", "import_code": ["import math, psutil\n", "from collections import OrderedDict\n"]}
{"id": "d2179c83-65e2-3e46-bb51-b092118c0978_0", "content": "def get_K(b2):\n    \"\"\"\n    This will be a pretty simple function. Use steady-state equilibrium\n    market clearing condition for the capital market (25) to return\n    aggregate capital K as a function of savings b2.\n\n    Parameters\n    ----------\n    b2 : numeric\n        value for b_2\n\n    Returns\n    -------\n    K : numeric\n        returns value for K\n    \"\"\"\n    K = b2\n    return K\n", "import_code": []}
{"id": "d2179c83-65e2-3e46-bb51-b092118c0978_1", "content": "def get_L(nvec):\n    \"\"\"\n    This function will also be a simple function. Use steady-state\n    equilibrium market clearing condition for the labor market (24) to\n    return aggregate labor L as a function of nvec.\n\n    Parameters\n    ----------\n    nvec : np.array\n        1 dimensional vector with n1 and n2\n\n    Returns\n    -------\n    L : numeric\n        returns value for L\n    \"\"\"\n    L = np.sum(nvec)\n    return L\n", "import_code": ["import numpy as np\n"]}
{"id": "d2179c83-65e2-3e46-bb51-b092118c0978_2", "content": "def get_r(b2, args):\n    \"\"\"\n    This function computes the steady-state interest rate as a function\n    of savings b2 using steady-state equilibrium equation (28).\n\n    Parameters\n    ----------\n    b2 : numeric\n        Value for b_2 variable\n    args : tuple\n        Tuple ordered as:\n            - nvec: np.array (1 dimension) with n1 and n2\n            - alpha: numeric [0,1]\n            - A: numeric\n            - delta: numeric\n    Returns\n    -------\n    r : numeric\n        returns value for r\n    \"\"\"\n    nvec, alpha, A, delta, beta, gamma = args\n    K = get_K(b2)\n    L = get_L(nvec)\n    r = alpha * A * (L / K) ** (1 - alpha) - delta\n    return r\n", "import_code": []}
{"id": "d2179c83-65e2-3e46-bb51-b092118c0978_3", "content": "def get_w(b2, args):\n    \"\"\"\n    This function computes the steady-state wage as a function of\n    savings b2 using steady-state equilibrium equation (29).\n\n    Parameters\n    ----------\n    b2 : numeric\n        Value for b_2 variable\n    args : tuple\n        Tuple ordered as:\n            - nvec: np.array (1 dimension) with n1 and n2\n            - alpha: numeric [0,1]\n            - A: numeric\n    Returns\n    -------\n    w : numeric\n        returns value for w\n    \"\"\"\n    nvec, alpha, A, delta, beta, gamma = args\n    K = get_K(b2)\n    L = get_L(nvec)\n    w = (1 - alpha) * A * (K / L) ** alpha\n    return w\n", "import_code": []}
{"id": "288bf363-a1a2-36ee-84b6-17941ae0f6f5_0", "content": "def cut_rule(rule):\n    result = ''\n    for i in re.findall('(/[^/]*)', rule):\n        if re.match('/\\\\<int\\\\>', i):\n            result += '/(\\\\d+)'\n        elif re.match('/\\\\<str\\\\>', i):\n            result += '/(\\\\w+)'\n        elif re.match('/\\\\<path\\\\>', i):\n            result += '/([\\\\w/.]*)'\n        else:\n            result += i\n    return result + '$'\n", "import_code": ["import re\n"]}
{"id": "288bf363-a1a2-36ee-84b6-17941ae0f6f5_1", "content": "def get_rule(rule_list):\n    resultMap = collections.OrderedDict()\n    for r in rule_list:\n        resultMap[r.split(' ')[1]] = cut_rule(r.split(' ')[0])\n    return resultMap\n", "import_code": ["import collections\n"]}
{"id": "44d1d477-d4fd-367e-834e-d0054fccfc60_0", "content": "def text_tokenizer(text):\n    \"\"\"Return two array of tokenized lower case words with and without\n    stopwords removed.\n\n    Parameters\n    ----------\n    text: string\n    \"\"\"\n    tokenizer = RegexpTokenizer('\\\\w+')\n    text = text.lower().replace(\"'\", '')\n    all_tokens = tokenizer.tokenize(text)\n    without_stopwords = [word for word in all_tokens if not word in STOPWORDS]\n    return np.array(all_tokens), np.array(without_stopwords)\n", "import_code": ["import numpy as np\n", "from nltk.tokenize import RegexpTokenizer\n"]}
{"id": "358311e3-a728-35a6-af51-691a53fd97b1_2", "content": "def init_db(db_path):\n    \"\"\"\n    This function initializes the database and creates the necessary tables.\n    \"\"\"\n    print('initizing database...')\n    db_conn = sqlite3.connect(db_path)\n    db_conn.executescript(\n        \"\"\"create table document (\n docid integer primary key, \n title text not null,\n content text not null,\n kw_title text,\n kw_content text,\n cats text not null,\n file_path text not null);\n\ncreate table sdocument (\n docid integer primary key, \n title text not null,\n content text not null,\n kw_title text,\n kw_content text,\n cats text not null,\n file_path text not null);\n\n create table cate (name text,\n docid integer not null)\n\"\"\"\n        )\n    return db_conn\n", "import_code": ["import sqlite3\n"]}
{"id": "358311e3-a728-35a6-af51-691a53fd97b1_5", "content": "def process_doc(news_file, cat_set):\n    \"\"\"\n    This function processes a document by loading it, segmenting the words, and pos tagging them.\n    \"\"\"\n    title, content = load_doc(news_file)\n    title, content = segment_word(title, content)\n    return title, content\n", "import_code": []}
{"id": "358311e3-a728-35a6-af51-691a53fd97b1_6", "content": "def load_doc(news_file):\n    \"\"\"\n    This function loads a document from a file.\n    \"\"\"\n    f = open(news_file)\n    title = f.readline()\n    content = f.read()\n    f.close()\n    return title, content\n", "import_code": []}
{"id": "358311e3-a728-35a6-af51-691a53fd97b1_7", "content": "def pos_text(text):\n    \"\"\"\n    This function tokenizes the text, POS tags the tokens, and selects and stems/lemmatizes the words.\n    \"\"\"\n    tokens = nltk.word_tokenize(text)\n    wordtags = nltk.pos_tag(tokens)\n    stemmer = nltk.PorterStemmer()\n    selectedwords = []\n    patn = re.compile('[^a-zA-Z\\\\-]')\n    for word, tag in wordtags:\n        word = patn.sub('', word)\n        lword = word.lower()\n        if lword in stopword_set or len(word) <= 1:\n            continue\n        if tag[0] == 'N':\n            nword = stemmer.stem_word(lword)\n            selectedwords.append((nword, word, 'N'))\n        elif tag[0] == 'V':\n            nword = stemmer.stem_word(lword)\n            selectedwords.append((nword, word, 'V'))\n        elif tag[0] == 'J':\n            nword = stemmer.stem_word(lword)\n            selectedwords.append((nword, word, 'J'))\n    return selectedwords\n", "import_code": ["import nltk\n", "from nltk import snowball\n", "import os, sys, re\n"]}
{"id": "358311e3-a728-35a6-af51-691a53fd97b1_8", "content": "def segment_word(title, content):\n    \"\"\"\n    This function segments the title and content into words, POS tags them, and returns the results.\n    \"\"\"\n    title_words = pos_text(title)\n    content_words = pos_text(content)\n    return title_words, content_words\n", "import_code": []}
{"id": "96c0696e-e694-3a82-9ce7-edb9301a3697_0", "content": "def http_get_request(url):\n    headers = {'User-Agent':\n        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:75.0) Gecko/20100101 Firefox/75.0'\n        , 'Accept':\n        'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n        , 'Accept-Language':\n        'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',\n        'Connection': 'keep-alive', 'Upgrade-Insecure-Requests': '1',\n        'Pragma': 'no-cache', 'Cache-Control': 'no-cache'}\n    resp = requests.get(url, headers=headers)\n    if resp.status_code != 200:\n        return False\n    return resp\n", "import_code": ["import requests\n"]}
{"id": "96c0696e-e694-3a82-9ce7-edb9301a3697_2", "content": "def get_IP_address_of_domain(domain, myip=None):\n    primary_domain = domain\n    domain_part = primary_domain.split('.')\n    if len(domain_part) > 2:\n        primary_domain = '.'.join(domain_part[-2:])\n    if primary_domain == domain:\n        url = 'https://' + primary_domain + '.ipaddress.com/'\n    else:\n        url = 'https://' + primary_domain + '.ipaddress.com/' + domain\n    data = http_get_request(url)\n    if not data:\n        return\n    else:\n        data = data.text\n    re_compile = (\n        '<a href=\\\\\"https://www.ipaddress.com/ipv4/([\\\\d]{1,3}.[\\\\d]{1,3}.[\\\\d]{1,3}.[\\\\d]{1,3})'\n        )\n    pattern = re.compile(re_compile)\n    result = pattern.findall(data)\n    return result\n", "import_code": ["import re\n", "import requests\n"]}
{"id": "a1c10cb1-8714-38f6-a52d-9487bc1c8922_0", "content": "def hash_message(consumer_secret, path, body_message):\n    \"\"\"\n    This function hashes a given message using HMAC-SHA1 algorithm.\n\n    Args:\n        consumer_secret (str): The consumer secret of client application.\n        path (str): The endpoint of the method.\n        body_message (str): A string of the message body.\n\n    Returns:\n        str: The hash string of the message.\n    \"\"\"\n    message = path + json.dumps(body_message)\n    encrypted_content = hmac.new(consumer_secret.encode(), msg=message.\n        encode(), digestmod=hashlib.sha1)\n    hashed_value = encrypted_content.hexdigest()\n    return hashed_value\n", "import_code": ["import hmac\n", "import hashlib\n", "import json\n"]}
{"id": "a1c10cb1-8714-38f6-a52d-9487bc1c8922_1", "content": "def is_an_allowed_file_extension(file_name):\n    \"\"\"\n    This function checks if a given file belongs to allowed extensions.\n\n    Args:\n        file_name (str): The file name.\n\n    Returns:\n        bool: True if the file extension is allowed, False otherwise.\n    \"\"\"\n    if '.' in file_name:\n        if file_name.split('.')[1] in ALLOWED_EXTENSIONS:\n            return True\n    return False\n", "import_code": []}
{"id": "a1c10cb1-8714-38f6-a52d-9487bc1c8922_2", "content": "def create_directory_tree(directory, cache_directory_depth, file_name):\n    \"\"\"\n    This function creates a directory tree.\n\n    Args:\n        directory (str): The path of the directory.\n        cache_directory_depth (int): The number of the directory depth.\n        file_name (str): The file name.\n\n    Returns:\n        str: The directory path.\n    \"\"\"\n    depth_lst = list(file_name[:cache_directory_depth])\n    for char in depth_lst:\n        directory = os.path.join(directory, char)\n        os.makedirs(directory, exist_ok=True)\n    return directory\n", "import_code": ["import os\n"]}
{"id": "a1c10cb1-8714-38f6-a52d-9487bc1c8922_3", "content": "def remove_photos_from_directory(directory, filename_list):\n    \"\"\"\n    This function removes a file from a directory.\n\n    Args:\n        directory (str): The directory path where the file is stored.\n        filename_list (list): A list of file names which should be removed.\n\n    Returns:\n        int: The number of files that were removed.\n    \"\"\"\n    count = 0\n    for root, dirs, files in os.walk(directory):\n        for image_name in files:\n            if image_name.split('.')[0] in filename_list:\n                count += 1\n                photo_path = os.path.join(root, image_name)\n                print('photo_path', photo_path)\n                os.remove(photo_path)\n    return count\n", "import_code": ["import os\n"]}
{"id": "30711fde-57a0-33fe-a085-a464c5e81655_1", "content": "def getWebPageContent(webUrl):\n    headers = getDynamicHeader()\n    webpage = requests.get(webUrl, headers=headers, verify=False).text\n    soup = BeautifulSoup(webpage, 'lxml')\n    return soup\n", "import_code": ["from bs4 import BeautifulSoup\n", "import requests\n"]}
{"id": "30711fde-57a0-33fe-a085-a464c5e81655_2", "content": "def getEmail(emailContent):\n    emailRegEx = re.compile('([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+)'\n        )\n    m = emailRegEx.search(emailContent)\n    if m:\n        email = m.group(0)\n    return email\n", "import_code": ["import requests\n", "import re\n", "from collections import OrderedDict\n"]}
{"id": "30711fde-57a0-33fe-a085-a464c5e81655_3", "content": "def getWebSite(webContent):\n    webRegex = re.compile(\n        '(?i)\\\\b((?:https?://|www\\\\d{0,3}[.]|[a-z0-9.\\\\-]+[.][a-z]{2,4}/)(?:[^\\\\s()<>]+|\\\\(([^\\\\s()<>]+|(\\\\([^\\\\s()<>]+\\\\)))*\\\\))+(?:\\\\(([^\\\\s()<>]+|(\\\\([^\\\\s()<>]+\\\\)))*\\\\)|[^\\\\s`!()\\\\[\\\\]{};:\\'\\\\\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))'\n        )\n    m = webRegex.search(webContent)\n    if m:\n        website = m.group(0)\n    return website\n", "import_code": ["import requests\n", "import re\n", "from collections import OrderedDict\n"]}
{"id": "30711fde-57a0-33fe-a085-a464c5e81655_4", "content": "def formatAddressContent(companyAddress):\n    companyAddress = companyAddress.replace('\\t', '')\n    companyAddress = companyAddress.replace('\\r', '')\n    companyAddress = companyAddress.replace('\\n', '')\n    companyAddress = companyAddress.strip()\n    return companyAddress\n", "import_code": []}
{"id": "30711fde-57a0-33fe-a085-a464c5e81655_5", "content": "def soupContentSelect(soupContent, htmlDom):\n    contactInfo = soupContent.select(htmlDom)\n    return contactInfo\n", "import_code": []}
{"id": "30711fde-57a0-33fe-a085-a464c5e81655_6", "content": "def getAllHrefLinks(soupContent, linkPathPrefix, findFromContent=False):\n    if findFromContent == True:\n        filtered = soupContent.select(linkPathPrefix)\n    else:\n        filtered = []\n        for linkContent in soupContent:\n            if linkPathPrefix:\n                filtered.append(linkContent.find('a', href=re.compile(\n                    linkPathPrefix)))\n            else:\n                filtered.append(linkContent.find('a'))\n    newList = []\n    for link in filtered:\n        try:\n            newList.append(link.get('href'))\n        except:\n            continue\n    newList = list(dict.fromkeys(newList))\n    return newList\n", "import_code": ["import requests\n", "import re\n", "from collections import OrderedDict\n"]}
{"id": "30711fde-57a0-33fe-a085-a464c5e81655_7", "content": "def getHrefText(link):\n    if link:\n        link.get('href')\n        return link\n", "import_code": []}
{"id": "fadb459c-031b-3738-91f2-c836382efe14_1", "content": "def fk(x, k):\n    return (-x) ** k / float(factorial(k))\n", "import_code": ["from math import e, cos, factorial, pi\n"]}
{"id": "fadb459c-031b-3738-91f2-c836382efe14_2", "content": "def S(x, N, M):\n    s = 0\n    for k in range(N, M):\n        s += fk(x, k)\n    return s\n", "import_code": []}
{"id": "fadb459c-031b-3738-91f2-c836382efe14_3", "content": "def animate(k):\n    x = np.linspace(0, 15, 256)\n    y = fk(x, k)\n    line.set_data(x, y)\n    return line,\n", "import_code": ["import numpy as np\n"]}
{"id": "4cda5c00-dc43-359f-b1d0-9495951c555e_1", "content": "def list_of_lists(n):\n    return [[(i + 1) for i in xrange(x + 1)] for x in xrange(n)]\n", "import_code": []}
{"id": "ee8bacf4-f1ee-34bd-af99-dbad48a7b0fe_0", "content": "def stress_cpu(num_cpus, time):\n    subprocess.check_call(['stress', '--cpu', str(num_cpus), '--timeout',\n        '{}s'.format(time)])\n    return\n", "import_code": ["import subprocess\n"]}
{"id": "ee8bacf4-f1ee-34bd-af99-dbad48a7b0fe_1", "content": "def cooldown(interval=60, filename=None):\n    \"\"\"Lets the CPU cool down until the temperature does not change anymore.\n    \"\"\"\n    prev_tmp = measure_temp(filename=filename)\n    while True:\n        tme.sleep(interval)\n        tmp = measure_temp(filename=filename)\n        print(\n            'Current temperature: {:4.1f}\u00b0C - Previous temperature: {:4.1f}\u00b0C'\n            .format(tmp, prev_tmp))\n        if abs(tmp - prev_tmp) < 0.2:\n            break\n        prev_tmp = tmp\n    return tmp\n", "import_code": ["import time as tme\n"]}
{"id": "ee8bacf4-f1ee-34bd-af99-dbad48a7b0fe_2", "content": "def measure_temp(filename=None):\n    \"\"\"Returns the core temperature in Celsius.\n    \"\"\"\n    if filename is not None:\n        with open(filename, 'r') as f:\n            temp = float(f.read()) / 1000\n    else:\n        sys_tempfile = open('/sys/class/thermal/thermal_zone0/temp', 'r')\n        sys_temp = sys_tempfile.read()\n        sys_tempfile.close()\n        temp_int = int(sys_temp)\n        temp = float(temp_int / 1000)\n    return temp\n", "import_code": []}
{"id": "ee8bacf4-f1ee-34bd-af99-dbad48a7b0fe_3", "content": "def measure_core_frequency(filename=None):\n    \"\"\"Returns the CPU frequency in MHz\n    \"\"\"\n    if filename is not None:\n        with open(filename, 'r') as f:\n            frequency = float(f.read()) / 1000\n    else:\n        sys_freqfile = open(\n            '/sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq', 'r')\n        sys_freq = sys_freqfile.read()\n        sys_freqfile.close()\n        freq_int = int(sys_freq)\n        frequency = freq_int / 1000\n    return frequency\n", "import_code": []}
{"id": "ee8bacf4-f1ee-34bd-af99-dbad48a7b0fe_4", "content": "def test(stress_duration, idle_duration, cores):\n    \"\"\"Run stress test for specified duration with specified idle times\n       at the start and end of the test.\n    \"\"\"\n    if cores is None:\n        cores = cpu_count()\n    print('Preparing to stress [{}] CPU Cores for [{}] seconds'.format(\n        cores, stress_duration))\n    print('Idling for {} seconds...'.format(idle_duration))\n    tme.sleep(idle_duration)\n    stress_cpu(num_cpus=cores, time=stress_duration)\n    print('Idling for {} seconds...'.format(idle_duration))\n    tme.sleep(idle_duration)\n    return\n", "import_code": ["import time as tme\n", "from os import cpu_count\n"]}
{"id": "3ac88693-ce41-3526-b27b-b8f4440bba02_1", "content": "def bytes2human(n):\n    symbols = 'K', 'M', 'G', 'T', 'P', 'E', 'Z', 'Y'\n    prefix = {}\n    for i, s in enumerate(symbols):\n        prefix[s] = 1 << (i + 1) * 10\n    for s in reversed(symbols):\n        if n >= prefix[s]:\n            value = float(n) / prefix[s]\n            return '%.1f%s' % (value, s)\n    return '%sB' % n\n", "import_code": []}
{"id": "e5cedc80-bb2c-3b42-9538-83b8b0a05c62_1", "content": "def sanitize_endline(line):\n    return ROGUE_NL.sub('\\n', line)\n", "import_code": []}
{"id": "e5cedc80-bb2c-3b42-9538-83b8b0a05c62_2", "content": "def sanitize_tabs(line):\n    matches = INDENTED_LINE.match(line)\n    indent = ''\n    for char in matches.group('indent'):\n        if char == '\\t':\n            indent += TAB_SPACES\n        else:\n            indent += char\n    return indent + matches.group('content')\n", "import_code": []}
{"id": "ac058a31-ba6f-3f0a-9590-4b7c18fcd041_2", "content": "def _provider_slug(repo_url):\n    \"\"\"Get the code hosting provider for the current CircleCI build.\n\n    Args:\n        repo_url (str): The URL of a code hosting repository.\n\n    Returns:\n        Tuple[CircleCIRepoProvider, str]: Pair of the code hosting provider\n            for the current CircleCI build and the repository slug.\n\n    Raises:\n        ValueError: If ``repo_url`` contains the GitHub host but\n            does not start with the corresponding expected prefix.\n        ValueError: If ``repo_url`` contains the Bitbucket host but\n            does not start with the corresponding expected prefix.\n        ValueError: If ``repo_url`` doesn't match either the GitHub\n            or Bitbucket hosts.\n    \"\"\"\n    if _GITHUB_HOST in repo_url:\n        if repo_url.startswith(_GITHUB_PREFIX):\n            _, slug = repo_url.split(_GITHUB_PREFIX, 1)\n            return CircleCIRepoProvider.github, slug\n        else:\n            raise ValueError('Repository URL contained host', _GITHUB_HOST,\n                'but did not begin as expected', 'expected prefix',\n                _GITHUB_PREFIX)\n    elif _BITBUCKET_HOST in repo_url:\n        if repo_url.startswith(_BITBUCKET_PREFIX):\n            _, slug = repo_url.split(_BITBUCKET_PREFIX, 1)\n            return CircleCIRepoProvider.bitbucket, slug\n        else:\n            raise ValueError('Repository URL contained host',\n                _BITBUCKET_HOST, 'but did not begin as expected',\n                'expected prefix', _BITBUCKET_PREFIX)\n    else:\n        raise ValueError('Invalid repo URL', repo_url,\n            'Expected a URL for one of', [enum_val.name for enum_val in\n            CircleCIRepoProvider])\n", "import_code": []}
{"id": "98e9443e-1105-36ce-a666-d1603c3785f4_1", "content": "def rfc3339_2_epochms(rfc3339_str):\n    utc_dt = datetime.strptime(rfc3339_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n    timestamp = (utc_dt - datetime(1970, 1, 1)).total_seconds()\n    return int(timestamp * 1000)\n", "import_code": ["from datetime import datetime, timedelta\n"]}
{"id": "98e9443e-1105-36ce-a666-d1603c3785f4_3", "content": "def amagipldt_2_rfc3339(amagipldt_str):\n    _date, _time, _utcoff = amagipldt_str.split(' ')\n    rfc3339_str = _date + 'T' + _time + 'Z'\n    return rfc3339_str\n", "import_code": []}
{"id": "98e9443e-1105-36ce-a666-d1603c3785f4_4", "content": "def amagipldt_2_epochms(amagipldt_str):\n    return rfc3339_2_epochms(amagipldt_2_rfc3339(amagipldt_str))\n", "import_code": []}
{"id": "98e9443e-1105-36ce-a666-d1603c3785f4_5", "content": "def epochms_2_rfc3339(timestamp_ms):\n    us = timestamp_ms * 1000\n    utc_dt = datetime(1970, 1, 1) + timedelta(microseconds=us)\n    return utc_dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'\n", "import_code": ["from datetime import datetime, timedelta\n", "from datetime import datetime, timedelta\n"]}
{"id": "98e9443e-1105-36ce-a666-d1603c3785f4_6", "content": "def get_json_obj(filepath):\n    json_obj = None\n    try:\n        with open(filepath) as fp:\n            json_obj = json.load(fp)\n    except Exception as e:\n        eprint(e)\n    return json_obj\n", "import_code": ["import json\n"]}
{"id": "5c6c193a-f9b4-3916-b23b-8e39351a168e_0", "content": "def decrypt(word):\n    decoded = ''\n    for letter in word:\n        try:\n            decoded += code[letter]\n        except KeyError:\n            decoded += letter\n    return decoded\n", "import_code": []}
{"id": "5c6c193a-f9b4-3916-b23b-8e39351a168e_2", "content": "def get_ctext(filename):\n    data = ''\n    with open(filename, 'r') as f:\n        data = f.read()\n    return data\n", "import_code": []}
{"id": "3278f9e5-0dbc-373c-8b1d-00f6602f267d_0", "content": "def load_settings(settingsFile):\n    \"\"\"\n    Reads settings file into a list.\n    :param settingsFile: the name of the settings file e.g. ``'settings.txt'``\n    :type settingsFile: string\n    :returns: the newly-created list\n    \"\"\"\n    with open(settingsFile, 'r') as fileHandle:\n        settings = fileHandle.readlines()\n    fileHandle.close()\n    return settings\n", "import_code": []}
{"id": "3278f9e5-0dbc-373c-8b1d-00f6602f267d_4", "content": "def createRoom(roomTitle):\n    url = 'https://api.ciscospark.com/v1/rooms'\n    header = {'Content-type': 'application/json', 'Authorization': \n        'Bearer ' + token}\n    payload = {'title': roomTitle}\n    r = requests.post(url, headers=header, data=json.dumps(payload))\n    if r.status_code == 200:\n        return 'Create Room {} Succeed'.format(roomTitle)\n    else:\n        return 'Create Room Failed'\n", "import_code": ["import requests\n", "import json\n"]}
{"id": "3278f9e5-0dbc-373c-8b1d-00f6602f267d_5", "content": "def addMember(personEmail):\n    url = 'https://api.ciscospark.com/v1/memberships'\n    header = {'Content-type': 'application/json; charset=utf-8',\n        'Authorization': 'Bearer ' + token}\n    payload = {'roomId':\n        'Y2lzY29zcGFyazovL3VzL1JPT00vMGU0MGRhMjAtMGQ1Mi0xMWU3LTg0ODAtYzcyN2M5YzlmYzA2'\n        , 'personEmail': 'r2d2@example.com'}\n    r = requests.post(url, headers=header, data=json.dumps(payload))\n    if r.status_code == 200:\n        return 'Add {} Succeed'.format(personEmail)\n    else:\n        return 'Add Member Failed'\n", "import_code": ["import requests\n", "import json\n"]}
{"id": "3278f9e5-0dbc-373c-8b1d-00f6602f267d_6", "content": "def createTeam(teamName):\n    url = 'https://api.ciscospark.com/v1/teams'\n    header = {'Content-type': 'application/json; charset=utf-8',\n        'Authorization': 'Bearer ' + token}\n    payload = {'name': teamName}\n    r = requests.post(url, headers=header, data=json.dumps(payload))\n    if r.status_code == 200:\n        return 'Create Team {} Succeed'.format(teamName)\n    else:\n        return 'Create Team Failed'\n", "import_code": ["import requests\n", "import json\n"]}
{"id": "3278f9e5-0dbc-373c-8b1d-00f6602f267d_8", "content": "def postMessage(message, roomid):\n    url = 'https://api.ciscospark.com/v1/messages'\n    payload = {'roomId': roomid, 'text': message}\n    header = {'Content-type': 'application/json; charset=utf-8',\n        'Authorization': 'Bearer ' + token}\n    r = requests.post(url, headers=header, data=json.dumps(payload))\n    if r.status_code == 200:\n        return 'Success'\n    else:\n        return 'Failed'\n", "import_code": ["import requests\n", "import json\n"]}
{"id": "3278f9e5-0dbc-373c-8b1d-00f6602f267d_9", "content": "def postPicture(picurl, roomid):\n    url = 'https://api.ciscospark.com/v1/messages'\n    payload = {'roomId': roomid, 'file': picurl}\n    header = {'Content-type': 'application/json; charset=utf-8',\n        'Authorization': 'Bearer ' + token}\n    r = requests.post(url, headers=header, data=json.dumps(payload))\n    if r.status_code == 200:\n        return 'Success'\n    else:\n        return 'Failed'\n", "import_code": ["import requests\n", "import json\n"]}
{"id": "e432d634-cdbb-39e8-a1ed-92f4ebb6a5f5_0", "content": "def hash_input(val):\n    return hashlib.sha256(val.encode()).hexdigest()\n", "import_code": ["import hashlib\n"]}
{"id": "e432d634-cdbb-39e8-a1ed-92f4ebb6a5f5_1", "content": "def length(val):\n    return 8 <= len(val) <= 32\n", "import_code": []}
{"id": "e432d634-cdbb-39e8-a1ed-92f4ebb6a5f5_2", "content": "def spaces(val):\n    if val[0] == ' ' or val[len(val) - 1] == ' ':\n        return False\n    for idx in range(len(val)):\n        if chr(13) == val[idx] or chr(10) == val[idx] or '/' == val[idx\n            ] or '\\\\' == val[idx]:\n            return False\n    return True\n", "import_code": []}
{"id": "e432d634-cdbb-39e8-a1ed-92f4ebb6a5f5_3", "content": "def consecutiveChars(val):\n    for idx in range(len(val) - 2):\n        if val[idx] == val[idx + 1] and val[idx + 1] == val[idx + 2]:\n            return False\n    return True\n", "import_code": []}
{"id": "e432d634-cdbb-39e8-a1ed-92f4ebb6a5f5_4", "content": "def upperLower(val):\n    return re.search('[A-Z]', val) is not None and re.search('[a-z]', val\n        ) is not None\n", "import_code": ["import re\n"]}
{"id": "e432d634-cdbb-39e8-a1ed-92f4ebb6a5f5_5", "content": "def number(val):\n    return re.search('[1-9]', val) is not None\n", "import_code": ["import re\n"]}
{"id": "e432d634-cdbb-39e8-a1ed-92f4ebb6a5f5_6", "content": "def special(val):\n    return re.search('[!@#$%&*()]', val) is not None\n", "import_code": ["import re\n"]}
{"id": "4df547db-620c-3666-bedf-46b5cc1e8d04_1", "content": "def gen_L0_octave_code(Lambda):\n    p1 = \"\"\"\n    Im = imread(input_path);\n    [dir, name, ext] = fileparts(input_path);\n    \"\"\"\n    p2 = 'S = L0Smoothing(Im,' + str(Lambda) + ');'\n    p3 = \"\"\"\n    write_name = strrep(input_path, dir, output_path);\n    [filepath,~,~] = fileparts(write_name);\n    if ~exist(filepath, 'dir')\n        mkdir(filepath);\n    end\n    imwrite(S, write_name);    \n    \"\"\"\n    octave_code = p1 + p2 + p3\n    return octave_code\n", "import_code": []}
{"id": "101d402a-9d14-3c0c-bb4a-b8c5b332d97c_0", "content": "def basic_stats(people):\n    \"\"\"Returns the mean, median, and mode of a list of Person\"\"\"\n    ages = []\n    for i in range(0, len(people)):\n        ages.append(people[i].get_age())\n    mean = stats.mean(ages)\n    median = stats.median(ages)\n    mode = stats.mode(ages)\n    return mean, median, mode\n", "import_code": ["import statistics as stats\n"]}
{"id": "04abf0da-e7e7-35ec-9d70-061d6b3c65b7_0", "content": "def f1(string):\n    arr = [int(s) for s in string]\n    arr = np.insert(arr, 0, 0)\n    return np.abs(np.diff(arr)).sum()\n", "import_code": ["import numpy as np\n"]}
{"id": "04abf0da-e7e7-35ec-9d70-061d6b3c65b7_1", "content": "def f2(string):\n    arr = [int(s) for s in string]\n    return np.abs(np.diff(arr)).sum()\n", "import_code": ["import numpy as np\n"]}
{"id": "04abf0da-e7e7-35ec-9d70-061d6b3c65b7_2", "content": "def perm(s):\n    key = str(s)\n    res = []\n    if len(s) == 1:\n        return [s]\n    if key in cache:\n        return cache[key]\n    else:\n        for i, c in enumerate(s):\n            for p in perm(s[:i] + s[i + 1:]):\n                res += [[c] + p]\n    cache[key] = res\n    return res\n", "import_code": []}
{"id": "29c8a006-6e9a-363d-a50d-a7dae83ab748_0", "content": "def is_valid_password(password):\n    \"\"\"Determine if the provided password is valid.\"\"\"\n    if len(password) < password_length:\n        return False\n    count_lower = 0\n    count_upper = 0\n    count_digit = 0\n    count_special = 0\n    for char in password:\n        if char.islower():\n            count_lower += 1\n        if char.isupper():\n            count_upper += 1\n        if char.isdigit():\n            count_digit += 1\n        if char in SPECIAL_CHARACTERS:\n            count_special += 1\n    if (count_lower < num_of_lower or count_upper < num_of_upper or \n        count_digit < num_of_numeric or count_special < num_of_special_char):\n        return False\n    return True\n", "import_code": []}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_0", "content": "def util_task_eq_setup(input):\n    try:\n        return input is not None and re.match('^setup$', str(input).strip(),\n            re.I)\n    except:\n        return False\n", "import_code": ["from __future__ import absolute_import, division, print_function\n", "import re\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_1", "content": "def util_task_eq_teardown(input):\n    try:\n        return input is not None and re.match('^teardown$', str(input).\n            strip(), re.I)\n    except:\n        return False\n", "import_code": ["from __future__ import absolute_import, division, print_function\n", "import re\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_2", "content": "def util_task_valid(input):\n    try:\n        return input is not None and (util_task_eq_setup(input) or\n            util_task_eq_teardown(input))\n    except:\n        return False\n", "import_code": []}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_3", "content": "def util_lookup_yml_files(casename, taskname, typename):\n    try:\n        if casename is not None:\n            casename = str(casename).strip()\n        else:\n            return []\n        if taskname is not None:\n            taskname = str(taskname).strip().lower()\n        else:\n            return []\n        if typename is not None:\n            typename = str(typename).strip().lower()\n        else:\n            return []\n        lookup_files = []\n        lookup_dir_path = '%s/%s' % (TESTS_ROOT_PATH, LOOKUP_DIR_NAME)\n        lookup_dir_list = ['%s/all/%s' % (lookup_dir_path, typename), \n            '%s/all/%s/%s' % (lookup_dir_path, typename, taskname), \n            '%s/%s/%s' % (lookup_dir_path, casename, typename), \n            '%s/%s/%s/%s' % (lookup_dir_path, casename, typename, taskname),\n            None]\n        for lookup_dir in lookup_dir_list:\n            if lookup_dir is None:\n                break\n            list_dir_items = os.listdir(lookup_dir) if os.path.isdir(lookup_dir\n                ) else []\n            for dir_item in sorted(list_dir_items):\n                if dir_item is None:\n                    continue\n                if not os.path.isfile(dir_item):\n                    continue\n                if not re.search('\\\\.yml$', str(dir_item), re.I):\n                    continue\n                lookup_files += [dir_item]\n        return lookup_files\n    except:\n        return []\n", "import_code": ["from __future__ import absolute_import, division, print_function\n", "import re\n", "import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_4", "content": "def util_has_yml_files(casename, taskname, typename):\n    return len(util_lookup_yml_files(casename, taskname, typename))\n", "import_code": []}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_5", "content": "def util_lookup_files_dir(casename, taskname):\n    try:\n        if casename is not None:\n            casename = str(casename).strip()\n        else:\n            return []\n        if taskname is not None:\n            taskname = str(taskname).strip().lower()\n        else:\n            return []\n        lookup_dirs = []\n        lookup_dir_path = '%s/%s' % (TESTS_ROOT_PATH, LOOKUP_DIR_NAME)\n        lookup_dir_list = ['%s/all/files/%s' % (lookup_dir_path, taskname),\n            '%s/%s/files/%s' % (lookup_dir_path, casename, taskname), None]\n        for lookup_dir in lookup_dir_list:\n            if lookup_dir is None:\n                break\n            list_dir_items = os.listdir(lookup_dir) if os.path.isdir(lookup_dir\n                ) else []\n            for dir_item in sorted(list_dir_items):\n                if dir_item is None:\n                    continue\n                if os.path.isdir(dir_item):\n                    continue\n                lookup_dirs += [dir_item]\n        return lookup_dirs\n    except:\n        return []\n", "import_code": ["import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_6", "content": "def util_has_files_dir(casename, taskname, typename):\n    return len(util_lookup_files_dir(casename, taskname, typename))\n", "import_code": []}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_7", "content": "def compat_exists(path):\n    try:\n        return path is not None and os.path.exists(path)\n    except:\n        return False\n", "import_code": ["import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_8", "content": "def compat_lexists(path):\n    try:\n        return path is not None and os.path.lexists(path)\n    except:\n        return False\n", "import_code": ["import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_9", "content": "def compat_isdir(path):\n    try:\n        return path is not None and os.path.isdir(path)\n    except:\n        return False\n", "import_code": ["import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_10", "content": "def compat_isfile(path):\n    try:\n        return path is not None and os.path.isfile(path)\n    except:\n        return False\n", "import_code": ["import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_11", "content": "def compat_islink(path):\n    try:\n        return path is not None and os.path.islink(path)\n    except:\n        return False\n", "import_code": ["import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_12", "content": "def compat_isabs(path):\n    try:\n        return path is not None and os.path.isabs(path)\n    except:\n        return False\n", "import_code": ["import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_13", "content": "def compat_ismount(path):\n    try:\n        return path is not None and os.path.ismount(path)\n    except:\n        return False\n", "import_code": ["import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_14", "content": "def compat_samefile(path1, path2):\n    try:\n        return path1 is not None and path2 is not None and os.path.samefile(\n            path1, path2)\n    except:\n        return False\n", "import_code": ["import os\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_15", "content": "def compat_match(input, pattern='', ignorecase=False, multiline=False):\n    try:\n        re_flags = 0\n        re_flags += re.I if ignorecase else 0\n        re_flags += re.M if multiline else 0\n        return input is not None and pattern is not None and re.match(pattern,\n            input, re_flags)\n    except:\n        return False\n", "import_code": ["from __future__ import absolute_import, division, print_function\n", "import re\n"]}
{"id": "26c6e82d-ed53-31b9-947c-74ff4629894a_16", "content": "def compat_search(input, pattern='', ignorecase=False, multiline=False):\n    try:\n        re_flags = 0\n        re_flags += re.I if ignorecase else 0\n        re_flags += re.M if multiline else 0\n        return input is not None and pattern is not None and re.search(pattern,\n            input, re_flags)\n    except:\n        return False\n", "import_code": ["from __future__ import absolute_import, division, print_function\n", "import re\n"]}
{"id": "508f4246-10ff-3f1e-85b6-f2991447562b_0", "content": "def get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n", "import_code": ["from nltk.corpus import wordnet\n", "from nltk import pos_tag\n"]}
{"id": "508f4246-10ff-3f1e-85b6-f2991447562b_1", "content": "def clean_text(text):\n    text = text.strip()\n    text = text.lower()\n    text = [word.strip(string.punctuation) for word in text.split(' ')]\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    stop = stopwords.words('english')\n    text = [x for x in text if x not in stop]\n    text = [t for t in text if len(t) > 2]\n    pos_tags = pos_tag(text)\n    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for\n        t in pos_tags]\n    text = ' '.join(text)\n    return text\n", "import_code": ["from nltk.stem import WordNetLemmatizer\n", "from nltk import pos_tag\n", "import string\n", "from nltk.corpus import stopwords\n"]}
{"id": "508f4246-10ff-3f1e-85b6-f2991447562b_3", "content": "def lemmatization(texts, tags=['NOUN', 'ADJ']):\n    output = []\n    for sent in texts:\n        doc = nlp(' '.join(sent))\n        output.append([token.lemma_ for token in doc if token.pos_ in tags])\n    return output\n", "import_code": []}
{"id": "fd78733b-8381-3a91-88d2-bfe7f040abc9_0", "content": "def word_count(text):\n    return Counter(re.sub('[^A-Za-z ]+', '', text).split(' '))\n", "import_code": ["from collections import Counter\n", "import re\n"]}
{"id": "b935bd75-67d4-36ca-abcf-565e1169dd30_0", "content": "def searchBing(query, num):\n    \"\"\"\n    This function searches Bing for a given query and returns a list of URLs.\n\n    Parameters:\n    query (str): The search query.\n    num (int): The number of URLs to return.\n\n    Returns:\n    list: A list of URLs as strings.\n    \"\"\"\n    url = 'https://www.bing.com/search?q=' + query\n    urls = []\n    page = requests.get(url, headers={'User-agent': 'John Doe'})\n    soup = bs(page.text, 'html.parser')\n    for link in soup.find_all('a'):\n        url = str(link.get('href'))\n        if url.startswith('http') and not url.startswith('http://go.m'\n            ) and not url.startswith('https://go.m'):\n            urls.append(url)\n    return urls[:num]\n", "import_code": ["from bs4 import BeautifulSoup as bs\n", "import requests\n"]}
{"id": "b935bd75-67d4-36ca-abcf-565e1169dd30_1", "content": "def extractText(url):\n    \"\"\"\n    This function extracts the text from a given URL.\n\n    Parameters:\n    url (str): The URL to extract text from.\n\n    Returns:\n    str: The extracted text as a string.\n    \"\"\"\n    page = requests.get(url)\n    soup = bs(page.text, 'html.parser')\n    return soup.get_text()\n", "import_code": ["from bs4 import BeautifulSoup as bs\n", "import requests\n"]}
{"id": "1c65749c-9dac-32ae-8f09-ba8315307e5a_1", "content": "def calc_occupancy(beg, end, intervals):\n    \"\"\"Calculate occupancy of all intervals with in the range.\n\n    This function will aggregate the number of occurrence of all\n    possible intervals between `beg` and `end`, given a collection of\n    occupied intervals.\n\n    Parameters\n    ----------\n    beg\n        Start of the range of interest.\n    end\n        End of the range of interest.\n    intervals\n        An iterable object of tuples of 2 which represents the start\n        and end of an interval.\n\n    Returns\n    -------\n    list\n        A sorted list of tuples of 3. The first two elements of the tuple\n        marks the start and end of an interval, and the second element is the\n        number of occurrences of the interval.\n\n    Examples\n    --------\n    Query the available slots within a certain range.\n\n    >>> calc_occupancy(0, 10, [(1, 2), (2, 3), (2, 4)])\n    [(0, 1, 0), (1, 2, 1), (2, 3, 2), (3, 4, 1), (4, 10, 0)]\n\n    \"\"\"\n    ycs = interval_count([(beg, end), *intervals])\n    xcs = map(lambda x: (x[0], x[1], x[2] - 1), filter(lambda x: x[0] >=\n        beg and x[1] <= end, ycs))\n    return list(xcs)\n", "import_code": []}
{"id": "326d8b6f-d760-3295-8245-62ac9d65a313_0", "content": "def PlotCumulativeRatio(df, Xcols, labelcols, M=10, width=0.8, figpath=None,\n    title=None, bar=True, reversed=False):\n    _, bins, _ = plt.hist(df[Xcols], bins=M)\n    X = bins[:-1][::-1] if reversed else bins[:-1]\n    buf = np.zeros(shape=(1, M))\n    groups = df[labelcols].unique()\n    for g in groups:\n        n, _, _ = plt.hist(df[df[labelcols] == g][Xcols].values, bins=M)\n        if reversed:\n            n = n[::-1]\n        buf = np.r_[buf, np.cumsum(n).reshape(1, -1)]\n    plt.clf()\n    fig, ax = plt.subplots()\n    buf /= len(df)\n    bottoms = np.cumsum(buf, axis=0)\n    cmap = plt.get_cmap('Accent', len(groups)).colors\n    for i, (Y, g) in enumerate(zip(buf[1:], groups)):\n        if bar:\n            ax.bar(X, Y, bottom=bottoms[i], width=width, label=g, align=\n                'center', color=cmap[i])\n        else:\n            ax.plot(X, Y + bottoms[i], label=g, color=cmap[i], marker='o')\n    if figpath is None:\n        figpath = f\"{datetime.now().strftime('%Y-%m-%d %H.%M.%S')}.png\"\n    if title is None:\n        title = f'The influence of \"{Xcols}\" for \"{labelcols}\".'\n    ax.legend()\n    ax.set_title(title)\n    ax.set_xlabel(Xcols)\n    ax.set_ylabel('Cumulative Ratio')\n    plt.savefig(figpath)\n    return ax\n", "import_code": ["import numpy as np\n", "from datetime import datetime\n", "import matplotlib.pyplot as plt\n"]}
{"id": "95f1153b-7af7-31fd-9a72-9d96b49cfd8c_0", "content": "def statement_tokenizer(sentence):\n    \"\"\"\n    This function tokenizes a sentence into individual words and creates a one-hot encoding for each word.\n    :param sentence: The sentence to be tokenized.\n    :return: A pandas DataFrame where each column is a unique word and each row represents the occurrence of that word.\n    \"\"\"\n    token_seq = str.split(sentence)\n    vocab = sorted(set(token_seq))\n    num_tokens = len(token_seq)\n    vocab_size = len(vocab)\n    onehot_vector = np.zeros((num_tokens, vocab_size), int)\n    for i, word in enumerate(token_seq):\n        onehot_vector[i, vocab.index(word)] = 1\n    one_df = pd.DataFrame(onehot_vector, columns=vocab)\n    return one_df\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "95f1153b-7af7-31fd-9a72-9d96b49cfd8c_1", "content": "def inbuild_tokenizer(sentence):\n    \"\"\"\n    This function uses NLTK's built-in tokenizer to tokenize a sentence.\n    :param sentence: The sentence to be tokenized.\n    :return: A list of tokens extracted from the sentence.\n    \"\"\"\n    tk = TreebankWordTokenizer()\n    tokens = tk.tokenize(sentence)\n    return tokens\n", "import_code": ["from nltk.tokenize import RegexpTokenizer, TreebankWordTokenizer, SpaceTokenizer\n"]}
{"id": "95f1153b-7af7-31fd-9a72-9d96b49cfd8c_2", "content": "def get_ngrams(sentence, n):\n    \"\"\"\n    This function extracts n-grams from a sentence.\n    :param sentence: The sentence to be tokenized.\n    :param n: The number of words in each n-gram.\n    :return: A list of n-grams extracted from the sentence.\n    \"\"\"\n    tokens = inbuild_tokenizer(sentence)\n    ng = list(ngrams(tokens, n))\n    return ng\n", "import_code": ["from nltk.util import ngrams\n"]}
{"id": "95f1153b-7af7-31fd-9a72-9d96b49cfd8c_3", "content": "def remove_stop_words(sentence):\n    \"\"\"\n    This function removes stop words from a sentence.\n    :param sentence: The sentence from which stop words need to be removed.\n    :return: A list of tokens with stop words removed.\n    \"\"\"\n    stop_words = nltk.corpus.stopwords.words('english')\n    tokens = inbuild_tokenizer(sentence)\n    removed_sw_tokens = [x for x in tokens if x not in stop_words]\n    return removed_sw_tokens\n", "import_code": ["import nltk\n", "from nltk.tokenize import RegexpTokenizer, TreebankWordTokenizer, SpaceTokenizer\n", "from nltk.util import ngrams\n", "from nltk.stem.porter import PorterStemmer\n"]}
{"id": "95f1153b-7af7-31fd-9a72-9d96b49cfd8c_4", "content": "def apply_stemming(sentence):\n    \"\"\"\n    This function applies stemming to a sentence.\n    :param sentence: The sentence to be stemmed.\n    :return: A string of stemmed words joined by spaces.\n    \"\"\"\n    stemmer = PorterStemmer()\n    tokens = inbuild_tokenizer(sentence)\n    stemmer_tokens = ' '.join([stemmer.stem(w) for w in tokens])\n    return stemmer_tokens\n", "import_code": ["from nltk.stem.porter import PorterStemmer\n"]}
{"id": "a5acb0a8-c161-3740-b6c2-39abbbd4c789_0", "content": "def read_filter_lbmp(path):\n    \"\"\"\n    This function reads one csv file with pandas.\n    It converts the 'Time Stamp' field to pandas datetime format.\n    It filters out non-NYC nodes and only keeps the required columns.\n    It changes the column names to snake case for easier access.\n\n    Parameters\n    ----------\n    path : str or other object for read_csv filepath parameter\n        Path to csv file with LBMP data\n\n    Returns\n    -------\n    DataFrame\n        df with 3 columns (time stamp, name of node, LBMP)\n    \"\"\"\n    df = pd.read_csv(path, parse_dates=['Time Stamp'])\n    df = df.loc[df.Name == 'N.Y.C.', ['Time Stamp', 'Name', 'LBMP ($/MWHr)']]\n    df.columns = ['time_stamp', 'name', 'lbmp']\n    return df\n", "import_code": ["import pandas as pd\n"]}
{"id": "a5acb0a8-c161-3740-b6c2-39abbbd4c789_1", "content": "def read_all_nyc(data_path):\n    \"\"\"\n    This function reads and combines individual LBMP data files.\n\n    Parameters\n    ----------\n    data_path : Path object\n        This is a pathlib Path object pointing to the LBMP data folder with\n        .csv files.\n\n    Returns\n    -------\n    DataFrame\n        df with 4 columns (time stamp, name of node, LBMP, hour of year)\n    \"\"\"\n    fnames = data_path.glob('**/*.csv')\n    dfs = [read_filter_lbmp(name) for name in fnames]\n    df = pd.concat(dfs)\n    df.sort_values('time_stamp', inplace=True)\n    df.reset_index(inplace=True, drop=True)\n    df['hour'] = df.index\n    return df\n", "import_code": ["import pandas as pd\n"]}
{"id": "66792fa7-f5d7-3c63-803f-20e86370100c_0", "content": "@csrf_exempt\ndef createUserRequest(request):\n    if request.method == 'POST':\n        try:\n            data = request.POST.dict()\n            if not data:\n                data = json.loads(request.body)\n            try:\n                User.objects.get(email=data['email'])\n                return HttpResponseBadRequest(json.dumps({'success': False,\n                    'msg': 'User already exist with given email id'}))\n            except:\n                createUser(data)\n                return HttpResponse(json.dumps({'success': True, 'msg':\n                    'User Successfully created, Verify Email Id'}))\n        except Exception as e:\n            return HttpResponseBadRequest(json.dumps({'success': False,\n                'msg': str(e)}))\n    else:\n        return HttpResponseForbidden(json.dumps({'success': False, 'msg':\n            'Get Request Not Accepted'}))\n", "import_code": ["from django.contrib.auth.models import User\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n", "from django.views.decorators.csrf import csrf_exempt\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n", "import json\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n"]}
{"id": "66792fa7-f5d7-3c63-803f-20e86370100c_2", "content": "@csrf_exempt\ndef loginUserRequest(request):\n    if request.method == 'POST':\n        try:\n            data = request.POST.dict()\n            if not data:\n                data = json.loads(request.body)\n            if data.get('password') == None or data.get('password') == '':\n                return HttpResponse(json.dumps({'success': False, 'msg':\n                    \"Password cann't be blank\"}))\n            if data.get('username'):\n                user = authenticate(username=data.get('username'), password\n                    =data.get('password'))\n            elif data.get('email'):\n                username = User.objects.get(email=data.get('email')).username\n                user = authenticate(username=username, password=data.get(\n                    'password'))\n            else:\n                return HttpResponseBadRequest(json.dumps({'success': False,\n                    'msg': 'You must provide username or email'}))\n            if user and user.profile.is_email_verified:\n                login(request, user)\n                return HttpResponse(json.dumps({'success': True, 'url': '/'}))\n            elif not user:\n                return HttpResponse(json.dumps({'success': False, 'msg':\n                    'Wrong Username and Password combination'}))\n            else:\n                return HttpResponse(json.dumps({'success': False, 'msg':\n                    'User Not Activated'}))\n        except Exception as e:\n            return HttpResponse(json.dumps({'success': False, 'msg': str(e)}))\n    else:\n        return HttpResponseForbidden(json.dumps({'success': False, 'msg':\n            'Get Request Not Accepted'}))\n", "import_code": ["from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n", "from django.contrib.auth.models import User\n", "from django.views.decorators.csrf import csrf_exempt\n", "from django.contrib.auth import authenticate, login, logout\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n", "from django.contrib.auth import authenticate, login, logout\n", "import json\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n"]}
{"id": "66792fa7-f5d7-3c63-803f-20e86370100c_3", "content": "@csrf_exempt\ndef googleLoginUserRequest(request):\n    if request.method == 'POST':\n        try:\n            data = request.POST.dict()\n            if not data:\n                data = json.loads(request.body)\n            token = data['token']\n            url = (\n                'https://www.googleapis.com/oauth2/v3/tokeninfo?id_token=' +\n                token)\n            r = requests.get(url)\n            a = json.loads(r.text)\n            email = a['email']\n            first_name = a['given_name']\n            try:\n                last_name = a['family_name']\n            except:\n                last_name = ''\n            try:\n                user = User.objects.get(email=email)\n                active_status = user.is_active\n                user.is_active = True\n                user.save()\n            except Exception as e:\n                user = User.objects.create_user(email, email=email,\n                    first_name=name, last_name=last_name, is_active=True)\n            login(request, user)\n            return HttpResponse(json.dumps({'success': True, 'url': '/'}))\n        except Exception as e:\n            print(str(e))\n            return HttpResponse(json.dumps({'success': False, 'msg': str(e)}))\n    else:\n        return HttpResponseForbidden(json.dumps({'success': False, 'msg':\n            'Get Request Not Accepted'}))\n", "import_code": ["from django.views.decorators.csrf import csrf_exempt\n", "from django.contrib.auth import authenticate, login, logout\n", "import json\n", "import requests\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n", "from django.contrib.auth.models import User\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n"]}
{"id": "66792fa7-f5d7-3c63-803f-20e86370100c_4", "content": "@csrf_exempt\ndef facebookLoginUserRequest(request):\n    if request.method == 'POST':\n        try:\n            data = request.POST.dict()\n            if not data:\n                data = json.loads(request.body)\n            token = data['token']\n            user_id = data['user_id']\n            url = 'https://graph.facebook.com/' + str(user_id\n                ) + '?fields=id,gender,email,name&access_token=' + token\n            r = requests.get(url)\n            a = json.loads(r.text)\n            email = a['email']\n            name = a['name']\n            names = name.split(' ')\n            first_name = names[0]\n            if len(names) > 1:\n                last_name = names[-1]\n            else:\n                last_name = ''\n            try:\n                user = User.objects.get(email=email)\n                user.is_active = True\n                user.save()\n            except Exception as e:\n                user = User.objects.create_user(email, email=email,\n                    first_name=first_name, last_name=last_name, is_active=True)\n            login(request, user)\n            return HttpResponse(json.dumps({'success': True, 'url': '/home'}))\n        except Exception as e:\n            return HttpResponse(json.dumps({'success': False, 'msg': str(e)}))\n    else:\n        return HttpResponseForbidden(json.dumps({'success': False, 'msg':\n            'Get Request Not Accepted'}))\n", "import_code": ["from django.views.decorators.csrf import csrf_exempt\n", "from django.contrib.auth import authenticate, login, logout\n", "import json\n", "import requests\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n", "from django.contrib.auth.models import User\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n"]}
{"id": "66792fa7-f5d7-3c63-803f-20e86370100c_5", "content": "@csrf_exempt\ndef get_user_details(request):\n    if request.method == 'GET':\n        if request.user:\n            try:\n                data = {}\n                data['name'] = request.user.first_name\n                data['email'] = request.user.email\n                return HttpResponse(json.dumps({'success': True, 'data': data})\n                    )\n            except Exception as e:\n                return HttpResponse(json.dumps({'success': False, 'msg':\n                    'No Logged in User'}))\n        else:\n            return HttpResponse(json.dumps({'success': False, 'msg':\n                'No Logged in User'}))\n    else:\n        return HttpResponseForbidden(json.dumps({'error': request.method +\n            'Request Not Accepted'}))\n", "import_code": ["from django.views.decorators.csrf import csrf_exempt\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n", "import json\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n"]}
{"id": "66792fa7-f5d7-3c63-803f-20e86370100c_6", "content": "@csrf_exempt\ndef logout_user(request):\n    if request.method == 'GET':\n        if request.user:\n            logout(request)\n            return HttpResponse(json.dumps({'success': True}))\n        else:\n            return HttpResponse(json.dumps({'success': False}))\n    else:\n        return HttpResponseForbidden(json.dumps({'error':\n            'Get Request Not Accepted'}))\n", "import_code": ["from django.views.decorators.csrf import csrf_exempt\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n", "import json\n", "from django.http import HttpResponse, Http404, HttpResponseForbidden, HttpResponseBadRequest\n", "from django.contrib.auth import authenticate, login, logout\n"]}
{"id": "bd256b9a-58cf-3737-9768-600c281b0e95_0", "content": "def phen_Cu(current_time):\n    return 7.94 + -10.66 * math.exp(-1.49 * current_time)\n", "import_code": ["import math\n"]}
{"id": "bd256b9a-58cf-3737-9768-600c281b0e95_1", "content": "def phen_Al(current_time):\n    return 11.22253 + -11.12797 * math.exp(-1.03475 * current_time)\n", "import_code": ["import math\n"]}
{"id": "bd256b9a-58cf-3737-9768-600c281b0e95_4", "content": "def do_timestep(u0, u, m):\n    current_time = m * dt\n    if current_time < Al_heating_time:\n        u0[121:139, 121:139] = phen_Al(current_time)\n    u[1:-1, 1:-1] = u0[1:-1, 1:-1] + D * dt * ((u0[2:, 1:-1] - 2 * u0[1:-1,\n        1:-1] + u0[:-2, 1:-1]) / dx2 + (u0[1:-1, 2:] - 2 * u0[1:-1, 1:-1] +\n        u0[1:-1, :-2]) / dy2)\n    u0 = u.copy()\n    return u0, u\n", "import_code": []}
{"id": "be4c8d64-c4bb-3505-a831-71058c67ccac_0", "content": "def sorter(list_1, list_2, reverse=False):\n    \"\"\"\n    Sorts list_1 and list_2 based on list_2's values;\n    Useful for processing data before plotting them.\n\n    list_1 and list_2 must be of equal length.\n\n    Parameters\n    ----------\n    list_1 : list\n    list_2 : list\n        list to sort by\n        note that both will be sorted\n\n    reverse : bool\n        if True descending\n        if False ascending\n\n    Returns\n    -------\n    tuple\n        (list_1_sorted, list_2_sorted) both sorted by list_2\n\n    \"\"\"\n    lists = [(l1, l2) for l1, l2 in zip(list_1, list_2)]\n    lists.sort(reverse=reverse, key=lambda x: x[1])\n    list_1_sorted = [item[0] for item in lists]\n    list_2_sorted = [item[1] for item in lists]\n    return list_1_sorted, list_2_sorted\n", "import_code": []}
{"id": "be4c8d64-c4bb-3505-a831-71058c67ccac_1", "content": "def clean(text):\n    \"\"\"\n    Cleans the content of text by stripping leading and trailing\n    whitespace, removing punctuation, and rendering all letters lowercase.\n\n    Parameter\n    ----------\n    text : str\n\n    Returns\n    -------\n    str\n        cleaned content of the text\n    \"\"\"\n    puncts = string.punctuation + '\u2018\u2019\u201c\u201d'\n    text = text.strip()\n    text = text.replace('.', ' ')\n    text = text.replace('\\t', ' ').replace('\\n', ' ')\n    text = text.translate(str.maketrans('', '', puncts))\n    text = text.replace('\\x92', \"'\")\n    return text.lower()\n", "import_code": ["import string\n"]}
{"id": "be4c8d64-c4bb-3505-a831-71058c67ccac_2", "content": "def ngrams(text, n=1):\n    \"\"\"\n    Counts the number of times a contiguous sequence of n words appeared\n    in the text.\n\n    Parameters\n    ----------\n    text : str\n        text to find ngrams in\n    n : int, optional (default 1)\n        value of -grams\n\n    Returns\n    -------\n    sorted_grams : dictionary\n        ngram string as keys and their counts as values\n    \"\"\"\n    grams_list = []\n    grams_dict = {}\n    list_of_paragraphs = text.split('\\n')\n    for paragraph in list_of_paragraphs:\n        words = paragraph.split()\n        for index in range(0, len(words)):\n            temp = words[index:index + n]\n            if len(temp) == n:\n                grams_list.append(' '.join(word for word in words[index:\n                    index + n]))\n    for term in grams_list:\n        if term not in grams_dict:\n            grams_dict[term] = 1\n        else:\n            grams_dict[term] += 1\n    sorted_grams = dict(sorted(grams_dict.items(), key=lambda temp: (-temp[\n        1], temp[0])))\n    return sorted_grams\n", "import_code": []}
{"id": "e84dd324-af7d-3785-96f1-fe23288e0d34_0", "content": "def flip(tuple_, first=False, second=False):\n    if first:\n        t1 = -tuple_[0]\n    else:\n        t1 = tuple_[0]\n    if second:\n        t2 = -tuple_[1]\n    else:\n        t2 = tuple_[1]\n    return t1, t2\n", "import_code": []}
{"id": "e84dd324-af7d-3785-96f1-fe23288e0d34_1", "content": "def add_up(tuple_a, tuple_b):\n    return tuple(map(sum, zip(tuple_a, tuple_b)))\n", "import_code": []}
{"id": "e84dd324-af7d-3785-96f1-fe23288e0d34_2", "content": "def subtract(tuple_a, tuple_b):\n    return add_up(tuple_a, flip(tuple_b, second=True))\n", "import_code": []}
{"id": "e84dd324-af7d-3785-96f1-fe23288e0d34_3", "content": "def multiple(tuple_, size):\n    return tuple([(size * x) for x in tuple_])\n", "import_code": []}
{"id": "e84dd324-af7d-3785-96f1-fe23288e0d34_4", "content": "def divide(tuple_, size):\n    return tuple([int(x / size) for x in tuple_])\n", "import_code": []}
{"id": "e84dd324-af7d-3785-96f1-fe23288e0d34_5", "content": "def calculate_pos(tuple_a, tuple_b, size):\n    return add_up(tuple_a, multiple(flip(tuple_b, second=True), size))\n", "import_code": []}
{"id": "8e21d907-5265-37e7-a698-b8ff1ef7cfdf_0", "content": "def hash_password(password, salt=None):\n    \"\"\"\n    Hash password with salt.\n    :param password: str\n    :param salt: salt algorithm to use.\n    :return:\n    \"\"\"\n    if salt is None:\n        salt_ = os.urandom(32)\n        salt = b64encode(salt_)\n    else:\n        salt_ = b64decode(salt)\n    hashed_password = hashlib.pbkdf2_hmac('sha256', password.encode('utf-8'\n        ), salt_, 100000).hex()\n    return salt, hashed_password\n", "import_code": ["from base64 import b64encode, b64decode\n", "from base64 import b64encode, b64decode\n", "import hashlib\n", "import os\n"]}
{"id": "32a2a2fd-1d15-305d-9b2a-666bb6c83cea_5", "content": "def authenticate(stored, plain_text, salt_length=None) ->bool:\n    \"\"\"\n    Authenticate by comparing stored and new hashes.\n\n    :param stored: str (salt + hash retrieved from database)\n    :param plain_text: str (user-supplied password)\n    :param salt_length: int\n    :return: bool\n    \"\"\"\n    salt = stored[:salt_length]\n    stored_hash = stored[salt_length:]\n    print('salt: ' + salt)\n    print('stroed hash: ' + stored_hash)\n    hashable = salt + plain_text\n    print('hashable: ' + hashable)\n    hashable = hashable.encode('utf-8')\n    this_hash = hashlib.sha256(hashable).hexdigest()\n    print('this_Hash = ' + this_hash)\n    print('stored hash = ' + stored_hash)\n    return this_hash == stored_hash\n", "import_code": ["import hashlib\n"]}
{"id": "32a2a2fd-1d15-305d-9b2a-666bb6c83cea_6", "content": "def hash_pw(plain_text, salt='') ->str:\n    randomThing = os.urandom(40)\n    salt = b64encode(randomThing).decode('utf-8')\n    hashable = salt + plain_text\n    print('hahsable in hash_pw: ' + hashable)\n    hashable = hashable.encode('utf-8')\n    this_hash = hashlib.sha256(hashable).hexdigest()\n    return salt + this_hash\n", "import_code": ["from base64 import b64encode\n", "import hashlib\n", "import os\n"]}
{"id": "32a2a2fd-1d15-305d-9b2a-666bb6c83cea_7", "content": "def getRandomPassword(length):\n    sampleSpecialChars = '?!#$%&*'\n    random_source = string.ascii_letters + string.digits + sampleSpecialChars\n    password = random.choice(string.ascii_lowercase)\n    password += random.choice(string.ascii_uppercase)\n    password += random.choice(string.digits)\n    password += random.choice(sampleSpecialChars)\n    for i in range(length):\n        password += random.choice(random_source)\n    password_list = list(password)\n    random.SystemRandom().shuffle(password_list)\n    password = ''.join(password_list)\n    return password\n", "import_code": ["import random\n", "import string\n"]}
{"id": "ae1ad043-0a7c-3a0b-8016-836b7f538cc5_0", "content": "def encrypt(msg, key):\n    f = Fernet(key)\n    encrypted_msg = f.encrypt(msg.encode())\n    return encrypted_msg\n", "import_code": ["from cryptography.fernet import Fernet\n"]}
{"id": "ae1ad043-0a7c-3a0b-8016-836b7f538cc5_1", "content": "def decrypt(msg, key):\n    f = Fernet(key)\n    decrypted_msg = f.decrypt(msg).decode()\n    return decrypted_msg\n", "import_code": ["from cryptography.fernet import Fernet\n"]}
{"id": "f031a2cf-2375-3804-9844-151b2a56d8c2_0", "content": "def name_to_artifact(name):\n    name = re.sub('(.)([A-Z][a-z]+)', '\\\\1-\\\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', '\\\\1-\\\\2', name).lower()\n", "import_code": ["import re\n"]}
{"id": "3c8d71f2-3853-3937-a927-1179c53afdef_0", "content": "def t_test_two_samp(df, alpha, alternative='two-sided'):\n    a = df[df.group == 'control']['prod']\n    b = df[df.group == 'intervention']['prod']\n    diff = a.mean() - b.mean()\n    res = ss.ttest_ind(a, b)\n    means = ws.CompareMeans(ws.DescrStatsW(a), ws.DescrStatsW(b))\n    confint = means.tconfint_diff(alpha=alpha, alternative=alternative,\n        usevar='unequal')\n    degfree = means.dof_satt()\n    index = ['DegFreedom', 'Difference', 'Statistic', 'PValue', 'Low95CI',\n        'High95CI']\n    return pd.Series([degfree, diff, res[0], res[1], confint[0], confint[1]\n        ], index=index)\n", "import_code": ["import pandas as pd\n", "import scipy.stats as ss\n", "import statsmodels.stats.weightstats as ws\n"]}
{"id": "5fc58c6f-e016-31bf-805d-89a5858e9305_0", "content": "def weighted_mean(var, wts):\n    \"\"\"Calculates the weighted mean\"\"\"\n    return np.average(var, weights=wts)\n", "import_code": ["import numpy as np\n", "import numpy as np\n"]}
{"id": "5fc58c6f-e016-31bf-805d-89a5858e9305_1", "content": "def weighted_variance(var, wts):\n    \"\"\"Calculates the weighted variance\"\"\"\n    return np.average((var - weighted_mean(var, wts)) ** 2, weights=wts)\n", "import_code": ["import numpy as np\n", "import numpy as np\n"]}
{"id": "5fc58c6f-e016-31bf-805d-89a5858e9305_2", "content": "def weighted_skew(var, wts):\n    \"\"\"Calculates the weighted skewness\"\"\"\n    return np.average((var - weighted_mean(var, wts)) ** 3, weights=wts\n        ) / weighted_variance(var, wts) ** 1.5\n", "import_code": ["import numpy as np\n", "import numpy as np\n"]}
{"id": "5fc58c6f-e016-31bf-805d-89a5858e9305_3", "content": "def weighted_kurtosis(var, wts):\n    \"\"\"Calculates the weighted kurtosis\"\"\"\n    return np.average((var - weighted_mean(var, wts)) ** 4, weights=wts\n        ) / weighted_variance(var, wts) ** 2\n", "import_code": ["import numpy as np\n", "import numpy as np\n"]}
{"id": "5fc58c6f-e016-31bf-805d-89a5858e9305_4", "content": "def weighted_covariance(x, y, wt):\n    \"\"\"Calculates the weighted covariance\"\"\"\n    return np.average((x - weighted_mean(x, wt)) * (y - weighted_mean(y, wt\n        )), weights=wt)\n", "import_code": ["import numpy as np\n", "import numpy as np\n"]}
{"id": "5fc58c6f-e016-31bf-805d-89a5858e9305_5", "content": "def weighted_correlation(x, y, wt):\n    \"\"\"Calculates the weighted correlation\"\"\"\n    return weighted_covariance(x, y, wt) / (np.sqrt(weighted_variance(x, wt\n        )) * np.sqrt(weighted_variance(y, wt)))\n", "import_code": ["import numpy as np\n", "import numpy as np\n"]}
{"id": "a4e496c7-712d-3d10-b838-38127ee4623b_0", "content": "def process_page(pagename):\n    \"\"\"\n    Makes a histogram that contains the words from a page.\n    pagename: string\n    returns: map from each word to the number of times it appears.\n    \"\"\"\n    hist = {}\n    page_content = wikipedia.page(pagename).content\n    print(type(page_content))\n    print(page_content)\n    page_content = page_content.replace('-', ' ')\n    strippables = string.punctuation + string.whitespace\n    for word in page_content.split():\n        word = word.strip(strippables)\n        word = word.lower()\n        hist[word] = hist.get(word, 0) + 1\n    return hist\n", "import_code": ["import wikipedia\n", "import string\n"]}
{"id": "a4e496c7-712d-3d10-b838-38127ee4623b_1", "content": "def most_common(hist, excluding_stopwords=True):\n    \"\"\"\n    Makes a list of word-freq pairs(tuples) in descending order of frequency.\n    hist: map from word to frequency\n    excluding_stopwords: a boolean value. If it is True, do not include any stopwords in the list.\n    returns: list of (frequency, word) pairs\n    \"\"\"\n    t = []\n    f = open('stopwords.txt', 'r')\n    stopwords = f.readlines()\n    stopwords = list(stopwords)\n    for word, freq in hist.items():\n        if excluding_stopwords:\n            if word in stopwords:\n                continue\n        t.append((freq, word))\n    t.sort(reverse=True)\n    return t\n", "import_code": []}
{"id": "5dbe9ffa-90dc-3481-86e1-f8b6e19f963e_0", "content": "def ISRU(x):\n    a = 1\n    return x / math.sqrt(a + x ** 2)\n", "import_code": ["import math\n", "import math\n"]}
{"id": "5dbe9ffa-90dc-3481-86e1-f8b6e19f963e_1", "content": "def ISRU(x):\n    a = 1\n    if x == 1 or x == 0:\n        return x\n    else:\n        return x / math.sqrt(a + x ** 2)\n", "import_code": ["import math\n", "import math\n"]}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_0", "content": "def read_csv(readpath):\n    \"\"\"\n    This function reads a large CSV file in chunks to prevent memory overflow.\n\n    Args:\n    readpath (str): The file path of the CSV file to be read.\n\n    Returns:\n    pd.DataFrame: A pandas DataFrame containing the data from the CSV file.\n    \"\"\"\n    print('Start to read - - %s' % readpath)\n    reader = pd.read_csv(readpath, iterator=True)\n    loop = True\n    chunkSize = 100000\n    chunks = []\n    while loop:\n        try:\n            chunk = reader.get_chunk(chunkSize)\n            chunks.append(chunk)\n        except StopIteration:\n            loop = False\n    read_d = pd.concat(chunks, ignore_index=True)\n    return read_d\n", "import_code": ["import pandas as pd\n"]}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_1", "content": "def get_files(dir, filetype='.csv', complete_path=True):\n    \"\"\"\n    This function returns a list of all file names in a directory with a specific file type.\n\n    Args:\n    dir (str): The directory to search for files.\n    filetype (str): The file type to search for. Default is \".csv\".\n    complete_path (bool): If True, the function returns the complete file paths. If False, it returns the file names without the extension.\n\n    Returns:\n    list: A list of file names or file paths.\n    \"\"\"\n    files = []\n    for filename in os.listdir(dir):\n        if os.path.splitext(filename)[1] == filetype:\n            if complete_path:\n                files.append(os.path.join(dir, filename))\n            else:\n                files.append(os.path.splitext(filename)[0])\n    return files\n", "import_code": ["import os\n"]}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_2", "content": "def get_subdir(sup_dir):\n    \"\"\"\n    This function returns a list of all subdirectories in a directory.\n\n    Args:\n    sup_dir (str): The directory to search for subdirectories.\n\n    Returns:\n    list: A list of subdirectory paths.\n    \"\"\"\n    DirList = []\n    for subdir in os.listdir(sup_dir):\n        abs_path = os.path.join(sup_dir, subdir)\n        if os.path.isdir(abs_path):\n            DirList.append(abs_path)\n    return DirList\n", "import_code": ["import os\n"]}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_3", "content": "def random_dataframe_sample(df, num_sample):\n    \"\"\"\n    This function randomly samples a portion of a DataFrame.\n\n    Args:\n    df (pd.DataFrame): The DataFrame to sample from.\n    num_sample (int): The number of samples to take.\n\n    Returns:\n    pd.DataFrame: A DataFrame containing the sampled rows.\n    \"\"\"\n    inds = list(df.index)\n    if num_sample <= len(df):\n        ind_sample = random.sample(inds, num_sample)\n        df_sample = df.ix[ind_sample, :]\n    else:\n        df_sample = df\n    return df_sample\n", "import_code": ["import random\n"]}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_4", "content": "def dataframe_filter(data, attr, lower=None, upper=None):\n    \"\"\"\n    This function filters a DataFrame based on a specific attribute and optional lower and upper bounds.\n\n    Args:\n    data (pd.DataFrame): The DataFrame to filter.\n    attr (str): The attribute to filter by.\n    lower (float): The lower bound for the attribute. Default is None.\n    upper (float): The upper bound for the attribute. Default is None.\n\n    Returns:\n    pd.DataFrame: The filtered DataFrame.\n    \"\"\"\n    origin_length = len(data)\n    if lower is not None:\n        data = data[data[attr] >= lower]\n    if upper is not None:\n        data = data[data[attr] <= upper]\n    filt_length = len(data)\n    left_rate = round(filt_length / origin_length, 3)\n    print('Filter: ', lower, ' ---- ', upper)\n    print('orgin data: %d  - - after filtering: %d  - - %.3f left' % (\n        origin_length, filt_length, left_rate))\n    return data\n", "import_code": []}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_5", "content": "def dataframe_slice_by_timestamp(df, filter_dict):\n    \"\"\"\n    This function slices a DataFrame by timestamp using a dictionary of timestamp ranges.\n\n    Args:\n    df (pd.DataFrame): The DataFrame to slice.\n    filter_dict (dict): A dictionary mapping attribute names to timestamp ranges.\n\n    Returns:\n    pd.DataFrame: The sliced DataFrame.\n    \"\"\"\n    ind_all = []\n    for attr, attr_value in filter_dict.items():\n        timedata = df[[attr]].copy()\n        timedata['IndexValue'] = list(timedata.index.values)\n        timedata.index = list(timedata[attr].values)\n        timedata = timedata[attr_value[0]:attr_value[1]]\n        ind_got = timedata['IndexValue']\n        ind_all.append(list(ind_got.values))\n    if len(ind_all) > 1:\n        set_ind1 = set(ind_all[0])\n        ind_return = list(set_ind1.intersection(*ind_all[1:]))\n    else:\n        ind_return = ind_all[0]\n    df = df.ix[ind_return, :]\n    return df\n", "import_code": []}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_6", "content": "def distribution_fre(data):\n    \"\"\"\n    This function calculates the frequency density distribution of a list or pandas Series.\n\n    Args:\n    data (list or pd.Series): The data to calculate the distribution for.\n\n    Returns:\n    pd.Series: A pandas Series containing the frequency density distribution.\n    \"\"\"\n    if data is None:\n        return None\n    if not isinstance(data, pd.Series):\n        data = pd.Series(data)\n    data_count = data.value_counts().sort_index()\n    data_p = data_count / data_count.sum()\n    return data_p\n", "import_code": ["import pandas as pd\n"]}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_7", "content": "def distribution_pdf(data, bins=None):\n    \"\"\"\n    This function calculates the probability density function of a dataset.\n\n    Args:\n    data (list or pd.Series): The data to calculate the PDF for.\n    bins (int): The number of bins to use for the histogram. Default is None.\n\n    Returns:\n    pd.Series: A pandas Series containing the probability density function.\n    \"\"\"\n    if data is None:\n        return None\n    if bins is None:\n        bins = 512\n    density, xdata = np.histogram(data, bins=bins, density=True)\n    xdata = (xdata + np.roll(xdata, -1))[:-1] / 2.0\n    data_pdf = pd.Series(density, index=xdata)\n    return data_pdf\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_8", "content": "def distribution_cdf(data, bins=None):\n    \"\"\"\n    This function calculates the cumulative probability density function of a dataset.\n\n    Args:\n    data (list or pd.Series): The data to calculate the CDF for.\n    bins (int): The number of bins to use for the histogram. Default is None.\n\n    Returns:\n    pd.Series: A pandas Series containing the cumulative probability density function.\n    \"\"\"\n    pdf = distribution_pdf(data, bins)\n    cdf = []\n    for ind in pdf.index:\n        cdf.append(pdf[ind:].sum())\n    cdf = pd.Series(cdf, index=pdf.index)\n    return cdf\n", "import_code": ["import pandas as pd\n"]}
{"id": "b0738de8-6da7-3e09-a4b8-d317260f5701_10", "content": "def normlize(data, lower=0, upper=1):\n    \"\"\"\n    This function normalizes data to a specific range.\n\n    Args:\n    data (list or array-like): The data to normalize.\n    lower (float): The lower bound of the normalization range. Default is 0.\n    upper (float): The upper bound of the normalization range. Default is 1.\n\n    Returns:\n    list: A list containing the normalized data.\n    \"\"\"\n    xmax = np.max(data)\n    xmin = np.min(data)\n    data_new = (upper - lower) * (data - xmin) / (xmax - xmin) + lower\n    return data_new\n", "import_code": ["import numpy as np\n"]}
{"id": "12caf7b1-e097-3a17-9a9b-3420d3f26bae_1", "content": "def get_geolocation(ip_address):\n    \"\"\"Get geolocation of an ip address\"\"\"\n    response = requests.get(f'https://ipinfo.io/{ip_address}')\n    data = response.json()\n    city, country = data['city'], data['country']\n    coords = [float(coord) for coord in data['loc'].split(',')]\n    return city, country, coords\n", "import_code": ["import requests\n"]}
{"id": "12caf7b1-e097-3a17-9a9b-3420d3f26bae_2", "content": "def greet(ip_address):\n    \"\"\"Greet the user based on their IP address\"\"\"\n    information = get_geolocation(ip_address)\n    return (\n        f'Hello, you are presently in {information[0]},{information[1]} and your coordinates are latitude: {information[2][0]} and longitude: {information[2][1]}'\n        )\n", "import_code": []}
{"id": "ea578691-6e60-3515-9f16-78e4bf3025cb_0", "content": "def stline(othr):\n    oth = re.match(\n        '^ ip address ([0-9]+\\\\.[0-9]+\\\\.[0-9]+\\\\.[0-9]+) ([0-9]+\\\\.[0-9]+\\\\.[0-9]+\\\\.[0-9]+)'\n        , othr)\n    if oth:\n        return {'ip': IPv4Interface(str(oth.group(1)) + '/' + str(oth.group\n            (2)))}\n    oth = re.match('^interface (.+)', othr)\n    if oth:\n        return {'int': oth.group(1)}\n    oth = re.match('^hostname (.+)', othr)\n    if oth:\n        return {'host': oth.group(1)}\n    return {'Other line'}\n", "import_code": ["from ipaddress import IPv4Interface\n", "from ipaddress import IPv4Interface\n", "import re\n"]}
{"id": "a7ff61ff-58f3-3afa-9ada-287e00e6d6a4_0", "content": "def selectHost(timestep, time):\n    timestep = json.loads(timestep)\n    out = []\n    for hostId, host in timestep.items():\n        host_ndna = 0\n        host_unmut = 0\n        for mitoId, mito in host['subcells'].items():\n            host_ndna += mito['n DNA']\n            if mito['n DNA'] > 0:\n                host_unmut += mito['unmut'] * mito['n DNA']\n        dctout = {'host': hostId, 'parent': host['parent'], 'time': time,\n            'n DNA': host_ndna, 'unmut': host_unmut, 'vol': host['vol'],\n            'total_vol': host['total_vol']}\n        dctout.update(host['evolvables'])\n        out.append(dctout)\n    return [out]\n", "import_code": ["import json\n"]}
{"id": "a7ff61ff-58f3-3afa-9ada-287e00e6d6a4_1", "content": "def gethostdct(line, host):\n    return list(filter(lambda living: living['host'] == host, line))[0]\n", "import_code": []}
{"id": "5720ad65-d2e6-3dc5-963b-af5525a273d0_0", "content": "def meter_data_from_csv(filepath_or_buffer, tz=None, start_col='start',\n    value_col='value', gzipped=False, freq=None, **kwargs):\n    \"\"\" Load meter data from a CSV file.\n\n    Default format::\n\n        start,value\n        2017-01-01T00:00:00+00:00,0.31\n        2017-01-02T00:00:00+00:00,0.4\n        2017-01-03T00:00:00+00:00,0.58\n\n    Parameters\n    ----------\n    filepath_or_buffer : :any:`str` or file-handle\n        File path or object.\n    tz : :any:`str`, optional\n        E.g., ``'UTC'`` or ``'US/Pacific'``\n    start_col : :any:`str`, optional, default ``'start'``\n        Date period start column.\n    value_col : :any:`str`, optional, default ``'value'``\n        Value column, can be in any unit.\n    gzipped : :any:`bool`, optional\n        Whether file is gzipped.\n    freq : :any:`str`, optional\n        If given, apply frequency to data using :any:`pandas.DataFrame.resample`.\n    **kwargs\n        Extra keyword arguments to pass to :any:`pandas.read_csv`, such as\n        ``sep='|'``.\n    \"\"\"\n    read_csv_kwargs = {'usecols': [start_col, value_col], 'dtype': {\n        value_col: np.float64}, 'parse_dates': [start_col], 'index_col':\n        start_col}\n    if gzipped:\n        read_csv_kwargs.update({'compression': 'gzip'})\n    read_csv_kwargs.update(kwargs)\n    df = pd.read_csv(filepath_or_buffer, **read_csv_kwargs)\n    df.index = pd.to_datetime(df.index, utc=True)\n    if df.index.tz is None:\n        df.index = df.index.tz_localize('UTC')\n    if tz is not None:\n        df = df.tz_convert(tz)\n    if freq == 'hourly':\n        df = df.resample('H').sum(min_count=1)\n    elif freq == 'daily':\n        df = df.resample('D').sum(min_count=1)\n    return df\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "5720ad65-d2e6-3dc5-963b-af5525a273d0_1", "content": "def temperature_data_from_csv(filepath_or_buffer, tz=None, date_col='dt',\n    temp_col='tempF', gzipped=False, freq=None, **kwargs):\n    \"\"\" Load temperature data from a CSV file.\n\n    Default format::\n\n        dt,tempF\n        2017-01-01T00:00:00+00:00,21\n        2017-01-01T01:00:00+00:00,22.5\n        2017-01-01T02:00:00+00:00,23.5\n\n    Parameters\n    ----------\n    filepath_or_buffer : :any:`str` or file-handle\n        File path or object.\n    tz : :any:`str`, optional\n        E.g., ``'UTC'`` or ``'US/Pacific'``\n    date_col : :any:`str`, optional, default ``'dt'``\n        Date period start column.\n    temp_col : :any:`str`, optional, default ``'tempF'``\n        Temperature column.\n    gzipped : :any:`bool`, optional\n        Whether file is gzipped.\n    freq : :any:`str`, optional\n        If given, apply frequency to data using :any:`pandas.Series.resample`.\n    **kwargs\n        Extra keyword arguments to pass to :any:`pandas.read_csv`, such as\n        ``sep='|'``.\n    \"\"\"\n    read_csv_kwargs = {'usecols': [date_col, temp_col], 'dtype': {temp_col:\n        np.float64}, 'parse_dates': [date_col], 'index_col': date_col}\n    if gzipped:\n        read_csv_kwargs.update({'compression': 'gzip'})\n    read_csv_kwargs.update(kwargs)\n    df = pd.read_csv(filepath_or_buffer, **read_csv_kwargs)\n    df.index = pd.to_datetime(df.index, utc=True)\n    if df.index.tz is None:\n        df.index = df.index.tz_localize('UTC')\n    if tz is not None:\n        df = df.tz_convert(tz)\n    if freq == 'hourly':\n        df = df.resample('H').sum(min_count=1)\n    return df[temp_col]\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "5720ad65-d2e6-3dc5-963b-af5525a273d0_3", "content": "def temperature_data_from_json(data, orient='list'):\n    \"\"\" Load temperature data from json. (Must be given in degrees\n    Fahrenheit).\n\n    Default format::\n\n        [\n            ['2017-01-01T00:00:00+00:00', 3.5],\n            ['2017-01-01T01:00:00+00:00', 5.4],\n            ['2017-01-01T02:00:00+00:00', 7.4],\n        ]\n\n    Parameters\n    ----------\n    data : :any:`list`\n        List elements are each a rows of data.\n\n    Returns\n    -------\n    series : :any:`pandas.Series`\n        DataFrame with a single column (``'tempF'``) and a\n        :any:`pandas.DatetimeIndex`.\n    \"\"\"\n    if orient == 'list':\n        df = pd.DataFrame(data, columns=['dt', 'tempF'])\n        series = df.tempF\n        series.index = pd.to_datetime(df.dt, utc=True)\n        return series\n    else:\n        raise ValueError('orientation not recognized.')\n", "import_code": ["import pandas as pd\n"]}
{"id": "5720ad65-d2e6-3dc5-963b-af5525a273d0_4", "content": "def meter_data_to_csv(meter_data, path_or_buf):\n    \"\"\" Write meter data to CSV. See also :any:`pandas.DataFrame.to_csv`.\n\n    Parameters\n    ----------\n    meter_data : :any:`pandas.DataFrame`\n        Meter data DataFrame with ``'value'`` column and\n        :any:`pandas.DatetimeIndex`.\n    path_or_buf : :any:`str` or file handle, default None\n        File path or object, if None is provided the result is returned as a string.\n    \"\"\"\n    if meter_data.index.name is None:\n        meter_data.index.name = 'start'\n    return meter_data.to_csv(path_or_buf, index=True)\n", "import_code": []}
{"id": "5720ad65-d2e6-3dc5-963b-af5525a273d0_5", "content": "def temperature_data_to_csv(temperature_data, path_or_buf):\n    \"\"\" Write temperature data to CSV. See also :any:`pandas.DataFrame.to_csv`.\n\n    Parameters\n    ----------\n    temperature_data : :any:`pandas.Series`\n        Temperature data series with :any:`pandas.DatetimeIndex`.\n    path_or_buf : :any:`str` or file handle, default None\n        File path or object, if None is provided the result is returned as a string.\n    \"\"\"\n    if temperature_data.index.name is None:\n        temperature_data.index.name = 'dt'\n    if temperature_data.name is None:\n        temperature_data.name = 'temperature'\n    return temperature_data.to_frame().to_csv(path_or_buf, index=True)\n", "import_code": []}
{"id": "73776d28-c6e3-3c7f-a2e3-9f97507e0c49_0", "content": "def normalize(image):\n    \"\"\"\n    Normalize an image between 0 and 255.\n\n    This function takes an image as input and normalizes it by subtracting the minimum value\n    from all pixels and then scaling the result to the range 0-255. This is useful for\n    normalizing image data before displaying it.\n\n    Args:\n    ----------\n    image : Numpy array\n        2-D or 3-D numpy array to be normalized\n\n    Returns:\n    ----------\n    output : Numpy array\n        Normalized version of input\n    \"\"\"\n    output = image - image.min()\n    output = np.uint8(255 * output / output.max())\n    return output\n", "import_code": ["import numpy as np\n"]}
{"id": "73776d28-c6e3-3c7f-a2e3-9f97507e0c49_1", "content": "def rgboverlay(im1, im2=None, im3=None):\n    \"\"\"\n    Create an RGB overlay using one or more images.\n\n    This function takes one, two, or three grayscale images and creates an RGB overlay by\n    normalizing each image and stacking the normalized images along the last axis. This is\n    useful for combining multiple grayscale images into a single RGB image for display.\n\n    Args:\n    ----------\n    im1 : Numpy array\n        2-D Numpy array containg an image\n    im2 : Numpy array\n        2-D Numpy array containg an image\n    im3 : Numpy array\n        2-D Numpy array containg an image\n\n    Returns:\n    ----------\n    rgb : Numpy array\n        3-D array which can be interpreted as RGB image by Matplotlib\n    \"\"\"\n    if len(np.shape(im1)) == 3:\n        rgb = np.dstack((normalize(im1[:, :, 0]), normalize(im1[:, :, 1]),\n            normalize(im1[:, :, 2])))\n    else:\n        rgb = np.dstack((normalize(im1), normalize(im2), normalize(im3)))\n    return rgb\n", "import_code": ["import numpy as np\n"]}
{"id": "73776d28-c6e3-3c7f-a2e3-9f97507e0c49_2", "content": "def gen_cmap(color, alpha=None, nbins=256):\n    \"\"\"\n    Create a Matplotlib colormap ranging from black to a user-defined color.\n\n    This function takes a color string and creates a Matplotlib colormap that ranges from\n    black to the specified color. The colormap can also have a transparency level specified\n    by the alpha parameter.\n\n    Args:\n    ----------\n    color : string\n        Matplotlib compatible color string (Ex. 'red', 'blue', 'r', 'b', etc.)\n    alpha : float\n        Degree of transparency (Default is None)\n    nbins : int\n        Number of bins for the color map (Default is 256)\n\n    Returns:\n    ----------\n    cmap : Matplotlib colormap\n        Matplotlib colormap\n    \"\"\"\n    if alpha:\n        cmap = mpl.colors.LinearSegmentedColormap.from_list('my_cmap', [\n            'black', color], nbins)\n        cmap._init()\n        cmap._lut[:, -1] = np.linspace(0, alpha, cmap.N + 3)\n    else:\n        cmap = mpl.colors.LinearSegmentedColormap.from_list('my_cmap', [\n            'black', color], nbins)\n    return cmap\n", "import_code": ["import numpy as np\n", "import matplotlib as mpl\n"]}
{"id": "92e195ff-9fb4-3f0d-9bd3-4dd8ecb85fa2_2", "content": "def generate_random_string(length):\n    count = 0\n    password = []\n    while count < length:\n        letter = random.randint(0, 1)\n        if letter:\n            password.append(get_random_letter())\n        else:\n            password.append(str(get_random_number()))\n        count += 1\n    return ''.join(password)\n", "import_code": ["import random\n"]}
{"id": "197e0f4a-6245-331e-b48f-44aed069386c_0", "content": "def iso_to_tuple(dt):\n    d, t = dt.split(' ')\n    return tuple(map(int, d.split('-') + t.split(':')))\n", "import_code": []}
{"id": "197e0f4a-6245-331e-b48f-44aed069386c_1", "content": "def calc_diff(dt1, dt2):\n    t1 = iso_to_tuple(dt1)\n    t2 = iso_to_tuple(dt2)\n    s1 = t1[2] * 86400 + t1[3] * 3600 + t1[4] * 60 + t1[5]\n    s2 = t2[2] * 86400 + t2[3] * 3600 + t2[4] * 60 + t2[5]\n    return s1 - s2\n", "import_code": []}
{"id": "d32ec027-1117-3e2a-a9d2-5d6a836b7bd6_0", "content": "def encode_image(imagename):\n    with open(imagename, 'rb') as f:\n        image_content = f.read()\n    f.close()\n    return base64.b64encode(image_content)\n", "import_code": ["import base64\n"]}
{"id": "4cb82a58-6e06-3b77-a487-ca4cb41a92a2_2", "content": "@app.route('/<path>')\ndef today(path):\n    \"\"\"\n    This function handles GET requests to any path.\n    It attempts to serve a file from the static directory.\n    If the file is not found, it defaults to rendering the 'index.html' template.\n    \"\"\"\n    try:\n        base_dir = os.path.dirname(__file__)\n        resp = make_response(open(os.path.join(base_dir, path)).read())\n        resp.headers['Content-type'] = 'text/plan;charset=UTF-8'\n        return resp\n    except:\n        return render_template('index.html')\n", "import_code": ["from flask import Flask, request, jsonify, render_template, make_response, send_from_directory, send_file\n", "from flask import Flask, request, jsonify, render_template, make_response, send_from_directory, send_file\n", "import os\n"]}
{"id": "fa852e51-5d24-3394-ad27-d78d296a75f9_0", "content": "def ortalama(df):\n    toplam = df.sum().sum()\n    adet = df.size - df.isnull().sum().sum()\n    return toplam / adet\n", "import_code": []}
{"id": "21ffca6d-b9c1-384a-9cdf-5cc79cea91a1_0", "content": "def parseCloseDF(data_file_path: str) ->DataFrame:\n    df = pd.read_csv(data_file_path)\n    df['Date'] = pd.to_datetime(df.Date, format='%Y-%m-%d')\n    df.index = df['Date']\n    data = df.sort_index(ascending=True, axis=0)\n    new_dataset = pd.DataFrame(index=range(0, len(df)), columns=['Date',\n        'Close'])\n    for i in range(0, len(data)):\n        new_dataset['Date'][i] = data['Date'][i]\n        new_dataset['Close'][i] = data['Close'][i]\n    new_dataset.index = new_dataset.Date\n    new_dataset.drop('Date', axis=1, inplace=True)\n    return new_dataset\n", "import_code": ["import pandas as pd\n", "from pandas.core.frame import DataFrame\n", "from pandas.core.frame import DataFrame\n"]}
{"id": "21ffca6d-b9c1-384a-9cdf-5cc79cea91a1_1", "content": "def parse_roc_df(data_file_path: str, filter_branch=None) ->DataFrame:\n    df = pd.read_csv(data_file_path)\n    if filter_branch:\n        df = df[df['Stock'] == filter_branch]\n    df['Date'] = pd.to_datetime(df.Date, format='%Y-%m-%d')\n    df.index = df['Date']\n    data = df.sort_index(ascending=True, axis=0)\n    new_dataset = pd.DataFrame(index=range(0, len(df)), columns=['Date',\n        'Close', 'ROC'])\n    new_dataset['ROC'][0] = 0\n    for i in range(1, len(data)):\n        new_dataset['Date'][i] = data['Date'][i]\n        new_dataset['Close'][i] = data['Close'][i]\n        new_dataset['ROC'][i] = data['Close'][i] / data['Close'][i - 1] * 100\n    new_dataset.index = new_dataset.Date\n    new_dataset.drop('Date', axis=1, inplace=True)\n    return new_dataset\n", "import_code": ["import pandas as pd\n", "from pandas.core.frame import DataFrame\n", "from pandas.core.frame import DataFrame\n"]}
{"id": "21ffca6d-b9c1-384a-9cdf-5cc79cea91a1_2", "content": "def calculate_roc(df: DataFrame, feature='Close') ->DataFrame:\n    new_df = df.copy()\n    new_df['ROC'] = 0.0\n    for i in range(1, len(new_df)):\n        new_df['ROC'][i] = new_df[feature][i] / new_df[feature][i - 1\n            ] * 100 - 100\n    print(new_df['ROC'])\n    return new_df\n", "import_code": ["from pandas.core.frame import DataFrame\n"]}
{"id": "e3627108-1312-3630-a703-16ee2d340f7d_0", "content": "def get_dataset(file_path):\n    df = pd.read_csv(file_path, sep=',')\n    str_column = ['userGenre1', 'userGenre2', 'userGenre3', 'userGenre4',\n        'userGenre5', 'movieGenre1', 'movieGenre2', 'movieGenre3']\n    for c in str_column:\n        df[c].fillna('', inplace=True)\n    num_column = ['userRatedMovie1', 'userRatedMovie2', 'userRatedMovie3',\n        'userRatedMovie4', 'userRatedMovie5']\n    for c in num_column:\n        df[c].fillna(0, inplace=True)\n    data_feature = {}\n    columns = df.columns.values\n    labels = []\n    for rating in df['rating'].values:\n        if int(rating) > 3:\n            labels.append(1)\n        else:\n            labels.append(0)\n    for column in columns:\n        if column == 'rating':\n            continue\n        dt = np.array(df[column].values).dtype\n        if dt == 'object':\n            dt = 'string'\n        elif dt == 'int64':\n            dt = 'int32'\n        elif dt == 'float64':\n            dt = 'float32'\n        else:\n            dt = 'int32'\n        data_feature[column] = tf.constant(df[column].values, dtype=dt)\n    return data_feature, np.array(labels)\n", "import_code": ["import pandas as pd\n", "import numpy as np\n", "import tensorflow as tf\n"]}
{"id": "02acb55c-8656-383d-9aa8-566890e369de_0", "content": "def get_best_tree_depth(x, y):\n    depth_score = []\n    for i in range(1, 17):\n        clf = tree.DecisionTreeClassifier(max_depth=i, class_weight='balanced')\n        scores = cross_val_score(estimator=clf, X=x, y=y, cv=7, n_jobs=4)\n        depth_score.append(scores.mean())\n    depth = np.argmax(depth_score) + 1\n    accuracy = depth_score[depth]\n    std_dev = np.array(depth_score).std() * 2\n    return depth, accuracy, std_dev\n", "import_code": ["import numpy as np\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn import tree\n", "import xml.etree.cElementTree as et\n", "from sklearn.model_selection import cross_val_score\n"]}
{"id": "02acb55c-8656-383d-9aa8-566890e369de_1", "content": "def xml_to_pd(xml_file, isForPred):\n    parsed_xml = et.parse(xml_file)\n    dfcols = ['index', 'school', 'sex', 'age', 'adress', 'traveltime',\n        'failures', 'schoolsup', 'activities', 'absences', 'finalGrade',\n        'subject', 'recentAvgGradeScore']\n    df_xml = pd.DataFrame(columns=dfcols)\n    for node in parsed_xml.getroot():\n        index = node.find('index')\n        school = node.find('school')\n        sex = node.find('sex')\n        age = node.find('age')\n        adress = node.find('adress')\n        traveltime = node.find('traveltime')\n        failures = node.find('failures')\n        schoolsup = node.find('schoolsup')\n        activities = node.find('activities')\n        absences = node.find('absences')\n        finalGrade = node.find('finalGrade')\n        subject = node.find('subject')\n        recentAvgGradeScore = node.find('recentAvgGradeScore')\n        df_xml = df_xml.append(pd.Series([getvalueofnode(index),\n            getvalueofnode(school), getvalueofnode(sex), getvalueofnode(age\n            ), getvalueofnode(adress), getvalueofnode(traveltime),\n            getvalueofnode(failures), getvalueofnode(schoolsup),\n            getvalueofnode(activities), getvalueofnode(absences),\n            getvalueofnode(finalGrade), getvalueofnode(subject),\n            getvalueofnode(recentAvgGradeScore)], index=dfcols),\n            ignore_index=True)\n        if isForPred:\n            df_xml = df_xml.drop(['finalGrade'], axis=1)\n    return df_xml\n", "import_code": ["import pandas as pd\n", "from sklearn.metrics import precision_score\n", "from sklearn import metrics\n", "import xml.etree.cElementTree as et\n"]}
{"id": "02acb55c-8656-383d-9aa8-566890e369de_2", "content": "def getvalueofnode(node):\n    return node.text if node is not None else None\n", "import_code": []}
{"id": "c50329d6-0050-31d1-ab5e-972d9ecb1a3b_0", "content": "def add(x):\n    return x + x\n", "import_code": []}
{"id": "7e399cba-7296-3eb1-85b8-0f8fd08fb220_0", "content": "def getColumnNames(headercsv):\n    \"\"\"get column names by reading first row of data\"\"\"\n    df = pd.read_csv(path[0] + headercsv, nrows=1)\n    return df.columns.values\n", "import_code": ["import pandas as pd\n"]}
{"id": "7e399cba-7296-3eb1-85b8-0f8fd08fb220_1", "content": "def defineDataTypes(columns):\n    \"\"\"define the data types of each column in a dict to pass to csv read\"\"\"\n    dtypedict = {}\n    colcount = 0\n    datatype = None\n    for column in columns:\n        if colcount in (55, 56, 58, 59, 61, 62, 64):\n            datatype = np.dtype('a64')\n        else:\n            datatype = np.int32\n        dtypedict[columns[colcount]] = datatype\n        colcount += 1\n    return dtypedict\n", "import_code": ["import numpy as np\n"]}
{"id": "7e399cba-7296-3eb1-85b8-0f8fd08fb220_2", "content": "def getNumberOfLinesCSV(noheaderCSV):\n    \"\"\"Get number of lines in the CSV file without headers, used to iterate\"\"\"\n    nlines = 0\n    for line in open(path[0] + noheaderCSV):\n        nlines += 1\n    return nlines\n", "import_code": []}
{"id": "2757ddb8-b0ca-3059-9995-9b1529054cb6_0", "content": "def to_usd(my_price):\n    \"\"\"\n    Converts a numeric value to usd-formatted string, for printing and display purposes.\n\n    Param: my_price (int or float) like 4000.444444\n\n    Example: to_usd(4000.444444)\n\n    Returns: $4,000.44\n    \"\"\"\n    return f'${my_price:,.2f}'\n", "import_code": []}
{"id": "2757ddb8-b0ca-3059-9995-9b1529054cb6_1", "content": "def month_num(month):\n    \"\"\"\n    Converts a full month name to numeric string.\n    Param: month (string) like February\n    Example: month_num('February')\n    Returns: '02'\n    \"\"\"\n    if month == 'January':\n        return '01'\n    elif month == 'February':\n        return '02'\n    elif month == 'March':\n        return '03'\n    elif month == 'April':\n        return '04'\n    elif month == 'May':\n        return '05'\n    elif month == 'June':\n        return '06'\n    elif month == 'July':\n        return '07'\n    elif month == 'August':\n        return '08'\n    elif month == 'September':\n        return '09'\n    elif month == 'October':\n        return '10'\n    elif month == 'November':\n        return '11'\n    else:\n        return '12'\n", "import_code": []}
{"id": "2757ddb8-b0ca-3059-9995-9b1529054cb6_2", "content": "def rev_month_num(mnum):\n    \"\"\"\n    Converts a numeric month string to full month name.\n    Param: month number (string) like 02\n    Example: month_num('02')\n    Returns: 'February'\n    \"\"\"\n    if mnum == '01':\n        return 'January'\n    elif mnum == '02':\n        return 'February'\n    elif mnum == '03':\n        return 'March'\n    elif mnum == '04':\n        return 'April'\n    elif mnum == '05':\n        return 'May'\n    elif mnum == '06':\n        return 'June'\n    elif mnum == '07':\n        return 'July'\n    elif mnum == '08':\n        return 'August'\n    elif mnum == '09':\n        return 'September'\n    elif mnum == '10':\n        return 'October'\n    elif mnum == '11':\n        return 'November'\n    else:\n        return 'December'\n", "import_code": []}
{"id": "2757ddb8-b0ca-3059-9995-9b1529054cb6_3", "content": "def validate(user_input, ref_list):\n    \"\"\"\n    Validates user inputs\n    \"\"\"\n    store = 0\n    for item in ref_list:\n        if item == user_input:\n            store += 1\n    if store > 0:\n        return 'match'\n    elif user_input == 'Exit':\n        return 'exit'\n    else:\n        return 'no match'\n", "import_code": []}
{"id": "2757ddb8-b0ca-3059-9995-9b1529054cb6_4", "content": "def prev_year(user_month, user_year, min_date):\n    \"\"\"\n    Defines period of up to 12 months prior to month of user input)\n    \"\"\"\n    if user_month == '01':\n        m_end = '12'\n        y_end = str(int(user_year) - 1)\n    else:\n        y_end = user_year\n        if int(user_month) <= 10:\n            m_end = '0' + str(int(user_month) - 1)\n        else:\n            m_end = str(int(user_month) - 1)\n    m_st = user_month\n    y_st = str(int(user_year) - 1)\n    comp_str = y_st + m_st\n    comp_int = int(comp_str)\n    repl_int = max(int(min_date), comp_int)\n    repl_str = str(repl_int)\n    if repl_str != comp_str:\n        m_st = repl_str[0:4]\n        y_st = repl_str[-2:]\n    return [m_st, y_st, m_end, y_end]\n", "import_code": []}
{"id": "6972813d-13c3-3e7b-b3d0-33a9a8918110_0", "content": "def dataentro_cal(dataset):\n    \"\"\"\n    This function calculates the entropy of the dataset.\n    Entropy is a measure of the impurity or uncertainty of a data set.\n\n    Args:\n    dataset\uff1aa numpy array where each row is a feature vector.\n\n    Return:\n    The entropy of the dataset.\n    \"\"\"\n    total_num = len(dataset)\n    data_labels = {}\n    for fea_vec in dataset:\n        data_labels[fea_vec[-1]] = data_labels.get(fea_vec[-1], 0) + 1\n    entropy = 0.0\n    for label in data_labels.keys():\n        label_prob = float(data_labels[label]) / total_num\n        entropy -= label_prob * log(label_prob, 2)\n    return entropy\n", "import_code": ["from math import log\n"]}
{"id": "6972813d-13c3-3e7b-b3d0-33a9a8918110_2", "content": "def split_dataset(dataset, index, value):\n    \"\"\"\n    This function splits the dataset into two parts based on the value of a feature.\n\n    Args:\n        dataset: A numpy array where each row is a feature vector.\n        index: The index of the feature to split on.\n        value: The value of the feature to split on.\n\n    Return:\n        ret_dataset: A list containing the feature vectors where the feature at the given index has the given value.\n    \"\"\"\n    ret_dataset = []\n    for fea_vec in dataset:\n        if fea_vec[index] == value:\n            redfea_vec = fea_vec[:index]\n            redfea_vec.extend(fea_vec[index + 1:])\n            ret_dataset.append(redfea_vec)\n    return ret_dataset\n", "import_code": []}
{"id": "6972813d-13c3-3e7b-b3d0-33a9a8918110_3", "content": "def bestfeature_split(dataset):\n    \"\"\"\n    This function iteratively splits the dataset and calculates the information gain,\n    and returns the best feature index on which the dataset is best split with maximum information gain.\n\n    Args:\n        dataset: A numpy array where each row is a feature vector.\n\n    Return:\n        The index of the best feature to split on.\n    \"\"\"\n    fea_nums = len(dataset[0]) - 1\n    base_entro = dataentro_cal(dataset)\n    best_gain = 0.0\n    for i in range(fea_nums):\n        con_entro = 0.0\n        fea_list = [sample[i] for sample in dataset]\n        fea_set = set(fea_list)\n        for value in fea_set:\n            sub_dataset = split_dataset(dataset, i, value)\n            entropy = dataentro_cal(sub_dataset)\n            prob_subdataset = len(sub_dataset) / float(len(dataset))\n            con_entro += prob_subdataset * entropy\n            info_gain = base_entro - con_entro\n        if info_gain > best_gain:\n            best_gain = info_gain\n            best_feature = i\n    return best_feature\n", "import_code": []}
{"id": "6972813d-13c3-3e7b-b3d0-33a9a8918110_4", "content": "def majority_cnt(classlist):\n    \"\"\"\n    This function counts the majority of the class list and returns the class with most number.\n\n    Args:\n        classlist: A list of class labels.\n\n    Return:\n        The class name of the majority one.\n    \"\"\"\n    class_count = {}\n    for vote in classlist:\n        class_count[vote] = class_count.get(vote, 0) + 1\n    sortedclass_count = sorted(class_count.items(), key=operator.itemgetter\n        (1), reverse=True)\n    return sortedclass_count[0][0]\n", "import_code": ["import operator\n"]}
{"id": "6972813d-13c3-3e7b-b3d0-33a9a8918110_5", "content": "def create_tree(dataset, labels):\n    \"\"\"\n    This function creates the decision tree based on the dataset and feature labels.\n\n    Args:\n        dataset: A numpy array where each row is a feature vector, and the last element is the classification.\n        labels: A list of feature names.\n\n    Returns:\n        mytree: A dictionary representing the structure of a decision tree.\n    \"\"\"\n    classlist = [sample[-1] for sample in dataset]\n    if classlist.count(classlist[0]) == len(classlist):\n        return classlist[0]\n    elif len(dataset[0]) == 1:\n        return majority_cnt(classlist)\n    bestfea_index = bestfeature_split(dataset)\n    bestfea_label = labels[bestfea_index]\n    mytree = {bestfea_label: {}}\n    feavalue_list = [sample[bestfea_index] for sample in dataset]\n    feavalue_set = set(feavalue_list)\n    sub_labels = labels[:]\n    del sub_labels[bestfea_index]\n    for value in feavalue_set:\n        sub_dataset = split_dataset(dataset, bestfea_index, value)\n        mytree[bestfea_label][value] = create_tree(sub_dataset, sub_labels)\n    return mytree\n", "import_code": []}
{"id": "f878138d-5037-3344-9f5d-3aa16751c820_2", "content": "def get_emr_package_dir(project):\n    return os.path.join(project.expand_path('$dir_target'), \n        _EMR_PACKAGE_DIR + '-' + project.version)\n", "import_code": ["import os\n"]}
{"id": "f878138d-5037-3344-9f5d-3aa16751c820_3", "content": "def get_path_to_zipfile(project):\n    return os.path.join(get_emr_package_dir(project), '{0}.zip'.format(\n        project.name))\n", "import_code": ["import os\n"]}
{"id": "54a23d16-3c9c-315a-95c3-716384cf11e3_0", "content": "def to_dict(path: str):\n    feature_name = path.split(os.sep)[-1].split('.')[0]\n    df = pd.read_csv(path, sep='\\t', index_col=[0, 1, 2, 3]).round(2)\n    df.columns = pd.MultiIndex.from_product([[feature_name], df.columns])\n    return df\n", "import_code": ["import pandas as pd\n", "import os\n"]}
{"id": "527c2a4d-e078-330c-a779-17528bb9653a_0", "content": "def calc_euclidean(train_data, test_data, train_point, test_point, features):\n    \"\"\"\n    This function calculates the Euclidean distance between two points.\n    It takes in the training and testing data, as well as the indices of the points and a list of features.\n    It returns the Euclidean distance.\n    \"\"\"\n    total_dist = 0\n    for feature in features:\n        dist = math.pow(test_data.loc[test_point, feature] - train_data.loc\n            [train_point, feature], 2)\n        total_dist += dist\n    return math.sqrt(total_dist)\n", "import_code": ["import math\n"]}
{"id": "527c2a4d-e078-330c-a779-17528bb9653a_1", "content": "def det_nearest_neighbors(train_data, test_data, test_point, features, k):\n    \"\"\"\n    This function finds the k nearest neighbors to a test point in the training data.\n    It takes in the training and testing data, as well as the index of the test point and a list of features.\n    It returns a list of the indices of the k nearest neighbors.\n    \"\"\"\n    dist_dict = {}\n    for train_point in train_data.index:\n        dist_dict[train_point] = calc_euclidean(train_data, test_data,\n            train_point, test_point, features)\n    dict_top_k = sorted(dist_dict, key=dist_dict.get, reverse=True)[:k]\n    return dict_top_k\n", "import_code": []}
{"id": "527c2a4d-e078-330c-a779-17528bb9653a_2", "content": "def KNN(test_data, train_data, features, target_col_name, k):\n    \"\"\"\n    This function performs the K-Nearest Neighbors algorithm on the test data using the training data.\n    It takes in the test and training data, as well as a list of features and the name of the target column.\n    It returns the modified test data with the predicted classes and the accuracy of the predictions.\n    \"\"\"\n    for test_point in test_data.index:\n        nearest_neighbors = det_nearest_neighbors(train_data, test_data,\n            test_point, features, k)\n        nearest_neighbors = train_data[train_data.index.isin(nearest_neighbors)\n            ]\n        agg_class = nearest_neighbors.groupby(target_col_name).size()\n        test_data.loc[test_point, 'classify'] = agg_class.idxmax()\n    len_classify_equals_target = len(test_data[test_data[target_col_name] ==\n        test_data['classify']].index)\n    len_test_data = len(test_data.index)\n    accuracy = len_classify_equals_target / len_test_data\n    return test_data, accuracy\n", "import_code": []}
{"id": "e2946184-eea9-348b-b21a-ee79fc18800f_0", "content": "def set_env_var(name, default=None):\n    \"\"\"Set an environmental variable or use given default value.\"\"\"\n    try:\n        var = os.environ[name]\n    except KeyError as e:\n        print('Warning: Environmental variable ' + str(e) + ' not defined.')\n        print('\\t Using default value: ' + str(default))\n        var = default\n    return var\n", "import_code": ["import os\n"]}
{"id": "12adb6cd-caed-3fe7-b719-3f4bcf211c59_0", "content": "def hash_str(s):\n    x = hmac.new(SECRET.encode('utf-8'), s.encode('utf-8')).hexdigest()\n    return x\n", "import_code": ["import string, hashlib, hmac, random\n"]}
{"id": "12adb6cd-caed-3fe7-b719-3f4bcf211c59_1", "content": "def make_secure_val(s):\n    return '%s|%s' % (s, hash_str(s))\n", "import_code": []}
{"id": "12adb6cd-caed-3fe7-b719-3f4bcf211c59_2", "content": "def check_secure_val(h):\n    x = h.split('|')[0]\n    if make_secure_val(x) == h:\n        return x\n", "import_code": []}
{"id": "12adb6cd-caed-3fe7-b719-3f4bcf211c59_4", "content": "def make_pw_hash(name, pw, salt=None):\n    if not salt:\n        salt = make_salt()\n    h = hashlib.sha256((name + pw + salt).encode('utf-8')).hexdigest()\n    return '%s,%s' % (h, salt)\n", "import_code": ["import string, hashlib, hmac, random\n"]}
{"id": "12adb6cd-caed-3fe7-b719-3f4bcf211c59_5", "content": "def valid_pw(name, pw, h):\n    salt = h.split(',')[1]\n    return make_pw_hash(name, pw, salt) == h\n", "import_code": []}
{"id": "26a8eb0f-b681-3f58-9688-23ae046b7b2a_0", "content": "def load(line):\n    key, eVal = line.split('\\t')\n    Value = pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n    return key, Value\n", "import_code": ["import base64, pickle, zlib, sys\n", "import base64, pickle, zlib, sys\n", "import base64, pickle, zlib, sys\n"]}
{"id": "a81bb562-3f9f-377c-a1e3-304fb73d0d79_0", "content": "def create(_clock=None, _uuid=None, _zlib=None, _base64=None, _urllib=None,\n    **kwargs):\n    \"\"\"\n    Create a URL string which can be used to redirect a samlp:AuthnRequest\n    to the identity provider. Return a URL string containing the\n    idp_sso_target_url and a deflated, base64-encoded, url-encoded\n    (in that order) samlp:AuthnRequest XML element as the value of the\n    SAMLRequest parameter.\n\n    Keyword arguments:\n    assertion_consumer_service_url -- The URL at which the SAML assertion\n                                      should be received.\n    issuer -- The name of your application. Some identity providers might need\n              this to establish the identity of the service provider requesting\n              the login.\n    name_identifier_format -- The format of the username required by this\n                              application. If you need the email address, use\n                              \"urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress\".\n                              See http://docs.oasis-open.org/security/saml/v2.0/saml-core-2.0-os.pdf\n                              section 8.3 for other options. Note that the\n                              identity provider might not support all options.\n    idp_sso_target_url -- The URL to which the authentication request should be\n                          sent. This would be on the identity\n    \"\"\"\n    if _clock is None:\n        _clock = datetime.utcnow\n    if _uuid is None:\n        _uuid = uuid.uuid4\n    if _zlib is None:\n        _zlib = zlib\n    if _base64 is None:\n        _base64 = base64\n    if _urllib is None:\n        _urllib = urllib\n    assertion_consumer_service_url = kwargs.pop(\n        'assertion_consumer_service_url')\n    issuer = kwargs.pop('issuer')\n    name_identifier_format = kwargs.pop('name_identifier_format')\n    idp_sso_target_url = kwargs.pop('idp_sso_target_url')\n    now = _clock()\n    now = now.replace(microsecond=0)\n    now_iso = now.isoformat() + 'Z'\n    unique_id = _uuid()\n    unique_id = unique_id.hex\n    samlp_maker = ElementMaker(namespace=\n        'urn:oasis:names:tc:SAML:2.0:protocol', nsmap=dict(samlp=\n        'urn:oasis:names:tc:SAML:2.0:protocol'))\n    saml_maker = ElementMaker(namespace=\n        'urn:oasis:names:tc:SAML:2.0:assertion', nsmap=dict(saml=\n        'urn:oasis:names:tc:SAML:2.0:assertion'))\n    authn_request = samlp_maker.AuthnRequest(ProtocolBinding=\n        'urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST', Version='2.0',\n        IssueInstant=now_iso, ID=unique_id, AssertionConsumerServiceURL=\n        assertion_consumer_service_url)\n    saml_issuer = saml_maker.Issuer()\n    saml_issuer.text = issuer\n    authn_request.append(saml_issuer)\n    name_id_policy = samlp_maker.NameIDPolicy(Format=name_identifier_format,\n        AllowCreate='true')\n    authn_request.append(name_id_policy)\n    request_authn_context = samlp_maker.RequestedAuthnContext(Comparison=\n        'exact')\n    authn_request.append(request_authn_context)\n    authn_context_class_ref = saml_maker.AuthnContextClassRef()\n    authn_context_class_ref.text = (\n        'urn:oasis:names:tc:SAML:2.0:ac:classes:PasswordProtectedTransport')\n    request_authn_context.append(authn_context_class_ref)\n    compressed_request = _zlib.compress(etree.tostring(authn_request))\n    deflated_request = compressed_request[2:-4]\n    encoded_request = _base64.b64encode(deflated_request)\n    urlencoded_request = _urllib.urlencode([('SAMLRequest', encoded_request)])\n    if '?' in idp_sso_target_url:\n        separator = '&'\n    else:\n        separator = '?'\n    return '{url}{sep}{query}'.format(url=idp_sso_target_url, sep=separator,\n        query=urlencoded_request)\n", "import_code": ["from datetime import datetime\n", "import base64\n", "import urllib\n", "import uuid\n", "from lxml.builder import ElementMaker\n", "import zlib\n", "from lxml import etree\n"]}
{"id": "9b3e5be6-bd8f-326b-a836-9df6fe4bdfd2_0", "content": "def bagOfWords(text):\n    text = removePunctuation(text)\n    return dict([(word.lower(), True) for word in text.split()])\n", "import_code": []}
{"id": "9b3e5be6-bd8f-326b-a836-9df6fe4bdfd2_1", "content": "def bagOfWordsCount(text):\n    text = removePunctuation(text)\n    return FreqDist([word.lower() for word in text.split()])\n", "import_code": ["from nltk import FreqDist\n"]}
{"id": "9b3e5be6-bd8f-326b-a836-9df6fe4bdfd2_2", "content": "def removePunctuation(text):\n    return re.sub('[^a-zA-Z0-9_\\\\s]', '', text)\n", "import_code": ["from nltk import FreqDist\n", "import re\n"]}
{"id": "9d361eae-91c9-3db0-90a9-a479b33f08f8_0", "content": "def get_rmp_comments(teachers, teacher_lastname):\n    for teacher, ratings in teachers:\n        if teacher.lastname == teacher_lastname:\n            comments = [x.comments for x in ratings]\n            return ' '.join(comments)\n", "import_code": []}
{"id": "9d361eae-91c9-3db0-90a9-a479b33f08f8_1", "content": "def get_all_rmp(teachers):\n    text = []\n    for teacher, ratings in teachers:\n        text.append(' '.join([x.comments for x in ratings]))\n    return ' '.join(text)\n", "import_code": []}
{"id": "9d361eae-91c9-3db0-90a9-a479b33f08f8_2", "content": "def get_unou_comments(reviews, teacher_lastname):\n    for name, t_reviews in reviews.items():\n        if teacher_lastname in name:\n            return ' '.join([(x['improvement'] + ' ' + x['comments']) for x in\n                t_reviews])\n", "import_code": []}
{"id": "9d361eae-91c9-3db0-90a9-a479b33f08f8_3", "content": "def clean_text(orig_text):\n    text = orig_text.lower()\n    text = re.sub(\"[^A-Za-z ']\", ' ', text)\n    text = text.split(' ')\n    text = [x for x in text if x.strip() not in common_words]\n    return ' '.join(text)\n", "import_code": ["import re\n"]}
{"id": "9763b99e-ac9c-3260-b439-d2f139792c7b_0", "content": "def get_soup_strings(soup, ignore_tags={'code'}):\n    \"\"\"\n    This function extracts all the textual content from a BeautifulSoup object.\n    It ignores tags specified in the `ignore_tags` parameter.\n    :param soup: BeautifulSoup object to extract text from.\n    :param ignore_tags: Set of tag names to ignore.\n    :return: List of strings extracted from the BeautifulSoup object.\n    \"\"\"\n    texts = []\n    for tag in soup.descendants:\n        if isinstance(tag, NavigableString):\n            if tag.parent.name not in ignore_tags:\n                if str(tag).strip():\n                    texts.append(str(tag).strip())\n    return texts\n", "import_code": ["from bs4 import BeautifulSoup, NavigableString\n"]}
{"id": "9763b99e-ac9c-3260-b439-d2f139792c7b_1", "content": "def clean(text):\n    \"\"\"\n    Cleans the passed English text by:\n    -- removing HTML tags\n    -- removing all the special characters except common linguistic punctuations\n    -- converting to lowercase\n    :param text: Text to be cleaned.\n    :return: Cleaned text.\n    \"\"\"\n    soup = BeautifulSoup(text, 'lxml')\n    text = ' '.join(get_soup_strings(soup))\n    re.sub('\\\\s+', ' ', text, flags=re.I)\n    text = re.sub('(\\\\d|[^!&,.:;?a-zA-Z])+', ' ', text)\n    text = text.lower()\n    return text\n", "import_code": ["from bs4 import BeautifulSoup, NavigableString\n", "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import TfidfTransformer\n", "import re\n"]}
{"id": "9763b99e-ac9c-3260-b439-d2f139792c7b_2", "content": "def get_tf_idf_vector_for_text(text, count_vectorizer, tfidf_transformer):\n    \"\"\"\n    Generates TF-IDF vector for the provided free text according to the trained TF-IDF model.\n    Unseen words are ignored, hence do not impact the tfidf scores.\n    :param text: Free text for which you want to generate a tfidf vector.\n    :param count_vectorizer: Trained BoW model.\n    :param tfidf_transformer: Trained TF-IDF transformer.\n    :return: the TF-IDF vector.\n    \"\"\"\n    tf_idf_vector = tfidf_transformer.transform(count_vectorizer.transform(\n        [text]))\n    return tf_idf_vector\n", "import_code": []}
{"id": "9763b99e-ac9c-3260-b439-d2f139792c7b_3", "content": "def fit_bow_tfidf(text_dataset_array):\n    \"\"\"\n    Fits a BoW model and uses a TF-IDF transformer on it to compute TF-IDF scores.\n    :param text_dataset_array: Numpy array of strings, or pandas series. This means you can pass all values of a\n    pandas data frame like this df[\"column_name\"]\n    :return: Tuple of word count matrix, TF-IDF matrix, BoW model, TF-IDF transformer\n    \"\"\"\n    count_vectorizer = CountVectorizer(min_df=5, stop_words=ENGLISH_STOP_WORDS)\n    X_train_counts = count_vectorizer.fit_transform(text_dataset_array)\n    tfidf_transformer = TfidfTransformer()\n    X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n    return X_train_counts, X_train_tfidf, count_vectorizer, tfidf_transformer\n", "import_code": ["from sklearn.feature_extraction.text import TfidfTransformer\n", "from sklearn.feature_extraction.text import CountVectorizer\n", "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n"]}
{"id": "b46b837b-0355-38e9-b86c-d1a8dc56b4c5_0", "content": "def connect_to_db(db_file):\n    \"\"\"\n    Connect to an SQlite database, if db file does not exist it will be created\n    :param db_file: absolute or relative path of db file\n    :return: sqlite3 connection\n    \"\"\"\n    sqlite3_conn = None\n    try:\n        sqlite3_conn = sqlite3.connect(db_file)\n        return sqlite3_conn\n    except Error as err:\n        print(err)\n        if sqlite3_conn is not None:\n            sqlite3_conn.close()\n", "import_code": ["import sqlite3\n", "from sqlite3 import Error\n", "from sqlite3 import Error\n"]}
{"id": "b46b837b-0355-38e9-b86c-d1a8dc56b4c5_2", "content": "def open_csv_file(csv_file_path):\n    \"\"\"\n    Open and read data from a csv file without headers (skipping the first row)\n    :param csv_file_path: path of the csv file to process\n    :return: a list with the csv content\n    \"\"\"\n    with open(csv_file_path, 'r', encoding='utf-8') as csv_file:\n        reader = csv.reader(csv_file)\n        next(reader)\n        data = list()\n        for row in reader:\n            data.append(row)\n        return data\n", "import_code": ["import csv\n"]}
{"id": "b46b837b-0355-38e9-b86c-d1a8dc56b4c5_3", "content": "def get_column_names_from_db_table(sql_cursor, table_name):\n    \"\"\"\n    Scrape the column names from a database table to a list and convert to a comma separated string, count the number\n    of columns in a database table\n    :param sql_cursor: sqlite cursor\n    :param table_name: table name to get the column names from\n    :return: a comma separated string with column names, an integer with number of columns\n    \"\"\"\n    table_column_names = 'PRAGMA table_info(' + table_name + ');'\n    sql_cursor.execute(table_column_names)\n    table_column_names = sql_cursor.fetchall()\n    column_count = len(table_column_names)\n    column_names = list()\n    for name in table_column_names:\n        column_names.append(name[1])\n    return ', '.join(column_names), column_count\n", "import_code": []}
{"id": "1a1a3d2a-db7f-3d45-bb96-53e02af6d2bd_0", "content": "def convertCompetitionOpen(df):\n    try:\n        date = '{}-{}'.format(int(df['CompetitionOpenSinceYear']), int(df[\n            'CompetitionOpenSinceMonth']))\n        return pd.to_datetime(date)\n    except:\n        return np.nan\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "1a1a3d2a-db7f-3d45-bb96-53e02af6d2bd_1", "content": "def convertPromo2(df):\n    try:\n        date = '{}{}1'.format(int(df['Promo2SinceYear']), int(df[\n            'Promo2SinceWeek']))\n        return pd.to_datetime(date, format='%Y%W%w')\n    except:\n        return np.nan\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "bda6a285-9331-3335-b437-3dbf84566425_0", "content": "def export_to_csv(modeladmin, request, queryset):\n    opts = modeladmin.model._meta\n    response = HttpResponse(content_type='text/csv')\n    response['Content-Disposition'] = 'attachment;filename={}.csv'.format(opts\n        .verbose_name)\n    writer = csv.writer(response)\n    fields = [field for field in opts.get_fields() if not field.\n        many_to_many and not field.one_to_many]\n    writer.writerow([field.verbose_name for field in fields])\n    for obj in queryset:\n        data_row = []\n        for field in fields:\n            value = getattr(obj, field.name)\n            if isinstance(value, datetime.datetime):\n                value = value.strftime('%d/%m/%Y')\n            data_row.append(value)\n        writer.writerow(data_row)\n    return response\n", "import_code": ["import csv\n", "from datetime import datetime, timedelta\n", "from django.http import HttpResponse, HttpResponse\n"]}
{"id": "7a153aa7-e030-3cc6-a801-e957318f6b7f_0", "content": "@app.route('/delete/<int:id1>')\ndef delete(id1):\n    task_to_delete = Todo.query.get_or_404(id1)\n    try:\n        db.session.delete(task_to_delete)\n        db.session.commit()\n        return redirect('/expense')\n    except:\n        return 'There was a problem deleting that task'\n", "import_code": ["from flask import Flask, request, jsonify, render_template, redirect, url_for\n"]}
{"id": "7a153aa7-e030-3cc6-a801-e957318f6b7f_5", "content": "@login_manager.user_loader\ndef load_user(user_id):\n    return User.query.get(int(user_id))\n", "import_code": []}
{"id": "b85aabdd-14aa-3bce-b423-73d040a3c19a_1", "content": "def generateData(start, end, delta):\n    \"\"\"\n    Generates a list of random dates between the start and end dates, spaced by the specified delta.\n    :PARAM start: Start date.\n    :PARAM end: End date.\n    :PARAM delta: Time between each date.\n    \"\"\"\n    curr = start\n    listofDateTime = []\n    while curr < end:\n        listofDateTime.append(curr.date())\n        curr += delta\n    return listofDateTime\n", "import_code": []}
{"id": "b85aabdd-14aa-3bce-b423-73d040a3c19a_2", "content": "def generateSchedule(listofDateTime):\n    \"\"\"\n    Generates a schedule based on the list of dates provided.\n    :PARAM listofDateTime: List of dates.\n    \"\"\"\n    span = [min(listofDateTime), max(listofDateTime)]\n    t = []\n    s = []\n    for key in SCHED:\n        t.append(key)\n        s.append(SCHED[key])\n    t = [datetime.strptime(i, '%H:%M').time() for i in t]\n    d = {}\n    for single_date in daterange(span[0], span[1] + timedelta(days=1)):\n        d[single_date] = [t, s]\n    return d\n", "import_code": ["from datetime import date, datetime, timedelta\n", "from datetime import date, datetime, timedelta\n"]}
{"id": "b85aabdd-14aa-3bce-b423-73d040a3c19a_3", "content": "def expandToList(dictionary):\n    \"\"\"\n    Expands the dictionary of schedules into a list, duplicating dates to fill the list.\n    :PARAM dictionary: Dictionary of schedules.\n    \"\"\"\n    _date = []\n    _time = []\n    _state = []\n    for k in dictionary:\n        for i in dictionary[k][0]:\n            _time.append(i)\n        for i in dictionary[k][1]:\n            _state.append(i)\n        for i in range(0, len(dictionary[k][1])):\n            _date.append(k)\n    n = 0\n    for i in _date:\n        _date[n] = datetime.datetime.combine(_date[n].date(), _time[n])\n        n += 1\n    df = pd.DataFrame({'Datetime': _date, 'Action': _state})\n    return df\n", "import_code": ["import pandas as pd\n", "from datetime import date, datetime, timedelta\n"]}
{"id": "104ef83a-9c03-3877-9adc-25c473ac5715_0", "content": "def are_anagrams(str1, str2):\n    if len(str1) != len(str2):\n        return False\n    dict1 = Counter(str1)\n    dict2 = Counter(str2)\n    for char in dict1:\n        if not char in dict2 or dict2[char] != dict1[char]:\n            return False\n    return True\n", "import_code": ["from collections import Counter\n"]}
{"id": "104ef83a-9c03-3877-9adc-25c473ac5715_1", "content": "def are_anagrams_dict(str1, str2):\n    if len(str1) != len(str2):\n        return False\n    char_dict = {}\n    for char in str1:\n        if not char in char_dict:\n            char_dict[char] = 0\n        char_dict[char] += 1\n    for char in str2:\n        if char not in char_dict or char_dict[char] <= 0:\n            return False\n        char_dict[char] -= 1\n    return True\n", "import_code": []}
{"id": "605f29e5-aee4-3808-a6d1-439b9678a476_0", "content": "def generate_rating_df(df):\n    rating_df = df.groupby(['rating', 'target_ages']).agg({'show_id': 'count'}\n        ).reset_index()\n    rating_df = rating_df[rating_df['show_id'] != 0]\n    rating_df.columns = ['rating', 'target_ages', 'counts']\n    rating_df = rating_df.sort_values('target_ages')\n    return rating_df\n", "import_code": []}
{"id": "605f29e5-aee4-3808-a6d1-439b9678a476_1", "content": "def calculate_mlb(series):\n    mlb = MultiLabelBinarizer()\n    mlb_df = pd.DataFrame(mlb.fit_transform(series), columns=mlb.classes_,\n        index=series.index)\n    return mlb_df\n", "import_code": ["import pandas as pd\n", "from pandas_profiling import ProfileReport\n", "from sklearn.preprocessing import MultiLabelBinarizer\n"]}
{"id": "dafe75b9-1d99-39f4-90c8-77a7239a631c_0", "content": "def Single_letter_eng(text, mostra_num=False):\n    \"\"\"\n    Returns most possible values for each letter, using single letters only. If mostra_num = True, returns the number appearances of each letter.\n    \"\"\"\n    llista_paraules = string.split(string.upper(text))\n    Dic_Res = {}\n    llista_frequency = list('ETAOINSHRDLCUMWFGYPBVKJXQZ')\n    total = 0\n    i = 0\n    for element in string.uppercase:\n        Dic_Res[element] = 0\n    for paraula in llista_paraules:\n        for lletra in paraula:\n            Dic_Res[lletra] += 1\n            total += 1\n    if mostra_num == False:\n        llista_resultat = sorted(Dic_Res.values(), reverse=True)\n        for valor in llista_resultat:\n            for key in Dic_Res.keys():\n                if Dic_Res[key] == valor:\n                    Dic_Res[key] = llista_frequency[i]\n            i += 1\n        return Dic_Res\n    Sorted_Res = sorted(Dic_Res.items(), key=operator.itemgetter(1),\n        reverse=True)\n    return Sorted_Res\n", "import_code": ["import string\n", "import operator\n"]}
{"id": "dafe75b9-1d99-39f4-90c8-77a7239a631c_1", "content": "def Duo_letter_eng(text, filtre=10):\n    \"\"\"\n    Returns the most used two-letter words. Use filtre to set the minimum value to appear on the list\n    \"\"\"\n    text = string.upper(text)\n    text = text.replace(' ', '')\n    llista_lletres = list(text)\n    Dic_Res = {}\n    for i in range(len(llista_lletres) - 1):\n        A_Duo = llista_lletres[i] + llista_lletres[i + 1]\n        if A_Duo in Dic_Res.keys():\n            Dic_Res[A_Duo] += 1\n        else:\n            Dic_Res[A_Duo] = 1\n    for key in Dic_Res.keys():\n        if Dic_Res[key] < filtre:\n            del Dic_Res[key]\n    Sorted_Res = sorted(Dic_Res.items(), key=operator.itemgetter(1),\n        reverse=True)\n    return Sorted_Res\n", "import_code": ["import operator\n", "import string\n"]}
{"id": "dafe75b9-1d99-39f4-90c8-77a7239a631c_2", "content": "def Trio_letter_eng(text, filtre=10):\n    \"\"\"\n    Returns the most used three-letter words. Use filter to set the minimum value to appear on the list\n    \"\"\"\n    text = string.upper(text)\n    text = text.replace(' ', '')\n    llista_lletres = list(text)\n    Dic_Res = {}\n    for i in range(len(llista_lletres) - 2):\n        A_Trio = llista_lletres[i] + llista_lletres[i + 1] + llista_lletres[\n            i + 2]\n        if A_Trio in Dic_Res.keys():\n            Dic_Res[A_Trio] += 1\n        else:\n            Dic_Res[A_Trio] = 1\n    for key in Dic_Res.keys():\n        if Dic_Res[key] < filtre:\n            del Dic_Res[key]\n    Sorted_Res = sorted(Dic_Res.items(), key=operator.itemgetter(1),\n        reverse=True)\n    return Sorted_Res\n", "import_code": ["import operator\n", "import string\n"]}
{"id": "c7669067-c4c2-31d3-b333-17468436212c_1", "content": "def key_word_counter(tuple):\n    return tuple[1]\n", "import_code": []}
{"id": "8ba53ad0-be5f-33f9-a635-e1554e044dac_0", "content": "def evalSameDiagonal(individual):\n    t1 = 0\n    t2 = 0\n    size = len(individual)\n    f1 = []\n    f2 = []\n    for i in range(size):\n        f1.append(individual[i] - i)\n        f2.append(1 + size - individual[i] - i)\n    f1.sort()\n    f2.sort()\n    for i in range(1, size):\n        if f1[i] == f1[i - 1]:\n            t1 = t1 + 1\n        if f2[i] == f2[i - 1]:\n            t2 = t2 + 1\n    fitness = t1 + t2\n    return fitness,\n", "import_code": []}
{"id": "8ba53ad0-be5f-33f9-a635-e1554e044dac_1", "content": "def cxCycle(ind1, ind2):\n    length = len(ind1)\n    index = random.randint(0, length - 1)\n    cycle = [0] * length\n    while not cycle[index]:\n        cycle[index] = 1\n        for i in range(0, length):\n            if ind1[index] == ind2[i]:\n                index = i\n                break\n    for j in range(0, length):\n        if cycle[j] == 1:\n            temp = ind1[j]\n            ind1[j] = ind2[j]\n            ind2[j] = temp\n    return ind1, ind2\n", "import_code": ["import random\n"]}
{"id": "9eb51006-05a9-3d0f-b81f-10159bb576b3_0", "content": "def scrape_page(url):\n    response = get(url)\n    html_soup = BeautifulSoup(response.text, 'html.parser')\n    building_containers = html_soup.find_all('div', class_='mainWrapper')\n    building_list = building_containers[0].script.text\n    json_acceptable_string = building_list.replace(\"'\", '\"')\n    building = json.loads(json_acceptable_string)\n    return pd.DataFrame.from_dict(building['about'])\n", "import_code": ["from bs4 import BeautifulSoup\n", "import pandas as pd\n", "from requests import get\n", "import json\n"]}
{"id": "9eb51006-05a9-3d0f-b81f-10159bb576b3_1", "content": "def get_all_page(url, page):\n    for i in range(1, page + 1):\n        if i == 1:\n            building_info = scrape_page(url)\n        else:\n            url_new = url.split('/', -1)[0] + '/' + url.split('/', -1)[1\n                ] + '/' + url.split('/', -1)[2] + '/' + url.split('/', -1)[3\n                ] + '/' + str(i) + '/' + url.split('/', -1)[4]\n            building_new = scrape_page(url_new)\n            building_info = building_info.append(building_new, ignore_index\n                =True)\n    return building_info\n", "import_code": []}
{"id": "9eb51006-05a9-3d0f-b81f-10159bb576b3_2", "content": "def get_num(url):\n    response = get(url)\n    html_soup = BeautifulSoup(response.text, 'html.parser')\n    page_num = html_soup.find_all('div', class_='paging')\n    next_url = page_num[0].find('a', attrs={'data-page': '2'}).text\n    return page_num\n", "import_code": ["from bs4 import BeautifulSoup\n", "from requests import get\n"]}
{"id": "4c18c1af-f04a-3edb-8e0c-37fd1dd8e87f_0", "content": "def mut2seq(mutations):\n    seq = 'MQYKLILNGKTLKGETTTEAVDAATAEKVFKQYANDNGVDGEWTYDDATKTFTVTE'\n    if len(mutations) == 0:\n        return seq\n    for mut in mutations.split(','):\n        pos = int(mut[1:-1]) - 1\n        newAA = mut[-1]\n        seq = seq[:pos] + newAA + seq[pos + 1:]\n    return seq\n", "import_code": []}
{"id": "93928445-377e-3aff-9d7b-52eea6f89f27_0", "content": "def set_index(df):\n    \"\"\"\n    Convert the 'date' column to a datetime object, set it as the index, and round to the nearest minute.\n    \"\"\"\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index('date')\n    df.index = df.index.round('min')\n    return df\n", "import_code": ["import pandas as pd\n"]}
{"id": "93928445-377e-3aff-9d7b-52eea6f89f27_2", "content": "def create_dataset(X, y, time_steps=1):\n    Xs, ys = [], []\n    for i in range(len(X) - time_steps):\n        v = X.iloc[i:i + time_steps].values\n        Xs.append(v)\n        ys.append(y.iloc[i + time_steps])\n    return np.array(Xs), np.array(ys)\n", "import_code": ["import numpy as np\n"]}
{"id": "c5678765-45ac-33e8-864e-c681e203e568_0", "content": "def generate_train(config):\n    n = sum(1 for line in open('train.csv')) - 1\n    s = config.nrows_train + config.nrows_test\n    skip = sorted(random.sample(range(1, n + 1), n - s))\n    df = pd.read_csv('train.csv', parse_dates=['date',\n        'items_first_enabled_date'], skiprows=skip)\n    msk = np.random.rand(len(df)) < config.nrows_train / (config.\n        nrows_train + config.nrows_test)\n    train_df = df[msk]\n    our_test = df[~msk]\n    return train_df, our_test\n", "import_code": ["import pandas as pd\n", "import numpy as np\n", "import random\n"]}
{"id": "c5678765-45ac-33e8-864e-c681e203e568_1", "content": "def get_train(config):\n    filename_train = os.path.join(os.getcwd(), 'train_' + config.\n        sample_name + '_' + str(config.nrows_train) + '.csv')\n    filename_our_test = os.path.join(os.getcwd(), 'our_test_' + config.\n        sample_name + '_' + str(config.nrows_test) + '.csv')\n    if config.get_new or not os.path.isfile(filename_train):\n        train_df, our_test_df = generate_train(config)\n        train_df.to_csv(filename_train)\n    else:\n        train_df = pd.read_csv(filename_train, parse_dates=['date',\n            'items_first_enabled_date'])\n    return train_df\n", "import_code": ["import pandas as pd\n", "import os\n"]}
{"id": "c5678765-45ac-33e8-864e-c681e203e568_2", "content": "def get_our_test(config):\n    filename_train = os.path.join(os.getcwd(), 'train_' + config.\n        sample_name + '_' + str(config.nrows_train) + '.csv')\n    filename_our_test = os.path.join(os.getcwd(), 'our_test_' + config.\n        sample_name + '_' + str(config.nrows_test) + '.csv')\n    if config.get_new or not os.path.isfile(filename_our_test):\n        train_df, our_test_df = generate_train(config)\n        train_df.to_csv(filename_train)\n        our_test_df.to_csv(filename_our_test)\n    else:\n        our_test_df = pd.read_csv(filename_our_test, parse_dates=['date',\n            'items_first_enabled_date'])\n    return our_test_df\n", "import_code": ["import pandas as pd\n", "import os\n"]}
{"id": "c5678765-45ac-33e8-864e-c681e203e568_3", "content": "def get_test(config):\n    if not config.full_test:\n        n = sum(1 for line in open('train.csv')) - 1\n        s = config.nrows_test\n        skip = sorted(random.sample(range(1, n + 1), n - s))\n        test_df = pd.read_csv('train.csv', parse_dates=['date',\n            'items_first_enabled_date'], skiprows=skip)\n    else:\n        test_df = pd.read_csv('test.csv', parse_dates=['date',\n            'items_first_enabled_date'])\n    return test_df\n", "import_code": ["import pandas as pd\n", "import random\n"]}
{"id": "5ce24a56-6491-3d37-a094-c5a04f568a86_0", "content": "def make_divisors(n):\n    divisors = []\n    for i in range(1, int(n ** 0.5) + 1):\n        if n % i == 0:\n            divisors.append(i)\n            if i != n // i:\n                divisors.append(n // i)\n    divisors.sort(reverse=True)\n    return divisors\n", "import_code": []}
{"id": "9b4c3d8e-17e4-385b-8662-b08e3fd34150_0", "content": "def load_into_dict(file_location):\n    \"\"\"Load data into dict.\"\"\"\n    data_dict = {}\n    data = open(file_location, 'r', encoding='cp1250')\n    csv_reader = csv.reader(data, delimiter=';', quoting=csv.QUOTE_NONNUMERIC)\n    for row in csv_reader:\n        data_dict[row[0]] = row[1]\n    data.close()\n    return data_dict\n", "import_code": ["import csv\n"]}
{"id": "9b4c3d8e-17e4-385b-8662-b08e3fd34150_1", "content": "def sort_date_values(date_dict):\n    \"\"\"Sort date values chronologically - date must be in YYYY-MM-DD format.\"\"\"\n    dates = list(date_dict.keys())\n    dates_sorted = sorted(dates)[1:]\n    values_sorted = []\n    for i in range(len(dates_sorted)):\n        values_sorted.append(date_dict[dates_sorted[i]])\n    return values_sorted\n", "import_code": []}
{"id": "9b4c3d8e-17e4-385b-8662-b08e3fd34150_2", "content": "def normalize(data, data_sum):\n    \"\"\"Normalize data, can be normalized against external sum.\"\"\"\n    normalized = []\n    for item in data:\n        normalized.append(item / data_sum * 100)\n    return normalized\n", "import_code": []}
{"id": "ce63e9c7-2a4a-3ec7-93fb-5d2ef2189668_0", "content": "def generate_spd(eigvals: np.array, hholder: np.array) ->np.array:\n    \"\"\"\n    Generate matrix with concrete eigenvalues and eigenvectors\n    Parameters:\n        eigvals -- eigenvalues of a matrix\n        hholder -- Householder matrix whose columns are eigenvectors\n    \"\"\"\n    return np.matmul(np.matmul(hholder, np.diag(eigvals)), hholder.T)\n", "import_code": ["import numpy as np\n"]}
{"id": "ce63e9c7-2a4a-3ec7-93fb-5d2ef2189668_1", "content": "def generate_eigvals(n: np.int64) ->np.array:\n    \"\"\"\n    Generate n-tuple of random numbers which are going to be eigenvalues of a matrix\n    Parameters:\n        n -- dimension of a matrix\n    \"\"\"\n    return np.random.rand(n).astype('double')\n", "import_code": ["import numpy as np\n"]}
{"id": "ce63e9c7-2a4a-3ec7-93fb-5d2ef2189668_2", "content": "def Householder(n: np.int64) ->np.array:\n    \"\"\"\n    Generate (n,n) Householder matrix which is needed in order to construct a matrix with known eigvals and hholder\n    Parameters:\n        n -- dimension of a matrix\n    \"\"\"\n    w = np.random.rand(n).reshape(1, n)\n    w = w / np.linalg.norm(w)\n    return (np.eye(n) - 2 * w * w.T).astype('double')\n", "import_code": ["import numpy as np\n"]}
{"id": "ce63e9c7-2a4a-3ec7-93fb-5d2ef2189668_3", "content": "def power_iteration(A: np.array, num_iterations: np.int64) ->np.double:\n    \"\"\"\n    Return the greatest (in absolute value) eigenvalue of given (diagonalizable) matrix\n    Parameters:\n        A -- diagonalizable matrix\n    \"\"\"\n    x = np.random.rand(A.shape[0])\n    l = np.double(0)\n    for _ in range(num_iterations):\n        v = x / np.linalg.norm(x)\n        x = np.matmul(A, v)\n        l = np.matmul(v.T, x)\n    return l\n", "import_code": ["import numpy as np\n"]}
{"id": "ce63e9c7-2a4a-3ec7-93fb-5d2ef2189668_4", "content": "def inverse_iteration(A: np.array, num_iterations: np.int64) ->np.double:\n    \"\"\"\n    Return the greatest (in absolute value) eigenvalue of an inverse matrix of given (diagonalizable) matrix\n    Parameters:\n        A -- diagonalizable matrix\n    \"\"\"\n    x = np.random.rand(A.shape[0])\n    l = np.double(0)\n    for _ in range(num_iterations):\n        v = x / np.linalg.norm(x)\n        x = np.linalg.solve(A, v)\n        l = np.matmul(v.T, x)\n    return l\n", "import_code": ["import numpy as np\n"]}
{"id": "b0846774-9b7e-304f-afe0-cc45f3bf98fa_1", "content": "def get_combos_dfs(digits: str) ->List[str]:\n    output = []\n    _get_combos([], digits, output)\n    return [''.join(s) for s in output]\n", "import_code": ["from typing import List\n"]}
{"id": "b0846774-9b7e-304f-afe0-cc45f3bf98fa_2", "content": "def get_combos_itertools(number: str) ->List[str]:\n    if not number:\n        return []\n    combos = itertools.product(*[letter_map[n] for n in number])\n    output = []\n    for c in combos:\n        output.append(''.join(c))\n    return output\n", "import_code": ["import itertools\n", "from typing import List\n"]}
{"id": "9087c5f5-0b35-32f1-bc94-6f9dedbc14be_0", "content": "def get_unique_id(df: pd.DataFrame, *args, join_symbol: str, col_name: str=\n    'ID', drop_cols: bool=False) ->pd.DataFrame:\n    \"\"\"\n    Creates column with unique ID\n\n    This function takes a pandas DataFrame and a list of columns to be converted to string type.\n    It then joins these columns using a specified join symbol and inserts the resulting string as a new column at the beginning of the DataFrame.\n    If the drop_cols parameter is True, the original columns will be dropped from the DataFrame.\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame\n    *args (list): The list of columns to be converted to string type and joined\n    join_symbol (str): The symbol to be used for joining the columns\n    col_name (str): The name of the new column to be created\n    drop_cols (bool): A flag to indicate whether to drop the original columns\n\n    Returns:\n    pd.DataFrame: The modified DataFrame with the new column\n    \"\"\"\n    for col in args:\n        df[col] = df[col].astype(str)\n    plan_item = df[list(args)].agg(join_symbol.join, axis=1)\n    df.insert(0, col_name, plan_item)\n    del plan_item\n    if drop_cols:\n        df.drop(list(args), axis=1, inplace=True)\n    return df\n", "import_code": ["import pandas as pd\n"]}
{"id": "9087c5f5-0b35-32f1-bc94-6f9dedbc14be_1", "content": "def get_lagged_features(df: pd.DataFrame, *args, partitioning_cols: List,\n    lags: List) ->pd.DataFrame:\n    \"\"\"\n    Gets lagged features from normal feature columns\n\n    This function takes a pandas DataFrame and a list of feature columns to be shifted.\n    It then shifts these columns by the specified number of lags and adds them as new columns to the DataFrame.\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame\n    *args (list): The list of feature columns to be shifted\n    partitioning_cols (list): The list of columns to partition the DataFrame by\n    lags (list): The list of lag values to be used for shifting\n\n    Returns:\n    pd.DataFrame: The modified DataFrame with the new lagged feature columns\n    \"\"\"\n    for feat in args:\n        for lag in lags:\n            df[f'{feat}_lagged_{lag}'] = df.groupby(partitioning_cols)[feat\n                ].shift(-lag)\n    return df\n", "import_code": ["import pandas as pd\n", "from typing import List\n"]}
{"id": "9087c5f5-0b35-32f1-bc94-6f9dedbc14be_2", "content": "def scale_data(data: pd.DataFrame, minimum: int, maximum: int) ->pd.DataFrame:\n    scaler = MinMaxScaler()\n    return data.reshape(minimum, maximum)\n", "import_code": ["import pandas as pd\n", "from sklearn.preprocessing import MinMaxScaler\n"]}
{"id": "3978b030-4c97-3eb3-8bdc-8636fec4bac4_0", "content": "def process(data, dtypes):\n    for index, (dtype, col) in enumerate(zip(dtypes, data.columns)):\n        data[col] = data[col].astype(dtype)\n    return data\n", "import_code": []}
{"id": "3978b030-4c97-3eb3-8bdc-8636fec4bac4_2", "content": "def preprocess(data_, scale):\n    cols = data_.columns\n    cols = cols.drop('Churn?')\n    if scale == 'scaler':\n        scaler1 = MinMaxScaler(feature_range=(0, 1))\n        data_[cols] = scaler1.fit_transform(data_[cols])\n    if scale == 'standard':\n        scaler2 = StandardScaler().fit(data_[cols])\n        data_[cols] = scaler2.transform(data_[cols])\n    if scale == 'normal':\n        scaler3 = Normalizer().fit(data_[cols])\n        data_[cols] = scaler3.transform(data_[cols])\n    else:\n        data_ = data_\n    data_ = data_.reset_index()\n    data_ = data_.drop('Unnamed: 0', axis=1)\n    data_['Churn?'] = np.where(data['Churn?'] == 'Yes', 1, 0)\n    data_ = data_.drop('inactive_reason', axis=1)\n    data_ = data_.drop('index', axis=1)\n    return data_\n", "import_code": ["from sklearn.preprocessing import Normalizer\n", "from sklearn.preprocessing import StandardScaler\n", "import numpy as np\n", "from sklearn.preprocessing import MinMaxScaler\n"]}
{"id": "3978b030-4c97-3eb3-8bdc-8636fec4bac4_3", "content": "def encode(data, kind):\n    df = data.copy()\n    if kind == 'onehot':\n        df = pd.get_dummies(data, columns=['Recent_Solicitor',\n            'Priority_Code', 'Postal_Code', 'Country', 'inactive_reason'],\n            prefix=['Solicitor', 'Priority', 'Postal_Code', 'Country',\n            'Reason'])\n    if kind == 'encode':\n        lb_make = LabelEncoder()\n        df['Solicitor_Code'] = lb_make.fit_transform(data['Recent_Solicitor'])\n        df['Prior_Code'] = lb_make.fit_transform(data['Priority_Code'])\n        df['Country_Code'] = lb_make.fit_transform(data['Country'])\n        df = df.drop('Recent_Solicitor', axis=1)\n        df = df.drop('Priority_Code', axis=1)\n        df = df.drop('Postal_Code', axis=1)\n        df = df.drop('Country', axis=1)\n    return df\n", "import_code": ["import pandas as pd\n", "from sklearn.preprocessing import LabelEncoder\n"]}
{"id": "3978b030-4c97-3eb3-8bdc-8636fec4bac4_4", "content": "def tt_split(data_, str_target, train_frac, rs):\n    columns = data_.columns.tolist()\n    columns.remove(str_target)\n    train = data_.sample(frac=train_frac, random_state=rs)\n    test = data_.loc[~data_.index.isin(train.index)]\n    return train, test, columns\n", "import_code": []}
{"id": "3978b030-4c97-3eb3-8bdc-8636fec4bac4_5", "content": "def gs_params(model, X, y, nfolds, param_grid):\n    grid_search = GridSearchCV(model, param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    return grid_search.best_params_\n", "import_code": []}
{"id": "3978b030-4c97-3eb3-8bdc-8636fec4bac4_7", "content": "def resample_data(data):\n    minority = data[data['Churn?'] == 1]\n    majority = data[data['Churn?'] == 0]\n    majority_downsample = resample(majority, replace=False, n_samples=len(\n        minority), random_state=42)\n    df_downsample = pd.concat([majority_downsample, minority])\n    return df_downsample\n", "import_code": ["import pandas as pd\n", "from sklearn.utils import resample\n"]}
{"id": "3978b030-4c97-3eb3-8bdc-8636fec4bac4_8", "content": "def lime_expl(model, explainer):\n    exp = explainer.explain_instance(test_array[1], model.predict_proba,\n        num_features=5)\n    fig = exp.as_pyplot_figure()\n    return exp\n", "import_code": []}
{"id": "d6e9ae84-7fc7-367c-8553-d830f5e1351b_1", "content": "def get_ip_list(list):\n    \"\"\"\n    This function generates a list of IP addresses based on the input list.\n    It takes a list of four elements, each representing a part of the IP address.\n    If any element is -1, it generates all possible values for that part (0 to 255).\n    Otherwise, it uses the provided values.\n    \"\"\"\n    ip_list = []\n    if list[0] is -1:\n        one_list = []\n        for i in range(0, 256):\n            one_list.append(i)\n    else:\n        one_list = [list[0]]\n    if list[1] is -1:\n        two_list = []\n        for i in range(0, 256):\n            two_list.append(i)\n    else:\n        two_list = [list[1]]\n    if list[2] is -1:\n        three_list = []\n        for i in range(0, 256):\n            three_list.append(i)\n    else:\n        three_list = [list[2]]\n    if list[3] is -1:\n        four_list = []\n        for i in range(0, 256):\n            four_list.append(i)\n    else:\n        four_list = [list[3]]\n    for _one in one_list:\n        for _two in two_list:\n            for _three in three_list:\n                for _four in four_list:\n                    ip_list.append(str(_one) + '.' + str(_two) + '.' + str(\n                        _three) + '.' + str(_four))\n    return ip_list\n", "import_code": []}
{"id": "33f4aba5-d491-31db-b26c-d3b0ebe81bd8_0", "content": "def get_outbreak_begins(n: int, outbreak_length: int, n_outbreaks: int) ->Set[\n    int]:\n    \"\"\"\n    Generate possible outbreak starts and randomly select them.\n\n    Parameters\n    ----------\n    n : int\n        Total number of weeks.\n    outbreak_length : int\n        Number of weeks each outbreak is long.\n    n_outbreaks : int\n        Number of outbreaks.\n\n    Returns\n    -------\n    Set[int]\n        Set of randomly selected outbreak starts.\n    \"\"\"\n    possible_outbreaks_starts = set(range(outbreak_length, n -\n        outbreak_length - 1))\n    outbreaks_starts = set()\n    for _ in range(n_outbreaks):\n        start = random.choice(tuple(possible_outbreaks_starts))\n        outbreaks_starts.add(start)\n        for i in range(start - outbreak_length, start + outbreak_length * 2):\n            try:\n                possible_outbreaks_starts.remove(i)\n            except KeyError:\n                continue\n    return outbreaks_starts\n", "import_code": ["import random\n", "from typing import Set\n"]}
{"id": "33f4aba5-d491-31db-b26c-d3b0ebe81bd8_1", "content": "def simulate_outbreaks(n: int=104, outbreak_length: int=5, n_outbreaks: int\n    =3, mu: float=1, outbreak_mu: float=10) ->pd.DataFrame:\n    \"\"\"\n    Simulate outbreaks based on Poisson distribution.\n\n    Parameters\n    ----------\n    n : int, optional\n        Number of weeks. Default is 104.\n    outbreak_length : int, optional\n        Number of weeks each outbreak is long. Default is 5.\n    n_outbreaks : int, optional\n        Number of outbreaks. Default is 3.\n    mu : float, optional\n        Mean for the baseline. Default is 1.\n    outbreak_mu : float, optional\n        Mean for the outbreaks. Default is 10.\n\n    Returns\n    -------\n    pd.DataFrame\n        Simulated case counts per week, separated into baseline and outbreak cases.\n    \"\"\"\n    baseline = stats.poisson.rvs(mu=mu, size=n)\n    n_outbreak_cases = np.zeros_like(baseline)\n    n_cases = baseline.copy()\n    outbreaks_starts = get_outbreak_begins(n, outbreak_length, n_outbreaks)\n    for start in outbreaks_starts:\n        outbreak_cases = stats.poisson.rvs(mu=outbreak_mu, size=outbreak_length\n            )\n        outbreak_cases += outbreak_cases == 0\n        n_cases[start:start + outbreak_length] += outbreak_cases\n        n_outbreak_cases[start:start + outbreak_length] = outbreak_cases\n    data = pd.DataFrame({'n_cases': n_cases, 'n_outbreak_cases':\n        n_outbreak_cases, 'outbreak': n_outbreak_cases > 0, 'baseline':\n        baseline}, index=pd.date_range(start='2020', periods=baseline.size,\n        freq='W-MON'))\n    data.index.name = 'date'\n    return data\n", "import_code": ["import pandas as pd\n", "import numpy as np\n", "from scipy import stats\n"]}
{"id": "f19292d5-c162-36ee-8b82-c825e3df05bf_0", "content": "def get_polygons_of_loccode(geo_df, dissolveby='OA11CD', search=None):\n    \"\"\"\n    Gets the polygon for a place based on it name, LSOA code or OA code\n\n    Parameters:\n    geo_df: (gpd.DataFrame): Geospatial dataframe containing the data\n    loc_code = LSOA11CD, OA11CD or LSOA11NM\n    search = search terms to find in the LSOA11NM column. Only needed if\n        intending to dissolve on a name in the LSOA11NM column\n    Returns: (gpd.DataFrame) aggregated multipolygons, aggregated on LSOA,\n        OA code, or a search in the LSOA11NM column\n    \"\"\"\n    if dissolveby in ['LSOA11CD', 'OA11CD']:\n        polygon_df = geo_df.dissolve(by=dissolveby)\n    else:\n        filtered_df = geo_df[geo_df[f'{dissolveby}'].str.contains(search)]\n        filtered_df.insert(0, 'place_name', search)\n        polygon_df = filtered_df.dissolve(by='place_name')\n    polygon_df = gpd.GeoDataFrame(polygon_df.pop('geometry'))\n    return polygon_df\n", "import_code": ["import geopandas as gpd\n"]}
{"id": "f19292d5-c162-36ee-8b82-c825e3df05bf_1", "content": "def demarc_urb_rural(urbDef):\n    \"\"\"\n    Creates spatial clusters of urban environments based on specified\n        definition of 'urban'.\n        - 'engwls' for the English/Welsh definition of urban\n        - 'scott' for the Scottish definition of urban\n        - 'euro' for the European definition of urban\n\n    Parameters:\n        urbDef (str): the definition of urban to be used\n    Returns: TBC (probably a polygon)\n    \"\"\"\n    return None\n", "import_code": []}
{"id": "f19292d5-c162-36ee-8b82-c825e3df05bf_2", "content": "def buffer_points(geo_df, metres=500):\n    \"\"\"\n    Provide a Geo Dataframe with points you want buffering.\n    Draws a 5km (radius) buffer around the points.\n    Puts the results into a new column called \"buffered\"\n    As 'epsg:27700' projections units of km, 500m is 0.5km.\n    \"\"\"\n    geo_df['geometry'] = geo_df.geometry.buffer(metres)\n    return geo_df\n", "import_code": []}
{"id": "f19292d5-c162-36ee-8b82-c825e3df05bf_3", "content": "def find_points_in_poly(geo_df, polygon_obj):\n    \"\"\"\n    Find points in polygon using geopandas' spatial join\n        which joins the supplied geo_df (as left_df) and the\n        polygon (as right_df).\n\n        Then drops all rows where the point is not in the polygon\n        (based on column index_right not being NaN). Finally it\n        drop all column names from that were created in the join,\n        leaving only the columns of the original geo_df\n\n        Arguments:\n            geo_df (gpg.DataFrame): a geo pandas dataframe\n            polygon_obj (string): a geopandas dataframe with a polygon column\n\n        Returns:\n            A geodata frame with the points inside the supplied polygon\n    \"\"\"\n    wanted_cols = geo_df.columns.to_list()\n    joined_df = gpd.sjoin(geo_df, polygon_obj, how='left', op='intersects')\n    filtered_df = joined_df[joined_df['index_right'].notna()]\n    filtered_df = filtered_df[wanted_cols]\n    return filtered_df\n", "import_code": ["import geopandas as gpd\n"]}
{"id": "f19292d5-c162-36ee-8b82-c825e3df05bf_4", "content": "def poly_from_polys(geo_df):\n    \"\"\"\n    Makes a combined polygon from the multiple polygons in a geometry\n        column in a geo dataframe.\n\n    Args:\n        geo_df (gpd.DataFrame): Geospatial dataframe containing the data\n\n    Returns:\n        class Polygon : a combined polygon which is the perimeter of the\n            polygons provided.\n    \"\"\"\n    poly = unary_union(list(geo_df.geometry))\n    return poly\n", "import_code": ["from shapely.ops import unary_union\n"]}
{"id": "f19292d5-c162-36ee-8b82-c825e3df05bf_5", "content": "def ward_nrthng_eastng(district, ward):\n    \"\"\"\n    Gets the eastings and northings of a ward in a metropolitan area\n\n    Args:\n        district (str): The district geo code\n        ward (str): The ward geo code\n\n    Returns:\n        dict : A dictionary with the minimum and maximum eastings and northings\n            of the ward.\n    \"\"\"\n    csvurl = (\n        f'https://www.doogal.co.uk/AdministrativeAreasCSV.ashx?district={district}&ward={ward}'\n        )\n    df = pd.read_csv(csvurl, usecols=['Easting', 'Northing'])\n    eastings = [easting for easting in df.Easting]\n    northings = [northing for northing in df.Northing]\n    mins_maxs = {'e_min': min(eastings), 'e_max': max(eastings), 'n_min':\n        min(northings), 'n_max': max(northings)}\n    return mins_maxs\n", "import_code": ["import geopandas as gpd\n", "import pandas as pd\n"]}
{"id": "f19292d5-c162-36ee-8b82-c825e3df05bf_6", "content": "def filter_stops_by_ward(df, mins_maxs):\n    \"\"\"\n    Makes a filtered dataframe (used for the filtering the stops dataframe)\n        based on northings and eastings.\n\n    Args:\n        df (pd.DataFrame): The full dataframe to be filtered\n        mins_maxs (dict): A dictionary with the mins and maxes of the eastings\n            and northings of the area to be filtered\n\n    Returns:\n        pd.DataFrame : A filtered dataframe, limited by the eastings and\n            northings supplied\n    \"\"\"\n    mm = mins_maxs\n    nrth_mask = (mm['n_min'] < df['Northing']) & (df['Northing'] < mm['n_max'])\n    east_mask = (mm['e_min'] < df['Easting']) & (df['Easting'] < mm['e_max'])\n    filtered_df = df[nrth_mask & east_mask]\n    return filtered_df\n", "import_code": []}
{"id": "42b29896-4afc-3fe1-9cf5-2a387a783a86_0", "content": "def _coords_to_indices(density, lat, lon):\n    lat = int((lat + 90) * density)\n    lon = int((lon + 180) * density)\n    return lat, lon\n", "import_code": []}
{"id": "42b29896-4afc-3fe1-9cf5-2a387a783a86_1", "content": "def _indices_to_coords(density, lat, lon):\n    lat = float(lat) / density - 90\n    lon = float(lon) / density - 180\n    return lat, lon\n", "import_code": []}
{"id": "42b29896-4afc-3fe1-9cf5-2a387a783a86_2", "content": "def _minmax_coords(center_coord, radius):\n    return center_coord - radius, center_coord + radius + 1\n", "import_code": []}
{"id": "42b29896-4afc-3fe1-9cf5-2a387a783a86_4", "content": "def _make_grid(ping_results, density):\n    probes_grid = defaultdict(list)\n    for _, _, _, _, _, lat, lon, rtt in ping_results:\n        for coord in _coords_in_circle(density, lat, lon):\n            probes_grid[coord].append(rtt)\n    final_grid = np.full((180 * density + 1, 360 * density), BASE_VALUE)\n    for (lat_index, lon_index), rtts in probes_grid.items():\n        final_grid[lat_index][lon_index] = max(30, sum(rtts) / len(rtts))\n    return final_grid\n", "import_code": ["import numpy as np\n", "from collections import defaultdict\n"]}
{"id": "95009865-16ab-3d4b-b07e-dbf48b1ea936_0", "content": "def basic_stats(v):\n    try:\n        info = {'ave': np.mean(v), 'std': np.std(v), 'med': np.median(v),\n            'low': np.min(v), 'hi': np.max(v)}\n    except ValueError as e:\n        info = {'value error': 0}\n    s = ''\n    for key, value in info.items():\n        s = s + key + ': %.3f' % value + '\\n'\n    return s\n", "import_code": ["import numpy as np\n"]}
{"id": "d451a848-cbc6-3ff5-88e7-ebc4ab512923_0", "content": "def bubblesort(arr):\n    n = len(arr)\n    for i in range(n):\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n    return arr\n", "import_code": []}
{"id": "d451a848-cbc6-3ff5-88e7-ebc4ab512923_1", "content": "def bubblesort_optimized(arr):\n    n = len(arr)\n    for i in range(n):\n        swapped = False\n        for j in range(0, n - i - 1):\n            if arr[j] > arr[j + 1]:\n                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n                swapped = True\n        if not swapped:\n            break\n    return arr\n", "import_code": []}
{"id": "3e1ec9e3-a0d7-3ea3-b888-ce264bbfbafb_1", "content": "def orderDataFrameBySeverity(df):\n    return df[['trace', 'debug', 'info', 'warn', 'error', 'fatal']]\n", "import_code": []}
{"id": "42dcf12b-af5c-391c-8e48-a551787a0097_5", "content": "def Freq(text, type, top=10):\n    if type == 'most':\n        a = sorted(list(text.values()))[-top:]\n    elif type == 'least':\n        a = sorted(list(text.values()))[:top]\n    words = []\n    for word, times in text.items():\n        if text[word] in a:\n            words.append(word)\n    return words[:top]\n", "import_code": []}
{"id": "42dcf12b-af5c-391c-8e48-a551787a0097_6", "content": "def GenerateText(sentences):\n    generated = ''\n    for _ in range(sentences):\n        word = choice(all_words)\n        generated += word[0].upper() + word[1:]\n        k = 0\n        while generated[-1] not in '.!?' and word not in end_words or k < 3:\n            if word in second_words:\n                word = choice(second_words[word])\n                generated += ' ' + word\n            else:\n                word = choice(all_words)\n            k += 1\n        if generated[-1] not in punctuation:\n            generated += choice(end_words[word])\n        generated += ' '\n    return generated[:-1]\n", "import_code": ["from random import choice, randint\n", "from string import punctuation\n"]}
{"id": "42dcf12b-af5c-391c-8e48-a551787a0097_11", "content": "def Bigrams(text):\n    my_bigrams = {}\n    i = 0\n    while i < len(text) - 1:\n        bigram = '{0} {1}'.format(text[i], text[i + 1])\n        if bigram in my_bigrams:\n            my_bigrams[bigram] = my_bigrams[bigram] + 1\n            i += 1\n        else:\n            my_bigrams[bigram] = 1\n            i += 1\n    return my_bigrams\n", "import_code": []}
{"id": "d31e5363-c169-31aa-9a1d-93814c8b939d_0", "content": "def color_picker(elevation):\n    minimum = int(min(vol_df['ELEV']))\n    maximum = int(max(vol_df['ELEV']))\n    step = int((maximum - minimum) / 3)\n    if elevation in range(minimum, minimum + step):\n        return 'green'\n    elif elevation in range(minimum + step, minimum + step * 2):\n        return 'orange'\n    else:\n        return 'red'\n", "import_code": []}
{"id": "5adc19f0-0f3d-3357-b6cc-74f075569d48_1", "content": "def format_email_subject(quote):\n    \"\"\"\n    This functions formats the subject field of the email to be send to the user as configured in bitstampconfig.py\n    :param quote: The current quote values to be inserted on the subject of the email\n    :return: the email subject to be sent with the current quote\n    \"\"\"\n    return 'Bitstamp Last Quote: {0} USD'.format(quote['last'])\n", "import_code": []}
{"id": "5adc19f0-0f3d-3357-b6cc-74f075569d48_2", "content": "def format_email_body(quote):\n    \"\"\"\n    The Email Body containing all quote information from bitstamp\n    :param quote: The current quote values to be inserted on the body of the email\n    :return: the email body to be sent with the current quote\n    \"\"\"\n    return (\n        \"\"\"\n            Bitstamp Current: {0} USD\n            Bitstamp Open   : {1} USD (Last 24 hours)\n            Bitstamp High   : {2} USD (Last 24 hours)\n            Bitstamp Low    : {3} USD (Last 24 hours)\n            Bitstamp Close  : {4} USD (Last 24 hours)\n            Bitstamp Volume : {5} BTC (Last 24 hours)\n            \"\"\"\n        .format(quote['last'], quote['open'], quote['high'], quote['low'],\n        quote['last'], quote['volume']))\n", "import_code": []}
{"id": "80ab97ea-4ea5-38a9-9360-f8e9046184df_0", "content": "def analyzeInstagramMessages(filename):\n    with open(filename) as json_data:\n        data = json.load(json_data)\n        df = pd.DataFrame.from_dict(data['messages'])\n        df2 = pd.DataFrame.from_dict(data['participants'])\n        return df\n", "import_code": ["import pandas as pd\n", "from pandas.io.json import json_normalize\n", "import json\n", "from pandas.io.json import json_normalize\n"]}
{"id": "80ab97ea-4ea5-38a9-9360-f8e9046184df_2", "content": "def analyzeSnapchatLocationHistory(filename):\n    with open(filename) as json_data:\n        data = json.load(json_data)\n        df4 = pd.DataFrame.from_dict(data['Location History'])\n        return df4\n", "import_code": ["import pandas as pd\n", "from pandas.io.json import json_normalize\n", "import json\n", "from pandas.io.json import json_normalize\n"]}
{"id": "80ab97ea-4ea5-38a9-9360-f8e9046184df_7", "content": "def isNoun(lines):\n    is_noun = lambda pos: pos[:2] == 'NN'\n    tokenized = nltk.word_tokenize(lines)\n    nouns = [word for word, pos in nltk.pos_tag(tokenized) if is_noun(pos)]\n    return nouns\n", "import_code": []}
{"id": "95962dde-17e0-370b-b9f8-fa63f76356b9_0", "content": "def fib(max):\n    n, a, b = 0, 0, 1\n    while n < max:\n        print(b)\n        a, b = b, a + b\n        n = n + 1\n    return 'done'\n", "import_code": []}
{"id": "95962dde-17e0-370b-b9f8-fa63f76356b9_1", "content": "def fib(max):\n    n, a, b = 0, 0, 1\n    while n < max:\n        yield b\n        a, b = b, a + b\n        n = n + 1\n    return 'done'\n", "import_code": []}
{"id": "05572c73-fc35-3e25-a3fe-8bef6480b9a6_0", "content": "def walker(steps):\n    initial_pos = 0\n    x = initial_pos\n    step_size = 1\n    x_data = []\n    step_data = []\n    for i in range(0, steps):\n        x = x + step_size * rnd.choice([1, -1])\n        x_data.append(x)\n        step_data.append(i + 1)\n    return x_data, step_data\n", "import_code": ["import random as rnd\n"]}
{"id": "d47c5838-dedd-3409-a953-5362c009bd69_1", "content": "def read_files(path):\n    \"\"\"\n    From the path given  it will try to first read gzip it by checking the magic number\n    If the files is not gzip I will try to read anyway but the read will throw an error for us to see.\n    All errors are raised to the function that invoked read_files\n    :param path:\n    :return:\n    \"\"\"\n    try:\n        magic_number = '\\x1f', '\\x8b', '\\x08', '\\x00'\n        bytes_check = None\n        with open(path, 'rb') as handle:\n            bytes_check = unpack('cccc', handle.read(4))\n        if magic_number == bytes_check:\n            with gzip.open(path, 'rb') as f:\n                file_content = f.read()\n            return json.loads(file_content.decode('utf-8'))\n        else:\n            with open(path, 'rt') as f:\n                file_content = f.read()\n            return json.loads(file_content.decode('utf-8'))\n    except Exception as e:\n        raise e\n", "import_code": ["import gzip\n", "from struct import unpack\n", "import json\n"]}
{"id": "d47c5838-dedd-3409-a953-5362c009bd69_2", "content": "def create_events_objects(path, recursive):\n    \"\"\"\n    From the path given in the command  I will try to read the files under it.\n    If recursive, then walk the full path\n    It will return a list of event from each file we read.\n    :param path:\n    :param recursive:\n    :return:\n    \"\"\"\n    events = {}\n    all_files = []\n    if recursive:\n        for root, dirs, files in walk(path):\n            for f in files:\n                all_files.append(join(root, f))\n    else:\n        all_files = [join(path, f) for f in listdir(path) if isfile(join(\n            path, f))]\n    if not all_files:\n        print('Could not find files to read. Exiting...')\n        sys.exit()\n    for file_path in all_files:\n        try:\n            data = read_files(file_path)\n            for n in data['Records']:\n                if n['userIdentity']['type'] in ['IAMUser']:\n                    name = n['userIdentity']['userName']\n                if n['userIdentity']['type'] in ['AssumedRole', 'FederatedUser'\n                    ]:\n                    try:\n                        name = n['userIdentity']['sessionContext'][\n                            'sessionIssuer']['userName']\n                    except KeyError as e:\n                        name = n['userIdentity']['principalId']\n                        pass\n                if n['userIdentity']['type'] in ['Root']:\n                    name = n['userIdentity']['type']\n                events[n['eventID']] = [n['eventTime'], n['eventName'],\n                    name, file_path]\n        except Exception as e:\n            print('Could not open file {path}. Error:{error}'.format(path=\n                path, error=e))\n    return events\n", "import_code": ["import sys\n", "from os.path import isfile, join, isdir\n", "from os import listdir, walk\n", "from os import listdir, walk\n", "from os.path import isfile, join, isdir\n"]}
{"id": "da7c1501-36d5-300c-9170-4b780cb42ef3_0", "content": "def generate_plaintext_random(plain_vocab, distribution):\n    \"\"\"Generates samples of text from the provided vocabulary.\n    Returns:\n        train_indices (np.array of Integers): random integers generated for training.\n            shape = [num_samples, length]\n        test_indices (np.array of Integers): random integers generated for testing.\n            shape = [num_samples, length]\n        plain_vocab     (list of Integers): unique vocabularies.\n    \"\"\"\n    plain_vocab = _EXTRA_VOCAB_ITEMS + plain_vocab.split(',')\n    distribution = None if distribution is None else [float(x.strip()) for\n        x in distribution.split(',')]\n    assert distribution is None or sum(distribution) == 1.0\n    train_samples = num_train\n    test_samples = num_test\n    length = sample_length\n    train_indices = np.random.choice(range(_CROP_AMOUNT, len(plain_vocab)),\n        (train_samples, length), p=distribution)\n    test_indices = np.random.choice(range(_CROP_AMOUNT, len(plain_vocab)),\n        (test_samples, length), p=distribution)\n    return train_indices, test_indices, plain_vocab\n", "import_code": ["import numpy as np\n"]}
{"id": "da7c1501-36d5-300c-9170-4b780cb42ef3_2", "content": "def encipher_shift(plaintext, plain_vocab, shift, separate_domains=False):\n    \"\"\"Encrypt plain text with a single shift layer\n    Args:\n        plaintext (list of list of Strings): a list of plain text to encrypt.\n        plain_vocab (list of Integer): unique vocabularies being used.\n        shift (Integer): number of shift, shift to the right if shift is positive.\n    Returns:\n        ciphertext (list of Strings): encrypted plain text.\n    \"\"\"\n    ciphertext = []\n    cipher = Layer(range(_CROP_AMOUNT, len(plain_vocab)), shift)\n    for i, sentence in enumerate(plaintext):\n        cipher_sentence = []\n        for j, character in enumerate(sentence):\n            encrypted_char = cipher.encrypt_character(character)\n            if separate_domains:\n                encrypted_char += len(plain_vocab) - _CROP_AMOUNT\n            cipher_sentence.append(encrypted_char)\n        ciphertext.append(cipher_sentence)\n    return ciphertext\n", "import_code": []}
{"id": "da7c1501-36d5-300c-9170-4b780cb42ef3_4", "content": "def cipher_generator(vocab_path, output_dir='data', cipher='vigenere',\n    separate_domains=False):\n    plain_vocab = load_vocab(vocab_path)\n    train_plain = load_data(plain_vocab, output_dir + '/train.txt')\n    test_plain = load_data(plain_vocab, output_dir + '/test.txt')\n    if cipher == 'shift':\n        shift = shift_amount\n        if shift == -1:\n            shift = np.random.randint(100000.0)\n        train_cipher = encipher_shift(train_plain, plain_vocab, shift)\n        test_cipher = encipher_shift(test_plain, plain_vocab, shift)\n    elif cipher == 'vigenere':\n        key = [int(c) for c in vigenere_key]\n        train_plain_cipher = encipher_vigenere(train_plain, plain_vocab, key)\n        test_plain_cipher = encipher_vigenere(test_plain, plain_vocab, key)\n    else:\n        raise Exception('Unknown cipher %s' % cipher)\n    return train_plain_cipher, test_plain_cipher, plain_vocab\n", "import_code": ["import numpy as np\n"]}
{"id": "da7c1501-36d5-300c-9170-4b780cb42ef3_5", "content": "def string2index(sentences, vocab):\n    \"\"\"Convert string to its corresponding index\n    i.e. A -> 0, B -> 1 ... for vocab [A,B,...]\n    Args:\n      sentences (np.array of String): list of String to convert.\n       shape = [num_samples, length]\n      vocab (list of String): list of vocabulary\n    Returns:\n          index (np.array of Integer): list of Integer after conversion\n            shape = [num_samples, length]\n    \"\"\"\n    alphabet_index = list(range(len(vocab)))\n    mapping = dict(zip(vocab, alphabet_index))\n    index = []\n    for i in range(len(sentences)):\n        sentence = []\n        for j in range(len(sentences[i])):\n            sentence.append(mapping[sentences[i, j]])\n        index.append(sentence)\n    return index\n", "import_code": []}
{"id": "da7c1501-36d5-300c-9170-4b780cb42ef3_6", "content": "def trim_vocab(word_frequency, vocab_size):\n    \"\"\"Given the max vocab size n, trim the word_frequency dictionary to only contain the\n    top n occurring words\n    Args:\n      word_frequency (Dictionary): dictionary of word, frequency pairs.\n      vocab_size (Integer): the maximum number of vocabulary allowed.\n    Returns:\n      retval (Dictionary): dictionary containing the top n occurring words as keys\n    \"\"\"\n    sorted_vocab = sorted(word_frequency.items(), key=operator.itemgetter(1\n        ), reverse=True)\n    max_count = min(len(word_frequency), vocab_size)\n    retval = [k for k, _ in sorted_vocab[:max_count]]\n    return retval\n", "import_code": ["import operator\n"]}
{"id": "da7c1501-36d5-300c-9170-4b780cb42ef3_7", "content": "def determine_frequency(corpus, character_level):\n    \"\"\"Go through corpus and determine frequency of each individual word\n    Args:\n      corpus (CategorizedTaggedCorpusReader): corpus object for the corpus being used\n    Returns:\n      unique_word_count (Dictionary): dictionary of word keys and corresponding frequency\n          value\n    \"\"\"\n    unique_word_count = dict()\n    lengths = []\n    for sentence in corpus.sents():\n        if not character_level:\n            lengths.append(len(sentence))\n        else:\n            lengths.append(sum(len(word) for word in sentence))\n        for word in sentence:\n            if character_level:\n                for character in word:\n                    if not character.lower() in unique_word_count:\n                        unique_word_count[character.lower()] = 1\n                    else:\n                        unique_word_count[character.lower()] += 1\n            elif not word.lower() in unique_word_count:\n                unique_word_count[word.lower()] = 1\n            else:\n                unique_word_count[word.lower()] += 1\n    print('Average sentence length: %d' % (sum(lengths) / len(lengths)))\n    print('Max sentence length: %d' % max(lengths))\n    print('Min sentence length: %d' % min(lengths))\n    return unique_word_count\n", "import_code": []}
{"id": "da7c1501-36d5-300c-9170-4b780cb42ef3_8", "content": "def tokenize_corpus(corpus, vocabulary, additional_items, character_level,\n    insert_unk):\n    \"\"\"Translate string words into int ids\n    Args:\n      corpus (CategorizedTaggedCorpusReader): corpus object for the corpus being used.\n      vocabulary (Dictionary): vocabulary being used. Also write vocab mapping to file.\n    Returns:\n      tokenized_corpus (Dictionary): tokenized corpus.\n      ordered_vocab (list of String): ordered vocabulary\n    \"\"\"\n    unique_words = dict()\n    vocab = dict()\n    tokenized_corpus = []\n    shift = len(additional_items)\n    unique_count = shift + 1\n    for i, item in enumerate(additional_items):\n        vocab[i] = item\n    vocab[shift] = '<unk>'\n    for sentence in corpus.sents():\n        tokenized_sentence = []\n        for word in sentence:\n            word = word.lower()\n            if character_level:\n                for character in word:\n                    if not character in unique_words:\n                        if character in vocabulary:\n                            unique_words[character] = unique_count\n                            vocab[unique_count] = character\n                            unique_count += 1\n                        else:\n                            unique_words[character] = shift\n                    if unique_words[character] == shift and insert_unk:\n                        tokenized_sentence.append(unique_words[character])\n                    elif unique_words[character] != shift:\n                        tokenized_sentence.append(unique_words[character])\n            else:\n                if not word in unique_words:\n                    if word in vocabulary:\n                        unique_words[word] = unique_count\n                        vocab[unique_count] = word\n                        unique_count += 1\n                    else:\n                        unique_words[word] = shift\n                if unique_words[word] == shift and insert_unk:\n                    tokenized_sentence.append(unique_words[word])\n                elif unique_words[word] != shift:\n                    tokenized_sentence.append(unique_words[word])\n        if len(tokenized_sentence) > 0:\n            tokenized_corpus.append(tokenized_sentence)\n    sorted_vocab = sorted(vocab.items(), key=lambda a: a[0])\n    ordered_vocab = [v for _, v in sorted_vocab]\n    return tokenized_corpus, ordered_vocab\n", "import_code": []}
{"id": "da7c1501-36d5-300c-9170-4b780cb42ef3_10", "content": "def load_vocab(path='data/vocab.txt'):\n    vocab = []\n    with open(path) as f:\n        for line in f:\n            vocab.append(line.strip())\n    return vocab\n", "import_code": []}
{"id": "1648a3cb-44a4-3d86-842a-a2827050df20_0", "content": "def harris_points_detector(image: np.array, thresh: float) ->List[cv2.KeyPoint\n    ]:\n    \"\"\"\n    This function detects Harris corners in an image and returns a list of keypoints.\n    The Harris corner detector computes a measure of cornererness for each pixel in the image.\n    The function uses the Sobel operator to compute the gradients in the x and y directions.\n    It then computes the Harris matrix components Ixx, Iyy, and Ixy, and applies a Gaussian filter to these components.\n    The function then finds the local maxima in the Harris response matrix, which correspond to potential corners.\n    The function finally applies a threshold to the Harris response values and returns a list of keypoints with their positions and orientations.\n    \"\"\"\n    global GAUSSIAN_WINDOW, SIGMA, LOCAL_MAX_WINDOW\n    image = image.astype(np.float32)\n    image /= image.max()\n    image_dx = sobel(image, axis=1, mode='reflect')\n    image_dy = sobel(image, axis=0, mode='reflect')\n    Ixx = image_dx ** 2\n    Iyy = image_dy ** 2\n    Ixy = image_dx * image_dy\n    kernel1d = cv2.getGaussianKernel(GAUSSIAN_WINDOW, SIGMA)\n    kernel2d = np.outer(kernel1d, kernel1d.transpose())\n    SIxx = convolve(Ixx, kernel2d, mode='reflect')\n    SIyy = convolve(Iyy, kernel2d, mode='reflect')\n    SIxy = convolve(Ixy, kernel2d, mode='reflect')\n    det = SIxx * SIyy - SIxy ** 2\n    trace = SIxx + SIyy\n    corner_mat = det - 0.1 * trace ** 2\n    angles_mat = np.arctan2(image_dy, image_dx)\n    np.nan_to_num(angles_mat, nan=0.0)\n    maxima_mat = corner_mat == maximum_filter(corner_mat, size=\n        LOCAL_MAX_WINDOW, mode='reflect')\n    kps: List[cv2.KeyPoint] = list()\n    for row_idx in range(maxima_mat.shape[0]):\n        for col_idx in range(maxima_mat.shape[1]):\n            if maxima_mat[row_idx, col_idx]:\n                if corner_mat[row_idx, col_idx] >= thresh:\n                    kp = cv2.KeyPoint(x=float(col_idx), y=float(row_idx),\n                        _size=1, _angle=angles_mat[row_idx, col_idx])\n                    kps.append(kp)\n    return kps\n", "import_code": ["import numpy as np\n", "from typing import List\n", "from scipy.ndimage import sobel, maximum_filter, convolve\n", "import cv2\n", "from scipy.ndimage import sobel, maximum_filter, convolve\n", "from scipy.ndimage import sobel, maximum_filter, convolve\n"]}
{"id": "5dc1c3b2-6fda-33af-928c-5d00ce1002b5_0", "content": "def fun(x):\n    return np.cos(x) * np.exp(x)\n", "import_code": ["import numpy as np\n"]}
{"id": "050431a7-9b44-359e-a5d2-f474255d27fd_0", "content": "def rand_array(steps):\n    rand_array = np.ceil(rng(steps) * 2) * 2 - 3\n    return rand_array\n", "import_code": ["from numpy.random import random as rng\n", "import numpy as np\n", "from numpy.random import random as rng\n"]}
{"id": "050431a7-9b44-359e-a5d2-f474255d27fd_1", "content": "def gen_data(steps):\n    init1_data = rand_array(steps - 1)\n    init2_data = rand_array(steps - 1)\n    x_data = np.zeros(steps)\n    y_data = np.zeros(steps)\n    for i in range(init1_data.size):\n        x_data[i + 1] = x_data[i] + init1_data[i]\n        y_data[i + 1] = y_data[i] + init2_data[i]\n    return x_data, y_data\n", "import_code": ["import numpy as np\n", "from numpy.random import random as rng\n"]}
{"id": "050431a7-9b44-359e-a5d2-f474255d27fd_3", "content": "def animate(i):\n    xd = x1[0:i]\n    yd = y1[0:i]\n    line.set_data(xd, yd)\n    return line,\n", "import_code": []}
{"id": "050431a7-9b44-359e-a5d2-f474255d27fd_4", "content": "def end_point(steps):\n    x_data, y_data = gen_data(steps)\n    x_end = x_data[-1]\n    y_end = y_data[-1]\n    distance = np.sqrt(x_end ** 2 + y_end ** 2)\n    return x_end, y_end, distance\n", "import_code": ["import numpy as np\n", "from numpy.random import random as rng\n"]}
{"id": "050431a7-9b44-359e-a5d2-f474255d27fd_5", "content": "def end_points(n, steps):\n    x_arr = np.zeros(n)\n    y_arr = np.zeros(n)\n    r_arr = np.zeros(n)\n    for i in range(n):\n        x_end, y_end, distance = end_point(steps)\n        x_arr[i] = x_end\n        y_arr[i] = y_end\n        r_arr[i] = distance\n    return x_arr, y_arr, r_arr\n", "import_code": ["import numpy as np\n", "from numpy.random import random as rng\n"]}
{"id": "abce5cf4-4abf-3d98-9d3f-af1db4cea369_0", "content": "def data_preprocess(df):\n    categoricals = df.select_dtypes(include='object')\n    categoricals = categoricals.astype(str)\n    label_ = preprocessing.LabelEncoder()\n    categoricals = categoricals.apply(label_.fit_transform)\n    oh = preprocessing.OneHotEncoder()\n    encoded_data = oh.fit(categoricals).transform(categoricals)\n    encoded_data = pd.DataFrame(encoded_data.todense())\n    encoded_data.reset_index(drop=True, inplace=True)\n    original_numeric = df.select_dtypes(include='number')\n    original_numeric.reset_index(drop=True, inplace=True)\n    data = pd.concat([original_numeric, encoded_data], axis=1)\n    return data\n", "import_code": ["import pandas as pd\n", "from sklearn import preprocessing\n"]}
{"id": "c161ce34-11f4-33e6-b818-e49ba2a1eae4_0", "content": "def create_data(seed, m):\n    random.seed(seed)\n    return [random.getrandbits(32) for i in range(int(m))]\n", "import_code": ["import random\n"]}
{"id": "c161ce34-11f4-33e6-b818-e49ba2a1eae4_1", "content": "def trailing_zeroes(num):\n    \"\"\"Counts the number of trailing 0 bits in num.\"\"\"\n    if num == 0:\n        return 32\n    p = 0\n    while num >> p & 1 == 0:\n        p += 1\n    return p\n", "import_code": []}
{"id": "c161ce34-11f4-33e6-b818-e49ba2a1eae4_2", "content": "def estimate_cardinality(data):\n    \"\"\"Estimates the number of unique elements in the input set values\n    Arguments:\n        values: An iterator of hashable elements to estimate the cardinality of.\n    \"\"\"\n    max_zeroes = max([trailing_zeroes(d) for d in data])\n    return 2 ** float(max_zeroes)\n", "import_code": []}
{"id": "c161ce34-11f4-33e6-b818-e49ba2a1eae4_3", "content": "def stochastic_averaging(est, num_groups, group_length):\n    group_avg = [np.mean(est[g * group_length:(g + 1) * group_length]) for\n        g in range(num_groups)]\n    return np.median(group_avg)\n", "import_code": ["import numpy as np\n"]}
{"id": "c161ce34-11f4-33e6-b818-e49ba2a1eae4_4", "content": "def rae(true, est):\n    return np.absolute(true - est) / true\n", "import_code": ["import numpy as np\n"]}
{"id": "d02df991-70c1-364b-b336-2a040bb67380_0", "content": "def get_csv(bucket_object_body: object):\n    return pd.read_csv(bucket_object_body)\n", "import_code": ["import pandas as pd\n"]}
{"id": "d02df991-70c1-364b-b336-2a040bb67380_1", "content": "def get_columns(csv_file):\n    return csv_file.columns\n", "import_code": []}
{"id": "d02df991-70c1-364b-b336-2a040bb67380_2", "content": "def save_csv_file(dataframe):\n    str_buffer = io.StringIO()\n    dataframe.to_csv(str_buffer)\n    return str_buffer\n", "import_code": ["import io\n"]}
{"id": "b5202766-c7a6-30e4-beaf-d7cc7f8af7ee_0", "content": "def preprocess(text):\n    text = text.lower()\n    text = re.sub('((www\\\\.[^\\\\s]+)|(https?://[^\\\\s]+)|(http?://[^\\\\s]+))',\n        '', text)\n    text = re.sub('@[^\\\\s]+', '', text)\n    text = re.sub('[^0-9A-Za-z \\t]', ' ', text)\n    text = re.sub('(.)\\\\1\\\\1+', '\\\\1\\\\1', text)\n    text = text.strip()\n    words = text.split()\n    stop_words = ['a', 'about', 'above', 'after', 'again', 'ain', 'all',\n        'am', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'because',\n        'been', 'before', 'being', 'below', 'between', 'both', 'by', 'can',\n        'd', 'did', 'do', 'does', 'doing', 'down', 'during', 'each', 'few',\n        'for', 'from', 'further', 'had', 'has', 'have', 'having', 'he',\n        'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how',\n        'i', 'if', 'in', 'into', 'is', 'it', 'its', 'itself', 'just', 'll',\n        'm', 'ma', 'me', 'more', 'most', 'my', 'myself', 'now', 'o', 'of',\n        'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves',\n        'out', 'own', 're', 's', 'same', 'she', 'shes', 'should',\n        'shouldve', 'so', 'some', 'such', 't', 'than', 'that', 'thatll',\n        'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there',\n        'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under',\n        'until', 'up', 've', 'very', 'was', 'we', 'were', 'what', 'when',\n        'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with',\n        'won', 'y', 'you', 'youd', 'youll', 'youre', 'youve', 'your',\n        'yours', 'yourself', 'yourselves']\n    lemmatizer = WordNetLemmatizer()\n    tokens = []\n    for token in words:\n        if token not in stop_words:\n            tokens.append(lemmatizer.lemmatize(token))\n    preprocessed_text = ' '.join(tokens)\n    return preprocessed_text\n", "import_code": ["from nltk.stem import WordNetLemmatizer\n", "import re\n", "from sklearn.feature_extraction.text import HashingVectorizer\n"]}
{"id": "b5202766-c7a6-30e4-beaf-d7cc7f8af7ee_1", "content": "def predict(model, text, vectoriser=HashingVectorizer(ngram_range=(1, 2))):\n    textdata = vectoriser.transform([preprocess(text)])\n    sentiment = model.predict(textdata)\n    score = round(np.amax(model.predict_proba(textdata)), 3)\n    data = [(text, sentiment, score)]\n    df = pd.DataFrame(data, columns=['text', 'sentiment', 'score'])\n    df['sentiment'] = df['sentiment'].replace([0, 1], ['Negative', 'Positive'])\n    return df\n", "import_code": ["import pandas as pd\n", "from sklearn.feature_extraction.text import HashingVectorizer\n", "import numpy as np\n"]}
{"id": "51091431-e0de-3657-bce6-5c8dcb335806_0", "content": "def quick_sort(input_list):\n    if len(input_list) <= 1:\n        return input_list\n    pivot_i = 0\n    pivot = input_list[pivot_i]\n    small, big = [], []\n    for i in range(1, len(input_list)):\n        if input_list[i] < pivot:\n            small.append(input_list[i])\n        else:\n            big.append(input_list[i])\n    return quick_sort(small) + [pivot] + quick_sort(big)\n", "import_code": []}
{"id": "e3459476-934d-326a-9ff7-41c5032054dd_2", "content": "def creneau(t):\n    t = t % 2\n    if t == int(t):\n        return 0\n    elif t < 1:\n        return 1\n    elif t < 2:\n        return -1\n", "import_code": []}
{"id": "e3459476-934d-326a-9ff7-41c5032054dd_3", "content": "def sp_creneau(n, t):\n    somme = 0\n    for p in range(n + 1):\n        somme = somme + sin((2 * p + 1) * pi * t) / (2 * p + 1)\n    return 4 * somme / pi\n", "import_code": ["from numpy import cos, sin, exp, floor, linspace, pi\n", "from numpy import cos, sin, exp, floor, linspace, pi\n"]}
{"id": "e3459476-934d-326a-9ff7-41c5032054dd_5", "content": "def triangle(t):\n    return 1 - 2 * t * creneau(t) % 2\n", "import_code": []}
{"id": "e3459476-934d-326a-9ff7-41c5032054dd_6", "content": "def sp_triangle(n, t):\n    somme = 0\n    for p in range(n + 1):\n        somme = somme + cos((2 * p + 1) * pi * t) / (2 * p + 1) ** 2\n    return 8 * somme / pi ** 2\n", "import_code": ["from numpy import cos, sin, exp, floor, linspace, pi\n", "from numpy import cos, sin, exp, floor, linspace, pi\n"]}
{"id": "d091eebc-f0b3-3540-a0c5-c117839420ad_0", "content": "def isOpen(ip, port):\n    \"\"\"\n    This function checks if a port is open on a specific IP address.\n    It creates a socket, tries to connect to the IP and port, and returns True if the connection is successful.\n    \"\"\"\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.connect((ip, int(port)))\n        s.shutdown(2)\n        return True\n    except:\n        return False\n", "import_code": ["import socket\n"]}
{"id": "d091eebc-f0b3-3540-a0c5-c117839420ad_1", "content": "def ip2num(ip):\n    \"\"\"\n    This function converts an IP address from string format to a 32-bit integer.\n    It splits the IP address into its constituent parts, converts each part to an integer,\n    and combines them using bitwise operations to form the final 32-bit integer.\n    \"\"\"\n    ips = [int(x) for x in ip.split('.')]\n    return ips[0] << 24 | ips[1] << 16 | ips[2] << 8 | ips[3]\n", "import_code": []}
{"id": "d091eebc-f0b3-3540-a0c5-c117839420ad_2", "content": "def num2ip(num):\n    \"\"\"\n    This function converts a 32-bit integer IP address back to its string format.\n    It uses bitwise operations to extract each part of the IP address from the integer,\n    and then combines them into a string format.\n    \"\"\"\n    return '%s.%s.%s.%s' % (num >> 24 & 255, num >> 16 & 255, num >> 8 & \n        255, num & 255)\n", "import_code": []}
{"id": "d091eebc-f0b3-3540-a0c5-c117839420ad_3", "content": "def gen_ip(ip):\n    \"\"\"\n    This function generates a list of IP addresses between two given IP addresses.\n    It converts the start and end IP addresses to 32-bit integers,\n    generates a range of IP addresses between the start and end IPs,\n    and converts each IP address back to its string format.\n    \"\"\"\n    start, end = [ip2num(x) for x in ip.split('-')]\n    return [num2ip(num) for num in range(start, end + 1) if num & 255]\n", "import_code": []}
{"id": "d091eebc-f0b3-3540-a0c5-c117839420ad_4", "content": "def runPing(ip):\n    \"\"\"\n    This function runs the ping command on a given IP address.\n    It uses the subprocess module to execute the ping command,\n    and returns True if the ping is successful (i.e., the return code is 0).\n    \"\"\"\n    command = ['ping', '-c', '1', ip]\n    print(f'start ping {ip} ..... ')\n    res = subprocess.call(command, stdout=subprocess.DEVNULL, stderr=\n        subprocess.DEVNULL) == 0\n    print(f'end ping {ip} : {res}')\n    return f'ping {ip} : {res}'\n", "import_code": ["import subprocess\n"]}
{"id": "d091eebc-f0b3-3540-a0c5-c117839420ad_5", "content": "def runTelnet(ip, port):\n    \"\"\"\n    This function checks if a specific port is open on a given IP address.\n    It uses the isOpen function to check if the port is open,\n    and returns a string indicating the IP address, port, and whether the port is open.\n    \"\"\"\n    print(f'start check {ip}:{port} ..... ')\n    res = isOpen(ip, port)\n    print(f'end check {ip}:{port} : {res}')\n    return f'telnet {ip}:{port} : {res}'\n", "import_code": []}
{"id": "5dc6cc29-5355-3dc6-ae42-1a417b8f6ee1_0", "content": "def gauss(x):\n    x = 1.0 / np.sqrt(8 * np.pi) * np.exp(-x ** 2 / 8.0)\n    return x\n", "import_code": ["import numpy as np\n"]}
{"id": "dfc138d0-0fca-3d0d-a37b-5ee5bc1102e6_1", "content": "def clean_data(X, y):\n    X = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n    X = X * 2 - 1\n    y = np.array(list([int(tmp == 0), int(tmp == 1), int(tmp == 2)] for tmp in\n        y))\n    return X, y\n", "import_code": ["import numpy as np\n"]}
{"id": "f9cdec25-1b59-3a83-bf84-2c6feeaa1df6_0", "content": "def write_into_list(data):\n    row = []\n    row.append(data['text'])\n    row.append(data['created_at'])\n    row.append(data['lang'])\n    row.append(data['retweet_count'])\n    row.append(data['retweeted'])\n    row.append(data['favorite_count'])\n    row.append(data['favorited'])\n    row.append(data['coordinates'])\n    row.append(data['user']['name'])\n    row.append(data['user']['screen_name'])\n    return row\n", "import_code": []}
{"id": "43b54485-988f-32b4-8329-71f2e8bd5606_4", "content": "def download_file(url, local_filename):\n    \"\"\"\n    Downloads a file from a URL to a local file path\n\n    Inputs:\n        url (str): URL of the file to download\n        local_filename (str): Local file path where the downloaded file will be saved\n\n    Returns:\n        str: Local file path where the downloaded file is saved\n    \"\"\"\n    with requests.get(url, stream=True, verify=False) as r:\n        r.raise_for_status()\n        with open(local_filename, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n    return local_filename\n", "import_code": ["import requests\n"]}
{"id": "0b90964f-6235-30d1-837c-d261060994a6_0", "content": "def plot_cdf(subplot, plot_data, **kwargs):\n    xlist = plot_data[:]\n    xlist = list(set(xlist))\n    xlist.sort()\n    ylist = []\n    so_far = 0.0\n    total = len(plot_data)\n    for i in xrange(len(xlist)):\n        count = plot_data.count(xlist[i])\n        so_far = so_far + 100 * count / float(total)\n        ylist.append(so_far)\n        if so_far > 99.99:\n            break\n    return subplot.plot(xlist[:i + 1], [(j / 100.0) for j in ylist], **kwargs)\n", "import_code": []}
{"id": "0b90964f-6235-30d1-837c-d261060994a6_1", "content": "def plot_list(subplot, plot_data, marker=None, markevery=None):\n    xlist = xrange(len(plot_data))\n    return subplot.plot(xlist, plot_data, marker, markevery)\n", "import_code": []}
{"id": "0b90964f-6235-30d1-837c-d261060994a6_2", "content": "def plot_bar(subplot, data, bottom=None, **kwargs):\n    ind = numpy.arange(len(data))\n    width = 0.35\n    return subplot.bar(ind, data, width, bottom, **kwargs)\n", "import_code": ["import numpy\n"]}
{"id": "956d9b32-1b6e-3b1c-b2af-d5b6bc3e0bc6_0", "content": "def truncnorm_rvs(a, b, mean, std):\n    a_use = (a - mean) / std\n    b_use = (b - mean) / std\n    return truncnorm.rvs(a_use, b_use, mean, std)\n", "import_code": ["from scipy.stats import norm, truncnorm\n"]}
{"id": "956d9b32-1b6e-3b1c-b2af-d5b6bc3e0bc6_1", "content": "def truncnorm_logpdf(x, a, b, mean, std):\n    a_use = (a - mean) / std\n    b_use = (b - mean) / std\n    return truncnorm.logpdf(x, a_use, b_use, mean, std)\n", "import_code": ["from scipy.stats import norm, truncnorm\n"]}
{"id": "aafbd25b-8eb8-3dd4-9f90-0a2a06ca2215_0", "content": "def prediction(param):\n    param = np.array(param).reshape(1, -1)\n    cls = pickle.load(open('cls_Admission.pkl', 'rb'))\n    return cls.predict(param)\n", "import_code": ["import numpy as np\n", "from numpy import concatenate\n", "import pickle\n"]}
{"id": "ea43eacd-d303-36f9-b006-757ca2d38558_0", "content": "def random_set_of_mean(counter):\n    dataSet = []\n    for i in range(0, counter):\n        randomIndex = random.randint(0, len(data) - 1)\n        value = data[randomIndex]\n        dataSet.append(value)\n    mean = statistics.mean(dataSet)\n    return mean\n", "import_code": ["import statistics\n", "import random\n"]}
{"id": "1c36342f-766f-3df4-ac5e-90a9f417ca54_0", "content": "def playerHand(a):\n    asuits = Counter()\n    hand = 0\n    for i in {1, 4, 7, 10, 13}:\n        asuits[a[i]] += 1\n    if asuits.most_common(1)[0][1] == 5:\n        hand = hands['FL']\n    aVals = []\n    for i in {0, 3, 6, 9, 12}:\n        aVals.append(values[a[i]])\n    aVals.sort()\n    diffList = list(aVals[i] - aVals[i - 1] for i in range(0, 5))[1:]\n    if diffList == [1, 1, 1, 1]:\n        if hand == hands['FL']:\n            hand = hands['SF'] + aVals[-1]\n        else:\n            hand = hands['ST'] + aVals[-1]\n    if hand != 0:\n        return hand\n    aCnt = Counter()\n    for i in aVals:\n        aCnt[i] += 1\n    aPair = aCnt.most_common(2)\n    if aPair[1][1] == 2 and aPair[0][1] == 3:\n        hand = hands['FH'] + aPair[0][0]\n    elif aPair[1][1] == aPair[0][1] == 2:\n        hand = hands['TP'] + aVals[3]\n    elif aPair[0][1] == 1:\n        hand = hands['HC'] + aVals[4]\n    elif aPair[0][1] == 2:\n        hand = hands['P'] + aPair[0][0]\n    elif aPair[0][1] == 3:\n        hand = hands['TOK'] + aPair[0][0]\n    elif aPair[0][1] == 4:\n        hand = hands['FOK'] + aPair[0][0]\n    return hand\n", "import_code": ["from collections import Counter\n"]}
{"id": "939d89bf-71ea-3e19-b7a8-8ef131b542c3_0", "content": "def search_key_in_all_files_in_entire_folder(key, directory_path):\n    occurrance_array = []\n    for root, subdirs, files in os.walk(directory_path, topdown=True):\n        for file in files:\n            with open(f'{root}/{file}', FILE_OPEN_MODE_READ, encoding=\n                ENCODING, errors='ignore') as file_name:\n                for line_number, line in enumerate(file_name, STARTING_LINE):\n                    if len(line) == 0:\n                        continue\n                    if key in line:\n                        occurrance_array.append({'FILE_NAME':\n                            f'{root}/{file}', 'LINE': line, 'LINE_NUMBER':\n                            line_number})\n    return occurrance_array\n", "import_code": ["import os\n"]}
{"id": "8c81fccb-5e1e-3c5d-a10d-8c83e5661cf9_0", "content": "def datetime_from_str(str_, name_from):\n    from_fmt = _datetime_names[name_from]['fmt']\n    from_tz = _datetime_names[name_from]['tz']\n    dt = datetime.strptime(str_, from_fmt)\n    dt_from = dt.replace(tzinfo=from_tz)\n    return dt_from\n", "import_code": ["from datetime import datetime, timedelta\n"]}
{"id": "8c81fccb-5e1e-3c5d-a10d-8c83e5661cf9_1", "content": "def datetime_to_tz(dt_from, name_to):\n    to_tz = _datetime_names[name_to]['tz']\n    dt_to = dt_from.astimezone(to_tz)\n    return dt_to\n", "import_code": []}
{"id": "8c81fccb-5e1e-3c5d-a10d-8c83e5661cf9_2", "content": "def datetime_to_fmt(dt_from, name_to):\n    to_fmt = _datetime_names[name_to]['fmt']\n    to_tz = _datetime_names[name_to]['tz']\n    fmt_post = _datetime_names[name_to].get('fmt_post')\n    dt_to = dt_from.astimezone(to_tz)\n    ret_to = dt_to.strftime(to_fmt)\n    if fmt_post:\n        ret_to = fmt_post(ret_to)\n    return ret_to\n", "import_code": []}
{"id": "8c81fccb-5e1e-3c5d-a10d-8c83e5661cf9_3", "content": "def datetime_tz_convert(str_from, name_from, name_to):\n    \"\"\"\n    Convert a date and time string from one format and timezone to another.\n    ValueError can be raised from strptime().\n    \"\"\"\n    dt_from = datetime_from_str(str_from, name_from)\n    ret_to = datetime_to_fmt(dt_from, name_to)\n    return ret_to\n", "import_code": []}
{"id": "8c81fccb-5e1e-3c5d-a10d-8c83e5661cf9_4", "content": "def date_range_last(n_days=1):\n    fmt = _datetime_names['cloud']['fmt']\n    tz = _datetime_names['cloudpub']['tz']\n    cloud_tz = _datetime_names['cloud']['tz']\n    now = datetime.now(tz).date()\n    now = datetime(now.year, now.month, now.day)\n    prev = now - timedelta(days=n_days)\n    now = tz.localize(now).astimezone(cloud_tz)\n    prev = tz.localize(prev).astimezone(cloud_tz)\n    since = prev.strftime(fmt)\n    until = now.strftime(fmt)\n    return since, until\n", "import_code": ["from datetime import datetime, timedelta\n", "from datetime import datetime, timedelta\n"]}
{"id": "a0798d86-6b4a-37a8-ada7-d3f4e84535fd_0", "content": "def _get_path_first_component(path: str) ->str:\n    \"\"\"Returns the first component of a path.\n\n    Example:\n        _get_path_first_component('data/foo/bar/README.md') returns 'data'.\n        _get_path_first_component('/data/foo/bar/README.md') returns ''.\n        _get_path_first_component('data') returns 'data'.\n         _get_path_first_component('') returns ''.\n    \"\"\"\n    index = path.find(os.path.sep)\n    if index != -1:\n        return path[:index]\n    return path\n", "import_code": ["import os\n"]}
{"id": "b85bc0c7-7a88-3404-9e83-5e2918817cfa_0", "content": "def read_line_of_ints(text):\n    ints = []\n    ints_as_strs = split_line(text)\n    for int_as_str in ints_as_strs:\n        ints.append(int(int_as_str))\n    return ints\n", "import_code": []}
{"id": "b85bc0c7-7a88-3404-9e83-5e2918817cfa_1", "content": "def split_line(line):\n    return line.split()\n", "import_code": []}
{"id": "b85bc0c7-7a88-3404-9e83-5e2918817cfa_2", "content": "def read_file_into_list(filename):\n    with open(filename) as file:\n        return file.readlines()\n", "import_code": []}
{"id": "b85bc0c7-7a88-3404-9e83-5e2918817cfa_3", "content": "def read_file_into_ints(filename):\n    lines = read_file_into_list(filename)\n    list_of_lists = []\n    for line in lines:\n        list_of_lists.append(read_line_of_ints(line))\n    return list_of_lists\n", "import_code": []}
{"id": "0b866910-a55d-34e3-a202-54b82568f6b2_1", "content": "def account_snapshot(key, secret):\n    \"\"\"\n    This function retrieves the daily account snapshot from Binance API.\n    \"\"\"\n    url = 'https://api.binance.com/sapi/v1/accountSnapshot'\n    ts = int(dt.datetime.timestamp(dt.datetime.now() + dt.timedelta(seconds\n        =15)) * 1000)\n    recvWindow = 5000\n    params = {'recvWindow': 10000, 'type': 'SPOT', 'timestamp': ts}\n    h = urlencode(params)\n    b = bytearray()\n    b.extend(secret.encode())\n    signature = hmac.new(b, msg=h.encode('utf-8'), digestmod=hashlib.sha256\n        ).hexdigest()\n    params['signature'] = signature\n    headers = {'X-MBX-APIKEY': key}\n    r = requests.get(url=url, params=params, headers=headers, verify=True)\n    js = r.json()\n    return js\n", "import_code": ["import requests\n", "import hashlib\n", "import datetime as dt\n", "import hmac\n", "from urllib import urlencode\n", "from urllib.parse import urlencode\n"]}
{"id": "00e7a6fb-4803-3188-b117-16f97a2b9d98_0", "content": "def xy(r, phi, c_1=0, c_2=0):\n    \"\"\"\n    Cartesian coordinates for point at polar coordinates r, phi with origin (c_1, c_2)\n    \"\"\"\n    return r * np.cos(phi) + c_1, r * np.sin(phi) + c_2\n", "import_code": ["import numpy as np\n"]}
{"id": "00e7a6fb-4803-3188-b117-16f97a2b9d98_1", "content": "def circle(r, phi_0=0, phi_1=2 * np.pi, c_1=0, c_2=0):\n    \"\"\"\n    Circle arc with center (c_1, c_2) and radius r between the given angle values.\n\n    Args:\n      - r (float) :     Circle radius\n      - phi_0 (float):  Angle for origin point of the arc\n      - phi_1 (float):  Angle for end point of the arc\n      - c_1 (float):    x coordinate of circle center\n      - c_2 (float):    y coordinate of circle center\n    \"\"\"\n    phis = np.arange(phi_0, phi_1, 0.01)\n    return xy(r, phis, c_1, c_2)\n", "import_code": ["import numpy as np\n"]}
{"id": "4a14379e-cd2d-3afd-b870-21540a3dff2f_0", "content": "def train_engine(plots: List):\n    \"\"\"\n    Train engine from dish names.\n\n    Args:\n      plots (array): List of dish names.\n\n    Returns:\n      tfidf_matrix (sparse matrix, [n_samples, n_features]): Tf-idf-weighted document-term matrix.\n    \"\"\"\n    vectorizer = TfidfVectorizer(analyzer='word', lowercase=True, min_df=3,\n        max_df=0.9, ngram_range=(1, 2), stop_words='english')\n    tfidf_matrix = vectorizer.fit_transform(plots)\n    return tfidf_matrix\n", "import_code": ["from sklearn.feature_extraction.text import TfidfVectorizer\n", "from typing import List\n"]}
{"id": "4a14379e-cd2d-3afd-b870-21540a3dff2f_1", "content": "def find_similarities(tfidf_matrix: List, index: int, top_n: int=5) ->dict:\n    \"\"\"\n    Use to find similarities.\n\n    Args:\n      tfidf_matrix (sparse matrix, [n_samples, n_features]): Tf-idf-weighted document-term matrix.\n      index (int): Dish id from dataset.\n      top_n (int): Max recommendation count, max value 30.\n\n    Returns:\n      indice, similarity value (dictionary): Indice, similarity value.\n    \"\"\"\n    cosine_similarities = linear_kernel(tfidf_matrix[index:index + 1],\n        tfidf_matrix).flatten()\n    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if\n        i != index]\n    return [(index, cosine_similarities[index]) for index in\n        related_docs_indices][0:top_n]\n", "import_code": ["from typing import List\n", "from sklearn.metrics.pairwise import linear_kernel\n"]}
{"id": "2298ac1e-cd76-31b7-ab55-b9d6faac4b18_0", "content": "def get_special_paths(directory):\n    \"\"\"Return a list of the absolute paths of the\n    special files in the given directory\"\"\"\n    file_list = []\n    for root, dirs, files in os.walk(directory):\n        for name in files:\n            file_list.append(os.path.join(root, name))\n    special_files = filter(lambda file: re.search('__\\\\w+__', file), file_list)\n    special_files = [os.path.abspath(filename) for filename in special_files]\n    return special_files\n", "import_code": ["import re\n", "import os\n"]}
{"id": "ffd7a3bb-d4de-3d52-b056-bd0956af392a_0", "content": "def index(request):\n    return render(request, 'index.html')\n", "import_code": ["from django.shortcuts import render\n"]}
{"id": "ffd7a3bb-d4de-3d52-b056-bd0956af392a_1", "content": "def query(request):\n    query = request.GET.get('query', None)\n    dict3 = {'cat': [], 'noncat': []}\n    dict4 = {'value': []}\n    starting_bit = query.split()[:2]\n    if starting_bit[0] == 'Graph':\n        query1 = query.split('of')\n        query2 = str(query1[1]).split('by')\n        non_cat = str(query2[0]).strip()\n        cat = str(query2[1]).strip()\n        if non_cat == 'Sales':\n            dict3['noncat'].append('SALES')\n        if cat == 'Product':\n            dict3['cat'].append('PRODUCTLINE')\n        if cat == 'Product Size':\n            dict3['cat'].append('DEALSIZE')\n        return JsonResponse(dict3, safe=False)\n    if starting_bit[0] == 'Average':\n        if starting_bit[1] == 'Price':\n            query3 = query.split('of')\n            product_name = str(query3[1]).strip()\n            results = solr.search(q=[product_name], rows=500000, start=0)\n            df1 = pd.DataFrame(results.docs)\n            dict4['value'].append(df1['PRICEEACH'].mean())\n            return JsonResponse(dict4, safe=False)\n        if starting_bit[1] == 'Sales':\n            query3 = query.split('of')\n            product_name = str(query3[1]).strip()\n            results = solr.search(q=[product_name], rows=500000, start=0)\n            df1 = pd.DataFrame(results.docs)\n            dict4['value'].append(df1['SALES'].mean())\n            return JsonResponse(dict4, safe=False)\n    if starting_bit[0] == 'Total':\n        if starting_bit[1] == 'Price':\n            query3 = query.split('of')\n            product_name = str(query3[1]).strip()\n            results = solr.search(q=[product_name], rows=500000, start=0)\n            df1 = pd.DataFrame(results.docs)\n            dict4['value'].append(df1['PRICEEACH'].sum())\n            return JsonResponse(dict4, safe=False)\n        if starting_bit[1] == 'Sales':\n            query3 = query.split('of')\n            product_name = str(query3[1]).strip()\n            results = solr.search(q=[product_name], rows=500000, start=0)\n            df1 = pd.DataFrame(results.docs)\n            dict4['value'].append(df1['SALES'].sum())\n            return JsonResponse(dict4, safe=False)\n    print(dict3)\n", "import_code": ["import pandas as pd\n", "from django.http import JsonResponse\n"]}
{"id": "d7ed659f-e784-3761-95f1-4d23f28594d3_0", "content": "def normalized(a, axis=0):\n    axisALL = range(0, len(a.shape))\n    axisC = list()\n    axisC = [i for i in axisALL if i != axis]\n    axisC = tuple(axisC)\n    a_mean = a.mean(axis=axisC, keepdims=False)\n    a_std = a.std(axis=axisC, keepdims=False)\n    a_std[a_std == 0] = 1\n    new_shape = np.repeat(1, len(a.shape))\n    new_shape[axis] = a.shape[axis]\n    a_std = a_std.reshape(new_shape)\n    a_mean = a_std.reshape(new_shape)\n    a = (a - a_mean) / a_std\n    return a, a_mean, a_std\n", "import_code": ["import numpy as np\n"]}
{"id": "d7ed659f-e784-3761-95f1-4d23f28594d3_1", "content": "def stacker(trainx, trainy, testx, testy):\n    train_x = np.stack([item for sublist in trainx for item in sublist])\n    print('Training X Data: Stacked!')\n    train_y = np.stack([item for sublist in trainy for item in sublist])\n    print('Training Y Data: Stacked!')\n    test_x = np.stack([item for sublist in testx for item in sublist])\n    print('Testing X Data: Stacked!')\n    test_y = np.stack([item for sublist in testy for item in sublist])\n    print('Testing Y Data: Stacked!')\n    return train_x, train_y, test_x, test_y\n", "import_code": ["import numpy as np\n"]}
{"id": "d7ed659f-e784-3761-95f1-4d23f28594d3_2", "content": "def normer(train_x, test_x):\n    print('Normalizing training data. This may take a minute.')\n    train_x, train_mean, train_std = normalized(train_x, axis=3)\n    print('Training data normalized!')\n    print('Normalizing testing data. This may take a minute.')\n    test_x = (test_x - train_mean) / train_std\n    print('Testing data normalized!')\n    return train_x, test_x\n", "import_code": []}
{"id": "6ff6bef6-9f75-3991-917c-8efc1d00e005_0", "content": "def hash_password(password):\n    \"\"\"Hash a password for storing.\"\"\"\n    salt = hashlib.sha256(os.urandom(60)).hexdigest().encode('ascii')\n    pwdhash = hashlib.pbkdf2_hmac('sha512', password.encode('utf-8'), salt,\n        100000)\n    pwdhash = binascii.hexlify(pwdhash)\n    return (salt + pwdhash).decode('ascii')\n", "import_code": ["import hashlib\n", "import binascii\n", "import os\n"]}
{"id": "7b5763e8-e32d-3bad-b9a7-139cf65cb210_1", "content": "def clean_string(s):\n    toReturn = ''.join(c for c in s if c not in punctuation)\n    toReturn = ' '.join(toReturn.split())\n    return toReturn\n", "import_code": ["from string import punctuation\n"]}
{"id": "7b5763e8-e32d-3bad-b9a7-139cf65cb210_2", "content": "def read_in_and_save(topic):\n    directory = os.getcwd()\n    directory = os.path.join(directory, 'bbc-fulltext', 'bbc', topic)\n    toReturn = []\n    counter = 0\n    for filename in os.listdir(directory):\n        with open(os.path.join(directory, filename)) as file:\n            if counter == 50:\n                break\n            else:\n                data = file.read()\n                data = clean_string(data)\n                toReturn.append(data)\n                counter += 1\n    return toReturn\n", "import_code": ["import os\n", "from sklearn.decomposition import LatentDirichletAllocation\n"]}
{"id": "b6d7fcf4-0544-35b8-b5e8-23333478684a_0", "content": "def home(request):\n    return render(request, 'base.html')\n", "import_code": ["from django.shortcuts import render\n"]}
{"id": "b6d7fcf4-0544-35b8-b5e8-23333478684a_1", "content": "def getData(request):\n    data = os.path.join(os.path.dirname(os.path.dirname(__file__)),\n        'data/sample_portfolio.csv')\n    pdata = pd.read_csv(data)\n    pdata['NormAge'] = (pdata['term'] - pdata['Repayments left']) / pdata[\n        'term'] * 100\n    bins = np.arange(0, 110, 10)\n    pdata.groupby(pd.cut(pdata['NormAge'], bins)).sum()\n    res = pdata.groupby(pd.cut(pdata['NormAge'], bins))\n    json_data = []\n    for name, group in res:\n        json_data.append([name.split(',')[1][1:-1] + '%', group['Principal'\n            ].sum()])\n    for_render = {'data': json_data}\n    return HttpResponse(json.dumps(for_render), content_type='application/json'\n        )\n", "import_code": ["import pandas as pd\n", "import numpy as np\n", "import json\n", "import os\n", "from django.http import HttpResponse\n"]}
{"id": "b2ca66b0-5caf-3906-9a83-c1520d188ae5_0", "content": "def get_server(headers: dict) ->typing.Tuple:\n    \"\"\"\n    Parse the host and port from the event headers to use as the `server` key in the\n    ASGI connection scope.\n    \"\"\"\n    server_name = headers.get('host', 'mangum')\n    if ':' not in server_name:\n        server_port = headers.get('x-forwarded-port', 80)\n    else:\n        server_name, server_port = server_name.split(':')\n    server = server_name, int(server_port)\n    return server\n", "import_code": ["import typing\n"]}
{"id": "b2ca66b0-5caf-3906-9a83-c1520d188ae5_1", "content": "def get_logger(log_level: str) ->logging.Logger:\n    \"\"\"\n    Create the default logger according to log level setting of the adapter instance.\n    \"\"\"\n    level = {'critical': logging.CRITICAL, 'error': logging.ERROR,\n        'warning': logging.WARNING, 'info': logging.INFO, 'debug': logging.\n        DEBUG}[log_level]\n    logging.basicConfig(format='[%(asctime)s] %(message)s', level=level,\n        datefmt='%d-%b-%y %H:%M:%S')\n    logger = logging.getLogger('mangum')\n    logger.setLevel(level)\n    return logger\n", "import_code": ["import logging\n"]}
{"id": "1a1a3247-0348-33aa-9995-ce0ff4c6294e_2", "content": "def calculate_sim_dot_product(X, ntake=40):\n    \"\"\"\n    Take X (N, D) features and for each index return closest ntake indices via dot product.\n    \"\"\"\n    S = np.dot(X, X.T)\n    IX = np.argsort(S, axis=1)[:, :-ntake - 1:-1]\n    return IX.tolist()\n", "import_code": ["import numpy as np\n"]}
{"id": "efdda400-23e0-3ef4-b21d-9951e8561db9_0", "content": "def PointDistance(point, list_point):\n    return math.sqrt(np.sum((point - list_point) ** 2))\n", "import_code": ["import numpy as np\n", "import math\n"]}
{"id": "efdda400-23e0-3ef4-b21d-9951e8561db9_1", "content": "def ValidPoint(point, current_list, radius):\n    output = True\n    for list_point in current_list:\n        if PointDistance(point, list_point) < 2 * radius:\n            output = False\n    return output\n", "import_code": []}
{"id": "efdda400-23e0-3ef4-b21d-9951e8561db9_2", "content": "def CreateCenterCoordinates(current_list, radius, length=500):\n    while True:\n        point = np.array([np.random.uniform(radius, length - radius), np.\n            random.uniform(radius, length - radius)])\n        if ValidPoint(point, current_list, radius):\n            current_list.append(point)\n            return point\n", "import_code": ["import numpy as np\n"]}
{"id": "efdda400-23e0-3ef4-b21d-9951e8561db9_3", "content": "def RandomImage(num, radius, length=500):\n    image = np.zeros((length, length))\n    list_of_centers = []\n    for _ in range(num):\n        x, y = CreateCenterCoordinates(list_of_centers, radius, length)\n        rr, cc = draw.circle(x, y, radius, shape=image.shape)\n        image[rr, cc] = 1\n    return image\n", "import_code": ["from skimage import draw\n", "import numpy as np\n"]}
{"id": "a9ef0617-3c1d-371d-b663-b71ec3d59a87_0", "content": "def count_tuples(text, tuple_size):\n    pairs = re.findall('(?=(.{' + str(tuple_size) + '}))', text)\n    return collections.Counter(pairs)\n", "import_code": ["import sys, collections, re, os\n", "import sys, collections, re, os\n"]}
{"id": "a9ef0617-3c1d-371d-b663-b71ec3d59a87_1", "content": "def count_words(text):\n    words = re.findall('\\\\w+', text)\n    return collections.Counter(words)\n", "import_code": ["import sys, collections, re, os\n", "import sys, collections, re, os\n"]}
{"id": "804d3b2f-b13c-30a7-b505-5c096d665ce2_0", "content": "def get_value(self):\n    \"\"\"Return parameters values\"\"\"\n    assert self.min_value < self.max_value\n    if self.type_value_gen == 0:\n        if self.type_value == 0:\n            dtype = float\n        elif self.type_value == 1:\n            dtype = int\n        else:\n            dtype = None\n        value = linspace(start=self.min_value, stop=self.max_value, num=\n            self.N, endpoint=True, dtype=dtype)\n    return value\n", "import_code": ["from numpy import linspace\n"]}
{"id": "6bf047c6-9b9b-3e9f-a4a8-83cfd2c253f6_0", "content": "def threaded_work(function, documents, tfidf_vec, feature_names, conferences):\n    q = Queue()\n    pool = ThreadPool(8)\n    pool.starmap(function, zip(documents, itertools.repeat(tfidf_vec),\n        itertools.repeat(feature_names), itertools.repeat(conferences),\n        itertools.repeat(q)))\n    pool.close()\n    pool.join()\n    columns = ['conferenceName', 'keyword', 'score']\n    score_pd = pd.DataFrame(columns=columns)\n    while not q.empty():\n        score_pd = pd.concat([score_pd, q.get()], sort=False)\n        score_pd = score_pd.reset_index(drop=True)\n    q.close()\n    return score_pd\n", "import_code": ["import itertools\n", "import pandas as pd\n", "from multiprocessing.dummy import Pool as ThreadPool\n", "from multiprocessing import Queue\n"]}
{"id": "6bf047c6-9b9b-3e9f-a4a8-83cfd2c253f6_1", "content": "def get_corpus(df, nlp, props):\n    conference_name = df['conference_type'].iloc[0]\n    year = df['year'].iloc[0]\n    abstracts = df['abstract']\n    stop_words = set(stopwords.words('english'))\n    lemmatized_corpus = ''\n    for abstract in abstracts:\n        abstract = abstract.lower()\n        abstract = re.sub('[^A-Za-z0-9]', ' ', abstract)\n        annotated_text = nlp.annotate(abstract, properties=props)\n        annotated_text = json.loads(annotated_text)\n        tokens = annotated_text['sentences']\n        for token_index in range(len(tokens)):\n            sentence_prop = tokens[token_index]['tokens']\n            for sentence_prop_index in range(len(sentence_prop)):\n                word = sentence_prop[sentence_prop_index]['lemma']\n                if not (word in string.punctuation or word in stop_words):\n                    lemmatized_corpus = lemmatized_corpus + word + ' '\n    return {'conference_name': conference_name + ' ' + str(year), 'corpus':\n        lemmatized_corpus}\n", "import_code": ["import re\n", "from multiprocessing.dummy import Pool as ThreadPool\n", "from sklearn.feature_extraction.text import TfidfVectorizer\n", "import json\n", "import string\n", "from nltk.corpus import stopwords\n"]}
{"id": "6bf047c6-9b9b-3e9f-a4a8-83cfd2c253f6_3", "content": "def get_keywords_score(corpuses, conferences):\n    vectorizer = TfidfVectorizer(ngram_range=(1, 4))\n    tfidf_vec = vectorizer.fit_transform(corpuses)\n    feature_names = vectorizer.get_feature_names()\n    scores = threaded_work(get_individual_score, range(len(corpuses)),\n        tfidf_vec, feature_names, conferences)\n    return scores\n", "import_code": ["from sklearn.feature_extraction.text import TfidfVectorizer\n"]}
{"id": "6bf047c6-9b9b-3e9f-a4a8-83cfd2c253f6_4", "content": "def extract_keywords(conferences, corpuses):\n    score_pd = get_keywords_score(corpuses, conferences)\n    return score_pd\n", "import_code": []}
{"id": "9abc8a70-0402-3fe4-b7e5-338a62eecdd8_0", "content": "def check_md5(fname):\n    alldata = hashlib.md5()\n    with open(fname, 'rb') as f1:\n        while True:\n            data = f1.read(4096)\n            if not data:\n                break\n            alldata.update(data)\n    return alldata.hexdigest()\n", "import_code": ["import hashlib\n"]}
{"id": "c7356e50-b77a-3918-9611-4e63066ed5e0_4", "content": "def decision(iterator, keys, values, min_value, max_value):\n    while iterator <= int(sys.argv[3]):\n        keys.append(iterator)\n        iterator += 1\n        if sys.argv[4] == 'r':\n            values.append(round(random.uniform(min_value, max_value)))\n        elif sys.argv[4] == 'n':\n            values.append(random.uniform(min_value, max_value))\n        else:\n            help_syntax()\n    return keys, values\n", "import_code": ["import random\n", "import sys\n"]}
{"id": "d04251d3-c351-37b4-b216-bc6997c73c4d_0", "content": "def predict(seed):\n    random.seed(seed)\n    return {'number': np.float32(random.random())}\n", "import_code": ["import numpy as np\n", "import random\n"]}
{"id": "ed36c70a-6c4f-30a4-9055-b427938d200a_0", "content": "def box_blur(image, box_size):\n    \"\"\"\n    This function applies a box blur filter to an image.\n\n    Box blur is a simple blurring filter that replaces each pixel's color value with the average of the neighboring pixels' color values.\n    The size of the box used for averaging is determined by the 'box_size' parameter.\n\n    Args:\n    image (numpy.ndarray): The input image to be blurred.\n    box_size (int): The size of the box to be used for blurring. This value must be greater than 0.\n\n    Returns:\n    numpy.ndarray: The blurred image.\n    \"\"\"\n    kernel = np.ones((box_size, box_size))\n    kernel /= np.square(box_size)\n    output = conv_matrix(image, kernel)\n    return output\n", "import_code": ["import numpy as np\n"]}
{"id": "9f2e9506-f994-3d62-a332-5da0318fa34b_0", "content": "def init_folders(base_folder, mode):\n    pdf_folder = os.path.join(base_folder, 'pdf', mode)\n    if not os.path.exists(pdf_folder):\n        os.makedirs(pdf_folder)\n    json_folder = os.path.join(base_folder, 'json', mode)\n    if not os.path.exists(json_folder):\n        os.makedirs(json_folder)\n    png_folder = os.path.join(base_folder, 'png', mode)\n    if not os.path.exists(png_folder):\n        os.makedirs(png_folder)\n    results_folder = os.path.join(base_folder, 'csv_tabula', mode)\n    if not os.path.exists(results_folder):\n        os.makedirs(results_folder)\n    return pdf_folder, json_folder, png_folder, results_folder\n", "import_code": ["import os\n"]}
{"id": "c0828d26-bca7-3b0b-98e2-bfc4fe838b50_0", "content": "def n_dim_spiral(cx, max_i, step_size):\n    i = 0\n    turns = 0\n    start_points = [{k: v for k, v in cx.items()}]\n    while True:\n        for k in cx.keys():\n            for step in range(turns + 1):\n                step = step_size\n                if turns % 2 == 1:\n                    step = -step\n                cx[k] += step\n                i += 1\n                outputDict = {k: v for k, v in cx.items()}\n                start_points.append(outputDict)\n                if max_i == i:\n                    break\n            if max_i == i:\n                break\n        if max_i == i:\n            break\n        turns += 1\n    return start_points\n", "import_code": []}
{"id": "abc673cd-686b-37ca-93ba-d97c45334f69_0", "content": "def dimbox(ndim, nsamples):\n    return np.random.rand(nsamples, ndim)\n", "import_code": ["import numpy as np\n"]}
{"id": "abc673cd-686b-37ca-93ba-d97c45334f69_1", "content": "def calcfracdist(data, dist):\n    num_points = data.shape[0]\n    close_to_edge = np.logical_or(data > 1 - dist, data < dist)\n    within_range = np.any(close_to_edge, axis=1)\n    num_in_range = np.count_nonzero(within_range)\n    return num_in_range / num_points, within_range\n", "import_code": ["import numpy as np\n"]}
{"id": "abc673cd-686b-37ca-93ba-d97c45334f69_2", "content": "def distancefromcenter(coordinates):\n    center = 0.5 * np.ones(coordinates[0, :].shape)\n    dist_between_points = np.sqrt(np.sum((coordinates - center) ** 2, axis=1))\n    return dist_between_points\n", "import_code": ["import numpy as np\n"]}
{"id": "56aca27b-2bc8-33fb-a844-21362519cdd2_1", "content": "def get_polar(u):\n    \"\"\"\n    Returns profile versus angle relative to the domain center ignoring NaN.\n    u: 2D array, shape (ny, nx)\n    \"\"\"\n    iy, ix = np.where(np.isfinite(u))\n    shape = u.shape\n    cx = (shape[0] - 1) * 0.5\n    cy = (shape[1] - 1) * 0.5\n    dx = np.array(ix) - cx\n    dy = np.array(iy) - cy\n    angle = np.arctan2(dx, dy)\n    argsort = np.argsort(angle)\n    return angle[argsort], u[iy, ix][argsort]\n", "import_code": ["import numpy as np\n"]}
{"id": "a6bdc73d-e699-39da-9fdf-cea88d7437e9_0", "content": "def get_smtp(restart=False):\n    if not hasattr(flask.g, 'smtp') or restart:\n        flask.g.smtp = smtplib.SMTP('smtp.yandex.ru:587')\n        flask.g.smtp.ehlo()\n        flask.g.smtp.starttls()\n        flask.g.smtp.login(FROM_MAIL, PASSWORD)\n    return flask.g.smtp\n", "import_code": ["import smtplib\n", "import flask\n", "import flask_cors\n"]}
{"id": "a6bdc73d-e699-39da-9fdf-cea88d7437e9_1", "content": "def send_mail(toaddr, subject, html):\n    msg = MIMEMultipart('alternative')\n    msg['Subject'] = subject\n    msg['From'] = FROM_MAIL\n    msg['To'] = toaddr\n    msg.attach(MIMEText(html, 'html'))\n    for restart in [False, True]:\n        try:\n            get_smtp(restart).sendmail(FROM_MAIL, toaddr, msg.as_string())\n            return\n        except Exception as e:\n            print('Error while sending mail to {}:\\n\\n{}'.format(toaddr, e))\n", "import_code": ["from email.mime.multipart import MIMEMultipart\n", "from email.mime.text import MIMEText\n"]}
{"id": "6c911848-0020-3a77-b511-0530b67cd437_1", "content": "def average_grade(students_dict, gender, exp, report_type):\n    avg_grade_group = list()\n    condition_1 = \"student_description['gender'] in gender\"\n    condition_2 = \"student_description['prog_exp'] in exp\"\n    for student_description in students_dict.values():\n        if report_type == 'homework':\n            if eval(condition_1) and eval(condition_2):\n                grades = student_description['homework_grades']\n                avg_grade_student = sum(grades) / len(grades)\n                avg_grade_group.append(avg_grade_student)\n        elif report_type == 'exam':\n            if eval(condition_1) and eval(condition_2):\n                avg_grade_group.append(student_description['exam_grade'])\n        else:\n            print('Invalid report type.')\n    avg_grade = sum(avg_grade_group) / len(avg_grade_group)\n    return round(avg_grade, 3)\n", "import_code": []}
{"id": "6c911848-0020-3a77-b511-0530b67cd437_2", "content": "def best_students(students_dict):\n    integral_grade_group = dict()\n    best_students_names = dict()\n    for student_id, student_description in students_dict.items():\n        student_intgrl_grade = dict()\n        name = student_description['last_name'] + ' ' + student_description[\n            'first_name']\n        integral_grade = sum(student_description['homework_grades']) / len(\n            student_description['homework_grades']\n            ) * 0.6 + student_description['exam_grade'] * 0.4\n        student_intgrl_grade[name] = round(integral_grade, 3)\n        integral_grade_group[student_id] = student_intgrl_grade\n    intgrl_grade_info = list(integral_grade_group.values())\n    student_max_grade = max(list(intgrl_grade_info[x].values())[0] for x in\n        range(len(intgrl_grade_info)))\n    for student_id, intgrl_grade_info in integral_grade_group.items():\n        for student_name, intgrl_grade in intgrl_grade_info.items():\n            if intgrl_grade == student_max_grade:\n                best_students_names[student_id] = student_name\n    best_students_names['integral_grade'] = student_max_grade\n    return best_students_names\n", "import_code": []}
{"id": "3fa1cf24-6839-3ad9-a5be-afba32e3e27c_0", "content": "def gauss(x, A, mu, sigm):\n    return A * np.exp(-(x - mu) ** 2 / (2.0 * sigm ** 2)) / (np.sqrt(2.0 *\n        np.pi) * sigm)\n", "import_code": ["import numpy as np\n"]}
{"id": "b024f0fa-4887-33da-856e-538303abf913_0", "content": "def StemTokens(tokens):\n    return [stemmer.stem(token) for token in tokens]\n", "import_code": []}
{"id": "b024f0fa-4887-33da-856e-538303abf913_1", "content": "def StemNormalize(text):\n    return StemTokens(nltk.word_tokenize(text.lower().translate(\n        remove_punct_dict)))\n", "import_code": ["import nltk, string, numpy\n"]}
{"id": "b024f0fa-4887-33da-856e-538303abf913_2", "content": "def LemTokens(tokens):\n    return [lemmer.lemmatize(token) for token in tokens]\n", "import_code": []}
{"id": "b024f0fa-4887-33da-856e-538303abf913_3", "content": "def LemNormalize(text):\n    return LemTokens(nltk.word_tokenize(text.lower().translate(\n        remove_punct_dict)))\n", "import_code": ["import nltk, string, numpy\n"]}
{"id": "b024f0fa-4887-33da-856e-538303abf913_4", "content": "def idf(n, df):\n    result = math.log((n + 1.0) / (df + 1.0)) + 1\n    return result\n", "import_code": ["import math\n"]}
{"id": "b024f0fa-4887-33da-856e-538303abf913_5", "content": "def cos_similarity(textlist):\n    tfidf = TfidfVec.fit_transform(textlist)\n    return (tfidf * tfidf.T).toarray()\n", "import_code": []}
{"id": "79d1090a-0407-3574-bc7f-57e130d6a537_0", "content": "def is_item_dir(path: Path) ->bool:\n    return path.is_dir() and (path / 'item.yaml').exists()\n", "import_code": ["from pathlib import Path\n"]}
{"id": "79d1090a-0407-3574-bc7f-57e130d6a537_1", "content": "def is_function_dir(path: Path) ->bool:\n    if path.is_file():\n        return False\n    return any(f.name == 'function.yaml' for f in path.iterdir())\n", "import_code": ["from pathlib import Path\n"]}
{"id": "79d1090a-0407-3574-bc7f-57e130d6a537_4", "content": "def install_python(directory: Union[str, Path]):\n    print(f'Installing python for {directory}...')\n    python_install: subprocess.CompletedProcess = subprocess.run(\n        'pipenv --rm;pipenv --python 3.7', stdout=sys.stdout, stderr=\n        subprocess.PIPE, cwd=directory, shell=True)\n    exit_on_non_zero_return(python_install)\n    stderr = python_install.stderr.decode('utf8').split('\\n')\n    python_location = [l for l in stderr if 'Virtualenv location: ' in l]\n    if python_location:\n        python_location = python_location[0].split('Virtualenv location: ')[-1\n            ] + 'bin/python'\n    else:\n        python_location = None\n    return python_location\n", "import_code": ["from typing import Union, List, Set, Dict\n", "import sys\n", "import subprocess\n", "from pathlib import Path\n"]}
{"id": "79d1090a-0407-3574-bc7f-57e130d6a537_6", "content": "def install_requirements(directory: str, requirements: Union[List[str], Set\n    [str]]):\n    \"\"\"\n    Installing requirements from a requirements list/set and from a requirements.txt file if found in directory\n    :param directory:       The relevant directory were the requirements are installed and collected\n    :param requirements:    Requirement list/set with or without bounds\n    \"\"\"\n    requirements_file = Path(directory) / 'requirements.txt'\n    if not requirements and not requirements_file.exists():\n        print(f'No requirements found for {directory}...')\n        return\n    if requirements_file.exists():\n        print(f'Installing requirements from {requirements_file}...')\n        _run_subprocess(f'pipenv install --skip-lock -r {requirements_file}',\n            directory)\n        with open(requirements_file, 'r') as f:\n            mlrun_version = [l.replace('\\n', '') for l in f.readlines() if \n                'mlrun' in l]\n            if mlrun_version:\n                requirements = [r for r in requirements if 'mlrun' not in r]\n    if requirements:\n        print(\n            f\"Installing requirements [{' '.join(requirements)}] for {directory}...\"\n            )\n        _run_subprocess(f\"pipenv install --skip-lock {' '.join(requirements)}\",\n            directory)\n", "import_code": ["from typing import Union, List, Set, Dict\n", "from typing import Union, List, Set, Dict\n", "from typing import Union, List, Set, Dict\n", "from pathlib import Path\n"]}
{"id": "79d1090a-0407-3574-bc7f-57e130d6a537_7", "content": "def get_item_yaml_values(item_path: pathlib.Path, keys: Union[str, Set[str]]\n    ) ->Dict[str, Set[str]]:\n    \"\"\"\n    Getting value from item.yaml requested field.\n\n    :param item_path:       The path to the item.yaml file or the parent dir of the item.yaml.\n    :param keys:            The fields names that contains the required values to collect,\n                            also looks for the fields inside `spec` inside dict.\n\n    :returns:               Set with all the values inside key.\n    \"\"\"\n    if isinstance(keys, str):\n        keys = {keys}\n    values_dict = {}\n    for key in keys:\n        values_set = set()\n        item_path = Path(item_path)\n        if item_path.is_dir():\n            item_path = item_path / 'item.yaml'\n        with open(item_path, 'r') as f:\n            item = yaml.full_load(f)\n        if key in item:\n            values = item.get(key, '')\n        elif 'spec' in item and key in item['spec']:\n            values = item['spec'].get(key, '') or ''\n        else:\n            values = ''\n        if values:\n            if isinstance(values, list):\n                values_set = set(values)\n            else:\n                values_set.add(values)\n        values_dict[key] = values_set\n    return values_dict\n", "import_code": ["from typing import Union, List, Set, Dict\n", "from typing import Union, List, Set, Dict\n", "import pathlib\n", "from pathlib import Path\n", "import yaml\n", "from typing import Union, List, Set, Dict\n", "from pathlib import Path\n"]}
{"id": "79d1090a-0407-3574-bc7f-57e130d6a537_8", "content": "def get_mock_requirements(source_dir: Union[str, Path]) ->List[str]:\n    \"\"\"\n    Getting all requirements from .py files inside all the subdirectories of the given source dir.\n    Only the files with the same name as their parent directory are taken in consideration.\n    The requirements are being collected from rows inside the files that starts with `from` or `import`\n    and parsed only to the base package.\n\n    :param source_dir: The directory that contains all the functions.\n\n    :return: A list of all the requirements.\n    \"\"\"\n    mock_reqs = set()\n    if isinstance(source_dir, Path):\n        source_dir = source_dir.__str__()\n    for filename in iglob(f'{source_dir}/**/*.py'):\n        file_path = Path(filename)\n        if file_path.parent.name != file_path.stem:\n            continue\n        with open(filename, 'r') as f:\n            lines = list(filter(None, f.read().split('\\n')))\n            for line in lines:\n                words = line.split(' ')\n                words = [w for w in words if w]\n                if words and (words[0] == 'from' or words[0] == 'import'):\n                    mock_reqs.add(words[1].split('.')[0])\n    return sorted(mock_reqs)\n", "import_code": ["from typing import Union, List, Set, Dict\n", "from typing import Union, List, Set, Dict\n", "from pathlib import Path\n", "from glob import iglob\n"]}
{"id": "e17f19f9-1c26-3193-b897-9bce62b6f3cc_3", "content": "def getItem(d, f):\n    if f[3] != '_' and f[3] != '-':\n        return f, None, lambda cmd=d + f: fun(cmd)\n    if f[4:9] == '-----':\n        return '', None, 'separator'\n    return f[4:], None, lambda cmd=d + f: fun(cmd)\n", "import_code": []}
{"id": "3f79ba99-0005-32eb-b2e8-379da77494ca_0", "content": "def get_season_table(df_laliga, season_name):\n    seasonTable = df_laliga[df_laliga['season'] == season_name]\n    seasonTable = seasonTable.sort_values(['points', 'goal_difference'],\n        ascending=False)\n    return seasonTable\n", "import_code": []}
{"id": "4f1df186-6ac8-3482-b252-2b79581a90bb_0", "content": "def read_pgm(pgmf):\n    assert pgmf.readline() == 'P5\\n'\n    width, height = [int(i) for i in pgmf.readline().split()]\n    depth = int(pgmf.readline())\n    assert depth <= 255\n    raster = []\n    for y in range(height):\n        row = []\n        for y in range(width):\n            row.append(ord(pgmf.read(1)))\n        raster.append(row)\n    return raster\n", "import_code": []}
{"id": "4f1df186-6ac8-3482-b252-2b79581a90bb_1", "content": "def part1_1_b(s):\n    s = s\n    s_diag = np.zeros((361, 361), dtype=float)\n    np.fill_diagonal(s_diag, s[:361])\n    energy = np.zeros(361, dtype=float)\n    prev_energy = 0\n    for x in range(361):\n        energy[x] = s[x] ** 2 + prev_energy\n        prev_energy = energy[x]\n    max_e = np.max(energy)\n    normalized_energy = energy / max_e\n    normalized_energy = np.array(normalized_energy)\n    return normalized_energy\n", "import_code": ["import numpy as np\n"]}
{"id": "4f1df186-6ac8-3482-b252-2b79581a90bb_2", "content": "def part1_1_c(normalized_energy):\n    normalized_energy = normalized_energy\n    check_99 = True\n    check_95 = True\n    check_90 = True\n    for x in range(normalized_energy.size):\n        if normalized_energy[x] >= 0.99 and check_99:\n            indice_99 = x\n            check_99 = False\n        if normalized_energy[x] >= 0.95 and check_95:\n            indice_95 = x\n            check_95 = False\n        if normalized_energy[x] >= 0.9 and check_90:\n            indice_90 = x\n            check_90 = False\n    return indice_99, indice_95, indice_90\n", "import_code": []}
{"id": "4f1df186-6ac8-3482-b252-2b79581a90bb_3", "content": "def initialization(rank_value, unitary_matrix_u, singular_values,\n    unitary_matrix_vh):\n    W_begin = np.zeros([361, rank_value])\n    H_begin = np.zeros([rank_value, unitary_matrix_vh[0].size])\n    for k_r in range(rank_value):\n        u_plus_arr, u_minus_arr, vh_plus_arr, vh_minus_arr = [], [], [], []\n        u_plus_arr.append(np.maximum(0, unitary_matrix_u[:, k_r]))\n        u_minus_arr.append(np.maximum(0, -1 * unitary_matrix_u[:, k_r]))\n        vh_plus_arr.append(np.maximum(0, unitary_matrix_vh[:, k_r]))\n        vh_minus_arr.append(np.maximum(0, -1 * unitary_matrix_vh[:, k_r]))\n        u_minus_arr = np.array(u_minus_arr)\n        u_plus_arr = np.array(u_plus_arr)\n        vh_plus_arr = np.array(vh_plus_arr)\n        vh_minus_arr = np.array(vh_minus_arr)\n        M_plus_arr = np.outer(u_plus_arr, vh_plus_arr.T)\n        M_minus_arr = np.outer(u_minus_arr, vh_minus_arr.T)\n        if np.linalg.norm(M_plus_arr) >= np.linalg.norm(M_minus_arr):\n            w_k = u_plus_arr / np.linalg.norm(u_plus_arr)\n            h_k_trans = singular_values[k_r] * np.linalg.norm(u_plus_arr\n                ) * vh_plus_arr\n        else:\n            w_k = u_minus_arr / np.linalg.norm(u_minus_arr)\n            h_k_trans = singular_values[k_r] * np.linalg.norm(u_minus_arr\n                ) * vh_minus_arr\n        W_begin[:, k_r] = w_k\n        H_begin[k_r, :] = h_k_trans\n    return W_begin, H_begin\n", "import_code": ["import numpy as np\n"]}
{"id": "4f1df186-6ac8-3482-b252-2b79581a90bb_4", "content": "def hals_update(x_hals, w_hals, h_hals, rank):\n    for l in range(rank):\n        temp_sum = np.zeros(np.expand_dims(w_hals[:, l], 1).shape)\n        for k in range(rank):\n            if k != l:\n                temp_sum += np.outer(w_hals[:, k], h_hals[k, :] @ h_hals[l,\n                    :].T)\n        temp_sum = np.array(temp_sum)\n        pay_w_update = x_hals @ h_hals[l, :].T - temp_sum.T\n        payda_w_update = np.linalg.norm(h_hals[l, :]) ** 2\n        w_nonegative = np.maximum(np.ones(pay_w_update.shape) * 1e-10, \n            pay_w_update / payda_w_update)\n        w_hals[:, l] = w_nonegative[0].T\n    return w_hals\n", "import_code": ["import numpy as np\n"]}
{"id": "4f1df186-6ac8-3482-b252-2b79581a90bb_5", "content": "def part1_2(X_first, rank, u, s, vh):\n    rank = rank\n    W, H = initialization(rank, u, s, vh.T)\n    for x in range(rank):\n        W = hals_update(X_first, W, H, rank)\n        H = hals_update(X_first.T, H.T, W.T, rank).T\n    return W, H, rank\n", "import_code": []}
{"id": "97b778ef-e48a-38cf-b2d4-12bc08315515_0", "content": "def random_set_of_mean(counter):\n    dataset = []\n    for i in range(0, counter):\n        random_index = random.randint(0, len(data))\n        value = data[random_index]\n        dataset.append(value)\n    mean = statistics.mean(dataset)\n    return mean\n", "import_code": ["import statistics\n", "import random\n"]}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_0", "content": "def powers_of_two(starting_power, ending_power, is_int_type=False):\n    assert ending_power >= starting_power\n    return np.logspace(starting_power, ending_power, num=ending_power -\n        starting_power + 1, base=2, dtype='float64' if is_int_type is False\n         else 'int64')\n", "import_code": ["import numpy as np\n"]}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_1", "content": "def set_object_variables(obj, d, abort_if_exists=False, abort_if_notexists=True\n    ):\n    d_to = vars(obj)\n    assert not abort_if_exists or all([(k not in d_to) for k in d.keys()])\n    assert not abort_if_notexists or all([(k in d_to) for k in d.keys()])\n    for k, v in d.items():\n        assert not hasattr(obj, k)\n        setattr(obj, k, v)\n    return obj\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_2", "content": "def get_object_variables(obj, varnames, tuple_fmt=False):\n    d = vars(obj)\n    if tuple_fmt:\n        return tuple([d[k] for k in varnames])\n    else:\n        return subset_dict_via_selection(d, varnames)\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_3", "content": "def partial_apply(fn, d):\n    return functools.partial(fn, **d)\n", "import_code": ["import functools\n"]}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_4", "content": "def to_list_fn(f):\n    return lambda xs: list(map(f, xs))\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_5", "content": "def transform(x, fns):\n    for f in fns:\n        x = f(x)\n    return x\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_6", "content": "def zip_toggle(xs):\n    \"\"\"\n    Toggle between two formats of a list of lists:\n    [[x1, ...], [x2, ...], [x3, ...]] --> [(x1, x2, .., xn) ...]\n    [(x1, x2, .., xn) ...] --> [[x1, ...], [x2, ...], [x3, ...]]\n    \"\"\"\n    assert isinstance(xs, list)\n    return list(zip(*xs))\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_7", "content": "def iter_product(lst_lst_vals, tuple_fmt=True):\n    vs = list(itertools.product(*lst_lst_vals))\n    if not tuple_fmt:\n        vs = list(map(list, vs))\n    return vs\n", "import_code": ["import itertools\n"]}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_8", "content": "def iter_ortho_all(lst_lst_vals, reference_idxs, ignore_repeats=True):\n    assert len(lst_lst_vals) == len(reference_idxs)\n    ref_r = [lst_lst_vals[pos][idx] for pos, idx in enumerate(reference_idxs)]\n    rs = [] if not ignore_repeats else [tuple(ref_r)]\n    num_lsts = len(lst_lst_vals)\n    for i in range(num_lsts):\n        num_vals = len(lst_lst_vals[i])\n        for j in range(num_vals):\n            if ignore_repeats and j == reference_idxs[i]:\n                continue\n            r = list(ref_r)\n            r[i] = lst_lst_vals[i][j]\n            rs.append(tuple(r))\n    return rs\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_9", "content": "def iter_ortho_single(lst_lst_vals, reference_idxs, iteration_idx,\n    put_reference_first=True):\n    assert len(lst_lst_vals) == len(reference_idxs)\n    ref_r = [lst_lst_vals[pos][idx] for pos, idx in enumerate(reference_idxs)]\n    rs = [] if not put_reference_first else [tuple(ref_r)]\n    num_vals = len(lst_lst_vals[iteration_idx])\n    for j in range(num_vals):\n        if put_reference_first and j == reference_idxs[iteration_idx]:\n            continue\n        r = [lst_lst_vals[pos][idx] for pos, idx in enumerate(reference_idxs)]\n        r[iteration_idx] = lst_lst_vals[iteration_idx][j]\n        rs.append(tuple(r))\n    return rs\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_10", "content": "def get_argument_names(fn):\n    return inspect.getargspec(fn).args\n", "import_code": ["import inspect\n"]}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_11", "content": "def create_dict(ks, vs):\n    assert len(ks) == len(vs)\n    return dict(list(zip(ks, vs)))\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_13", "content": "def copy_update_dict(d, d_other):\n    proc_d = dict(d)\n    proc_d.update(d_other)\n    return proc_d\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_14", "content": "def merge_dicts(ds):\n    out_d = {}\n    for d in ds:\n        for k, v in d.items():\n            assert k not in out_d\n            out_d[k] = v\n    return out_d\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_15", "content": "def groupby(xs, fn):\n    assert isinstance(xs, list)\n    d = {}\n    for x in xs:\n        fx = fn(x)\n        if fx not in d:\n            d[fx] = []\n        d[fx].append(x)\n    return d\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_16", "content": "def flatten(d):\n    assert isinstance(d, dict)\n    xs = []\n    for _, k_xs in d.items():\n        assert isinstance(k_xs, list)\n        xs.extend(k_xs)\n    return xs\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_17", "content": "def recursive_groupby(p, fn):\n    assert isinstance(p, (dict, list))\n    if isinstance(p, list):\n        return groupby(p, fn)\n    else:\n        return {k: recursive_groupby(k_p, fn) for k, k_p in p.items()}\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_18", "content": "def recursive_flatten(p):\n    assert isinstance(p, (dict, list))\n    if isinstance(p, list):\n        return list(p)\n    else:\n        xs = []\n        for _, k_p in p.items():\n            xs.extend(recursive_flatten(k_p))\n        return xs\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_19", "content": "def recursive_map(p, fn):\n    assert isinstance(p, (dict, list))\n    if isinstance(p, list):\n        return list(map(fn, p))\n    else:\n        return {k: recursive_map(k_p, fn) for k, k_p in p.items()}\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_20", "content": "def recursive_index(d, ks):\n    for k in ks:\n        d = d[k]\n    return d\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_21", "content": "def filter_dict(d, fn):\n    return {k: v for k, v in d.items() if fn(k, v)}\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_22", "content": "def map_dict(d, fn):\n    return {k: fn(k, v) for k, v in d.items()}\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_23", "content": "def invert_injective_dict(a_to_b):\n    b_to_a = dict([(b, a) for a, b in a_to_b.items()])\n    return b_to_a\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_24", "content": "def invert_noninjective_dict(a_to_b):\n    d = {}\n    for x, y in a_to_b.items():\n        if y not in d:\n            d[y] = []\n        d[y].append(x)\n    return d\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_25", "content": "def structure(ds, ks):\n    get_fn = lambda k: lambda x: x[k]\n    for k in ks:\n        ds = recursive_groupby(ds, get_fn(k))\n    return ds\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_26", "content": "def structure_with_fns(ds, fns):\n    for fn in fns:\n        ds = recursive_groupby(ds, fn)\n    return ds\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_28", "content": "def flatten_nested_list(xs):\n    assert isinstance(xs, list)\n    xs_res = []\n    for x_lst in xs:\n        assert isinstance(x_lst, list)\n        xs_res.extend(x_lst)\n    return xs_res\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_29", "content": "def key_union(ds):\n    ks = []\n    for d in ds:\n        ks.extend(list(d.keys()))\n    return list(set(ks))\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_30", "content": "def key_intersection(ds):\n    assert len(ds) > 0\n    ks = set(ds[0].keys())\n    for d in ds[1:]:\n        ks.intersection_update(list(d.keys()))\n    return list(ks)\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_31", "content": "def key_to_values(ds):\n    out_d = {}\n    for d in ds:\n        for k, v in d.items():\n            if k not in out_d:\n                out_d[k] = set()\n            out_d[k].add(v)\n    return out_d\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_32", "content": "def subset_dict_via_selection(d, ks):\n    return {k: d[k] for k in ks}\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_33", "content": "def subset_dict_via_deletion(d, ks):\n    out_d = dict(d)\n    for k in ks:\n        out_d.pop(k)\n    return out_d\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_35", "content": "def sort_dict_items(d, by_key=True, decreasing=False):\n    key_fn = (lambda x: x[0]) if by_key else lambda x: x[1]\n    return sorted(list(d.items()), key=key_fn, reverse=decreasing)\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_38", "content": "def collapse_nested_dict(d, sep='.'):\n    assert all([(type(k) == str) for k in d.keys()]) and all([all([(type(kk\n        ) == str) for kk in d[k].keys()]) for k in d.keys()])\n    ps = []\n    for k in d.keys():\n        for kk, v in d[k].items():\n            p = k + sep + kk, v\n            ps.append(p)\n    return dict(ps)\n", "import_code": []}
{"id": "01cc6104-0973-3527-8173-8ca202c20b60_39", "content": "def uncollapse_nested_dict(d, sep='.'):\n    assert all([(type(k) == str) for k in d.keys()]) and all([(len(k.split(\n        )) == 2) for k in d.keys()])\n    out_d = []\n    for k, v in d.items():\n        k1, k2 = k.split(sep)\n        if k1 not in out_d:\n            d[k1] = {}\n        d[k1][k2] = v\n    return out_d\n", "import_code": []}
{"id": "6662a239-3dab-387c-8c06-5f686633b670_2", "content": "@app.route('/empleados/<ci>')\ndef un_empleado(ci):\n    return 'empleado a devolver tiene la ci:{}'.format(ci)\n", "import_code": []}
{"id": "6662a239-3dab-387c-8c06-5f686633b670_4", "content": "@app.route('/empleados/<ci>', methods=['PUT'])\ndef actualizar_empleado(ci):\n    return 'empleado a actualizar tiene la ci:{}'.format(ci)\n", "import_code": []}
{"id": "6662a239-3dab-387c-8c06-5f686633b670_5", "content": "@app.route('/empleados/<ci>', methods=['DELETE'])\ndef borrar_empleado(ci):\n    return 'empleado a borrar tiene la ci:{}'.format(ci)\n", "import_code": []}
{"id": "b0e307b0-dc93-30d9-a038-0bf67c6585c1_3", "content": "def pareto_plot(column: pd.Series, use_given_index: bool=False, figsize:\n    Tuple[int, int]=(12, 8), return_freq_df: bool=False):\n    \"\"\"\n    Draw Pareto plot for categorical variable\n\n    Arguments:\n    ----------\n    column: pd.Series\n        Categorical input\n    figsize: Tuple\n        size of the figure\n    return_freq_df: bool\n        Returns frequency dataframe if True\n\n    Example:\n    --------\n    >>> pareto_plot(df['state'], figsize=(20, 10))\n\n    >>> df = pareto_plot(df['area code'], return_freq_df=True)\n    >>> df\n       label  frequency  cumpercentage\n    0    415       1655      49.654965\n    1    510        840      74.857486\n    2    408        838     100.000000\n    \"\"\"\n    freq = column.copy()\n    if use_given_index:\n        freq = column.value_counts().sort_values(ascending=False)\n    freq_df = pd.DataFrame({'label': freq.index, 'frequency': freq.values})\n    freq_df['cumpercentage'] = freq_df['frequency'].cumsum() / freq_df[\n        'frequency'].sum() * 100\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.bar(freq_df.index, freq_df['frequency'], color='C0')\n    ax2 = ax.twinx()\n    ax2.plot(freq_df.index, freq_df['cumpercentage'], color='C1', marker=\n        'D', ms=7)\n    ax2.yaxis.set_major_formatter(PercentFormatter())\n    ax.set_xticks(freq_df.index)\n    ax.set_xticklabels(freq_df['label'], fontsize=10, rotation=35)\n    ax.tick_params(axis='y', colors='C0')\n    ax2.tick_params(axis='y', colors='C1')\n    plt.show()\n    if return_freq_df:\n        return freq_df\n", "import_code": ["import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import matplotlib.pyplot as plt\n", "import matplotlib.pyplot as plt\n", "from typing import Tuple\n", "from matplotlib.ticker import PercentFormatter\n", "from matplotlib.ticker import PercentFormatter\n", "from matplotlib.ticker import PercentFormatter\n"]}
{"id": "26b9a1ed-5d78-3a90-b173-7ba024cd11e3_2", "content": "def date_parser(items):\n    \"\"\"\n    function should take a list of strings as input.\n    Each string in the input list is formatted as 'yyyy-mm-dd hh:mm:ss'.\n    The function should return a list of strings where each element in the\n    returned list contains only the date in the 'yyyy-mm-dd' format.\n    \"\"\"\n    just_dates = [i[0:10] for i in items]\n    return just_dates\n", "import_code": []}
{"id": "26b9a1ed-5d78-3a90-b173-7ba024cd11e3_4", "content": "def number_of_tweets_per_day(df):\n    \"\"\"\n    The function takes a pandas dataframe as input.\n    The function returns a new dataframe , grouped by day, with the numbers of tweets for that day\n    Get index of the new dataframe should be named \"Date\", and the column of the new dataframe should be 'tweets', corresponding to the date and number of 'Tweets, corresponding to the date and number of tweets, respectively.\n    The date and number be formated as yyyy-mm-dd, and should be a datetime object\n    \"\"\"\n    df1 = df['Date'].str.split(expand=True)\n    df['Date'] = df1[0]\n    df = df.groupby('Date').count()\n    return df\n", "import_code": []}
{"id": "26b9a1ed-5d78-3a90-b173-7ba024cd11e3_5", "content": "def word_spliter(df):\n    Split_tweets = [x.lower().split() for x in df['Tweets']]\n    d = pd.DataFrame(np.array(Split_tweets), columns=['Split Tweets'])\n    df_split = df.join(d, lsuffix='Date', rsuffix='Split Tweets')\n    return df_split\n", "import_code": ["import pandas as pd\n", "import pandas as pd\n", "import numpy as np\n", "import numpy as np\n", "import numpy as np\n", "import numpy as np\n"]}
{"id": "6d366872-2b99-3c63-a88b-a0368c8f1548_0", "content": "def check_keywords(input_file, keywords):\n    output_dictionary = {}\n    for x in input_file:\n        temp_str = ''\n        x = x.strip('\\n').strip(' ').strip('\\t')\n        for word in keywords:\n            if word in x.split():\n                temp_str += ',' + word\n        if temp_str != '':\n            if x[-1:] == '\\n':\n                output_dictionary[temp_str[1:]] = x[:-1]\n            else:\n                output_dictionary[temp_str[1:]] = x\n    return output_dictionary\n", "import_code": []}
{"id": "6d366872-2b99-3c63-a88b-a0368c8f1548_1", "content": "def CountWords(text):\n    nltk.download('punkt')\n    nltk.download('averaged_perceptron_tagger')\n    tokens = word_tokenize(text)\n    tokens = [w for w in tokens if w.isalpha()]\n    tagged = nltk.pos_tag(tokens)\n    filtered = [w[0] for w in tagged if w[1] == 'NN' or w[1] == 'NNP' or w ==\n        'NNS' or w == 'NNPS']\n    counts = Counter(filtered)\n    return counts\n", "import_code": ["from nltk.tokenize import word_tokenize\n", "import nltk\n", "from nltk.corpus import stopwords\n", "from nltk.tokenize import word_tokenize\n", "from collections import Counter\n"]}
{"id": "6d366872-2b99-3c63-a88b-a0368c8f1548_2", "content": "def PlotWordHistogram(text):\n    counts = CountWords(text)\n    labels, values = zip(*counts.items())\n    indSort = np.argsort(values)[::-1]\n    labels = np.array(labels)[indSort]\n    values = np.array(values)[indSort]\n    indexes = np.arange(len(labels))\n    bar_width = 0.35\n    plt.bar(indexes, values)\n    plt.xticks(indexes + bar_width, labels, rotation=90)\n    return plt\n", "import_code": ["import matplotlib.pyplot as plt\n", "import numpy as np\n"]}
{"id": "0f1964f2-dd32-38c0-b405-5729ae903213_0", "content": "def hack_calculator(password):\n    letters = {'a': 1, 'b': 2, 'c': 3}\n    for letter in list(password):\n        if letter not in letters:\n            return 'Invalid hack'\n    phrases = {'baa': 20, 'ba': 10}\n    hackPower = 0\n    counter = Counter(list(password))\n    for letter in counter.keys():\n        if letter in letters.keys():\n            hackPower += sum([(letters.get(letter) * i) for i in range(1, \n                counter.get(letter) + 1)])\n    sortedPhrases = collections.OrderedDict(sorted(phrases.items(), key=lambda\n        t: len(t[0]), reverse=True))\n    regex = '|'.join(re.escape(phrase) for phrase in sortedPhrases)\n    hackPower += sum(phrases[match] for match in re.findall(regex, password))\n    return hackPower\n", "import_code": ["import re\n", "import collections\n", "from collections import Counter\n", "from collections import Counter\n"]}
{"id": "361b6e97-cb6d-3cf4-ae52-3b92eb66ae38_0", "content": "def project_path(to_dir: str=None, filename: str=None, create: bool=True\n    ) ->str:\n    \"\"\"\n    Creates a path to the designated directory in the project's root. By default, if the\n    directory does not exist, this method will create it automatically.\n\n    Parameters\n    ----------\n    to_dir: str\n        The project directory to point to.\n    filename: str | None\n        An optional filename to point to.\n\n    Returns\n    -------\n    str\n        The constructed path pointing to the desired directory/file.\n    \"\"\"\n    root = os.path.dirname(__file__)\n    project = os.path.join(root, '..')\n    if to_dir is not None:\n        directory = os.path.join(project, to_dir)\n    else:\n        directory = project\n    if create and not os.path.exists(directory):\n        os.makedirs(directory)\n    if filename:\n        return os.path.join(directory, filename)\n    return directory.replace(' ', '\\\\ ')\n", "import_code": ["import os\n"]}
{"id": "361b6e97-cb6d-3cf4-ae52-3b92eb66ae38_1", "content": "def system_path(to_dir: str, filename: str=None) ->str:\n    \"\"\"\n    Creates a path to the designated directory on the disk.\n    Note: This method does NOT create the directory if it doesn't already\n    exist.\n\n    Parameters\n    ----------\n    to_dir: str\n        The project directory to point to.\n    filename: str | None\n        An optional filename to point to.\n\n    Returns\n    -------\n    str\n        The constructed path pointing to the desired directory/file.\n    \"\"\"\n    root = os.path.expanduser('~/')\n    directory = os.path.join(root, to_dir)\n    if filename:\n        return os.path.join(directory, to_dir)\n    return directory\n", "import_code": ["import os\n"]}
{"id": "361b6e97-cb6d-3cf4-ae52-3b92eb66ae38_2", "content": "def isfile(directory: str) ->bool:\n    \"\"\"\n    Determines if the object located at a specific directory is a file or not.\n\n    Parameters\n    ----------\n    directory: str\n        The full path to the directory/file to be checked.\n    \"\"\"\n    return os.path.isfile(directory)\n", "import_code": ["import os\n"]}
{"id": "361b6e97-cb6d-3cf4-ae52-3b92eb66ae38_3", "content": "def remove(project_directory: str=None, system_directory: str=None, file:\n    str=None) ->bool:\n    \"\"\"\n    Deletes an entire directory, if possible. Alternatively, you can remove a\n    single file from this directory by designating a specific file to delete.\n\n    Warning\n    -------\n    This method is designed to be very unforgiving. Only use this if you are\n    **absolutely** certain you no longer need the supplied directory.\n\n    Parameters\n    ----------\n    project_directory: str | None\n        The path relative to the project you wish to remove.\n    system_directory: str | None\n        The path relative to the system you wish to remove.\n\n    Returns\n    -------\n    bool\n        A bool indicating if the process was successful.\n    \"\"\"\n    if project_directory is not None and file is not None:\n        project_file_path = project_path(project_directory, file)\n        if isfile(project_file_path):\n            os.remove(project_file_path)\n        return True\n    if project_directory is not None:\n        project_directory_path = project_path(project_directory)\n        shutil.rmtree(project_directory_path)\n        return True\n    if system_directory is not None and file is not None:\n        system_file_path = system_path(system_directory, file)\n        if isfile(system_file_path):\n            os.remove(system_file_path)\n        return True\n    if system_directory is not None:\n        system_directory_path = system_path(system_directory)\n        shutil.rmtree(system_directory_path)\n        return True\n    if file is not None:\n        if isfile(file):\n            os.remove(file)\n        return True\n    return False\n", "import_code": ["import shutil\n", "import os\n"]}
{"id": "361b6e97-cb6d-3cf4-ae52-3b92eb66ae38_5", "content": "def list_dir(directory: str) ->list:\n    \"\"\"\n    Returns a list of file paths in a given directory.\n\n    Parameters\n    ----------\n    directory: str\n        The directory to list.\n\n    Returns\n    -------\n    list(str)\n        A list of file paths in the given directory.\n    \"\"\"\n    contents = []\n    for filename in os.listdir(directory):\n        file = f'{directory}/{filename}'\n        contents.append(file)\n    return contents\n", "import_code": ["import os\n"]}
{"id": "361b6e97-cb6d-3cf4-ae52-3b92eb66ae38_6", "content": "def file_name(from_dir: str) ->str:\n    \"\"\"\n    Extracts the name of the file from an absolute path. i.e., given the absolute path\n    of `foo/bar.txt` this method will return `bar.txt`.\n\n    Parameters\n    ----------\n    from_dir: str\n        The absolute path to extract the file name from.\n\n    Returns\n    -------\n    str\n        The name of the file at the given path.\n    \"\"\"\n    if isfile(from_dir):\n        parts = from_dir.split('/')\n        return parts[len(parts) - 1]\n    return None\n", "import_code": []}
{"id": "185957f8-a34b-38a0-b94b-60dc4ecf2010_0", "content": "def search_name(book):\n    \"\"\"\n    This function changes the current directory to the parent directory and loads the Excel file \"BibleCodes.xlsx\".\n    It then iterates over the rows of the sheet \"Sheet1\" to find the book name that matches the input book.\n    If a match is found, it returns the short form of the book name.\n    \"\"\"\n    os.chdir('..')\n    excel_file = 'BibleCodes.xlsx'\n    if os.path.exists(excel_file):\n        work_book = load_workbook(excel_file)\n        worksheet = work_book.get_sheet_by_name('Sheet1')\n        work_book.active\n        for row in range(2, worksheet.max_row + 1):\n            book_name = worksheet['A' + str(row)].value\n            if str(book) == str(book_name):\n                short_name = worksheet['B' + str(row)].value\n                os.chdir('docx')\n                return short_name\n", "import_code": ["from openpyxl import load_workbook\n", "import os\n"]}
{"id": "d59c7319-b2f8-3926-a5fe-e846d3444991_2", "content": "def convert_to_docx(files):\n    \"\"\"\n    function to iterate over the list of files for conversion\n    :param path: set of *.doc word file path\n    :return: none\n    \"\"\"\n    print('------- doc to docx conversion process -------')\n    for file in files:\n        subprocess.call(['soffice', '--headless', '--convert-to', 'docx',\n            file, '--outdir', os.path.dirname(file)])\n        update_file = file + 'x'\n        return update_file\n", "import_code": ["import os\n", "from os.path import dirname\n", "import subprocess\n"]}
{"id": "d59c7319-b2f8-3926-a5fe-e846d3444991_3", "content": "def abst_ext(file):\n    \"\"\"\n     Extracts the abstract of a given document\n    :param file: a word file path\n    :return: abstract of the given word document\n    \"\"\"\n    doc = Document(file)\n    for para in doc.paragraphs:\n        text = para.text.split(' ')\n        if len(text) > 50:\n            return para.text\n        else:\n            pass\n", "import_code": ["from docx import Document\n"]}
{"id": "d59c7319-b2f8-3926-a5fe-e846d3444991_4", "content": "def title_ext(file):\n    \"\"\"\n         Extracts the title of a given document\n        :param file: a word file path\n        :return: title of the given word document\n    \"\"\"\n    doc = Document(file)\n    title = doc.paragraphs[0].text\n    return title\n", "import_code": ["from docx import Document\n"]}
{"id": "d59c7319-b2f8-3926-a5fe-e846d3444991_5", "content": "def word_count(text):\n    \"\"\"\n    Counts the number of words in a given text\n    :param text: full content of a word document\n    :return: no of words in the text\n    \"\"\"\n    words = nltk.word_tokenize(text)\n    puntuation_removed_words = [x for x in words if not re.fullmatch('[' +\n        string.punctuation + ']+', x)]\n    no_of_words = puntuation_removed_words.__len__()\n    if no_of_words <= 4000:\n        c = 1\n    elif (no_of_words < 8000) & (no_of_words > 4000):\n        c = 2\n    else:\n        c = 3\n    return c\n", "import_code": ["import nltk.data\n", "import nltk\n", "import re\n", "import string\n"]}
{"id": "d59c7319-b2f8-3926-a5fe-e846d3444991_6", "content": "def get_tokens_lengths(s):\n    \"\"\"\n    Counts the no of words in a given sentence, by considering only the alphabets and numerics\n    :param s: A sentence\n    :return: No of words in a given sentence\n    \"\"\"\n    tokenize_string = str(s).split(' ')\n    remove_punctuation = list(filter(None, [re.sub('[^A-Za-z0-9]+', '', x) for\n        x in tokenize_string]))\n    count_of_tokens = remove_punctuation.__len__()\n    return count_of_tokens\n", "import_code": ["import re\n"]}
{"id": "d59c7319-b2f8-3926-a5fe-e846d3444991_7", "content": "def file_name_ext(file):\n    \"\"\"\n     Extracts the file name from the given path\n    :param file: file path of a word document\n    :return: word document name\n    \"\"\"\n    tmp_doc_file_path = file\n    pos = tmp_doc_file_path.rfind('\\\\')\n    file_name = tmp_doc_file_path[pos + 1:pos + 11]\n    return file_name\n", "import_code": []}
{"id": "6e65951f-5fa4-3511-943d-51e1362edf01_4", "content": "def read_tare_from_file(filename, designate_location=False):\n    \"\"\"\n    This function takes in a filename and an optional designate location as inputs.\n    It sets the data directory to the specified location if provided, otherwise it defaults to a specific directory.\n    It reads the tare data from the specified file into a DataFrame. It then extracts the tare count and standard deviation\n    from the DataFrame. Finally, it returns the extracted tare data.\n    \"\"\"\n    if designate_location == False:\n        data_directory = '/home/pi/Documents/MSci-Project/Data/Tares/'\n    file_to_read = data_directory + filename\n    df = pd.read_csv(file_to_read)\n    tare_data = [[df['Tare Count Load Cell 1'], df['Tare Count Load Cell 2'\n        ], df['Tare Count Load Cell 3'], df['Tare Count Load Cell 4']], [df\n        ['Tare Stds Load Cell 1'], df['Tare Stds Load Cell 2'], df[\n        'Tare Stds Load Cell 3'], df['Tare Stds Load Cell 4']]]\n    return tare_data\n", "import_code": ["import pandas as pd\n"]}
{"id": "6e65951f-5fa4-3511-943d-51e1362edf01_6", "content": "def link_raw_and_tare(session_location, raw_file_name):\n    \"\"\"\n    This function takes in a session location and a raw file name as inputs.\n    It sets the directory for the tare data. It finds the latest tare file in the directory.\n    It then creates a pandas DataFrame with the raw file name and the tare file name.\n    Finally, it appends the DataFrame to a CSV file called 'Calibration_index.csv' in the session location.\n    \"\"\"\n    tares_directory = '/home/pi/Documents/MSci-Project/Data/Tares/{}'.format(\n        session_location)\n    print(tares_directory)\n    list_of_files = glob.glob(tares_directory + '*.csv')\n    print(list_of_files)\n    latest_file = max(list_of_files, key=os.path.getctime)\n    print(latest_file)\n    tare_file_name = os.path.basename(latest_file)\n    raw_and_tare = pd.DataFrame({'raw_file': [str(raw_file_name)],\n        'tare_file': [str(tare_file_name)]})\n    with open(\n        '/home/pi/Documents/MSci-Project/Data/Raw Recorded Data/{}/Calibration_index.csv'\n        .format(session_location), 'a') as f:\n        raw_and_tare.to_csv(f, header=False)\n    return\n", "import_code": ["import pandas as pd\n", "import glob\n", "import os\n"]}
{"id": "6e65951f-5fa4-3511-943d-51e1362edf01_7", "content": "def find_tare_filename(raw_filename):\n    \"\"\"\n    This function takes in a raw filename as input.\n    It reads the 'Calibration_index.csv' file to get the index data.\n    It then finds the tare filename corresponding to the given raw filename.\n    Finally, it returns the raw filename and the tare filename.\n    \"\"\"\n    index_data = pd.read_csv('Calibration_index.csv', index_col=0)\n    tare_filename = index_data[index_data.raw_file.str.contains(str(\n        raw_filename), case=False)]['tare_file'][0]\n    return raw_filename, tare_filename\n", "import_code": ["import pandas as pd\n"]}
{"id": "c6a8748a-ff2a-3592-b0ac-a487fad95687_0", "content": "def calcolaPurity(labelConosciute, labels):\n    contingencymatrix = contingency_matrix(labelConosciute, labels)\n    purity = np.sum(np.amax(contingencymatrix, axis=0)) / np.sum(\n        contingencymatrix)\n    return purity\n", "import_code": ["import numpy as np\n", "from sklearn.metrics.cluster import contingency_matrix\n"]}
{"id": "c6a8748a-ff2a-3592-b0ac-a487fad95687_1", "content": "def evaluation(X_selected, X_test, n_clusters, y):\n    \"\"\"\n    This function calculates ARI, ACC and NMI of clustering results\n\n    Input\n    -----\n    X_selected: {numpy array}, shape (n_samples, n_selected_features}\n            input data on the selected features\n    n_clusters: {int}\n            number of clusters\n    y: {numpy array}, shape (n_samples,)\n            true labels\n\n    Output\n    ------\n    nmi: {float}\n        Normalized Mutual Information\n    acc: {float}\n        Accuracy\n    \"\"\"\n    k_means = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10,\n        max_iter=300, tol=0.0001, precompute_distances=True, verbose=0,\n        random_state=None, copy_x=True, n_jobs=1)\n    k_means.fit(X_selected)\n    y_predict = k_means.predict(X_test)\n    nmi = normalized_mutual_info_score(y, y_predict, average_method=\n        'arithmetic')\n    sil = silhouette_score(X_test, y_predict, metric='euclidean')\n    db_score = davies_bouldin_score(X_test, y_predict)\n    ch_score = calinski_harabasz_score(X_test, y_predict)\n    purity = calcolaPurity(y, y_predict)\n    return nmi, sil, db_score, ch_score, purity\n", "import_code": ["from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, normalized_mutual_info_score, confusion_matrix\n", "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, normalized_mutual_info_score, confusion_matrix\n", "from sklearn.cluster import KMeans\n", "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, normalized_mutual_info_score, confusion_matrix\n", "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, normalized_mutual_info_score, confusion_matrix\n"]}
{"id": "944e2963-b7ee-3895-aade-13ca1efab694_0", "content": "def xis(arg):\n    \"\"\"This function executes bash commands\"\"\"\n    bashCommand = arg\n    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n    output = process.communicate()[0]\n    print(output)\n    return process\n", "import_code": ["import subprocess\n"]}
{"id": "10887a15-7ef6-3863-aae6-884066396723_1", "content": "def for_each_cb(drop):\n    \"\"\"\n    To be called on each file upon pressing the \"process\" button.\n    This function is called for each file to be processed.\n    It takes the `drop` object as a parameter, which contains information about the file to be processed.\n    It prints a message indicating that the callback is happening, and then returns the path to the file.\n    \"\"\"\n    file_ = drop.path\n    print('CALLBACK HAPPENING!! processing: {}'.format(file_))\n    return file_\n", "import_code": []}
{"id": "10887a15-7ef6-3863-aae6-884066396723_2", "content": "def fail_condition_cb(result):\n    \"\"\"\n    Checks the result of for_each_cb and decides whether the file\n    failed or not.\n    This function checks the result from `for_each_cb` and decides whether the file processing failed or not.\n    If the result is `False`, it returns `True` to indicate that the file failed.\n    \"\"\"\n    if not result:\n        return True\n", "import_code": []}
{"id": "40ebc567-570d-3ebb-ab12-60ac1ac4c046_0", "content": "def imReadAndConvert(filename: str, representation: int) ->np.ndarray:\n    im = cv2.imread(filename)\n    np_im = cv2.normalize(im.astype('double'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n    if len(np_im.shape) < 3 and representation == 2:\n        print(1)\n        np_im = gray2rgb(im)\n    elif len(np_im.shape) == 3 and representation == 1:\n        print(3)\n        np_im = rgb2gray(np_im)\n    return np_im\n", "import_code": ["import cv2\n", "import numpy as np\n"]}
{"id": "40ebc567-570d-3ebb-ab12-60ac1ac4c046_1", "content": "def rgb2gray(rgb):\n    r, g, b = rgb[:, :, 0], rgb[:, :, 1], rgb[:, :, 2]\n    gray = 0.2989 * r + 0.587 * g + 0.114 * b\n    return gray\n", "import_code": []}
{"id": "40ebc567-570d-3ebb-ab12-60ac1ac4c046_2", "content": "def gray2rgb(gray):\n    rgb = list()\n    rgb[:, :, 0], rgb[:, :, 1], rgb[:, :, 2\n        ] = gray.color, gray.color, gray.color\n    return rgb\n", "import_code": []}
{"id": "40ebc567-570d-3ebb-ab12-60ac1ac4c046_4", "content": "def transformRGB2YIQ(imRGB: np.ndarray) ->np.ndarray:\n    if len(imRGB.shape) == 3:\n        yiq_ = np.array([[0.299, 0.587, 0.114], [0.596, -0.275, -0.321], [\n            0.212, -0.523, 0.311]])\n        imYI = np.dot(imRGB, yiq_.T.copy())\n        return imYI\n", "import_code": ["import numpy as np\n"]}
{"id": "40ebc567-570d-3ebb-ab12-60ac1ac4c046_5", "content": "def transformYIQ2RGB(imYIQ: np.ndarray) ->np.ndarray:\n    if len(imYIQ.shape) == 3:\n        rgb_ = np.array([[1.0, 0.956, 0.623], [1.0, -0.272, -0.648], [1.0, \n            -1.105, 0.705]])\n        imRGB = np.dot(imYIQ, rgb_.T.copy())\n        return imRGB\n", "import_code": ["import numpy as np\n"]}
{"id": "40ebc567-570d-3ebb-ab12-60ac1ac4c046_6", "content": "def histogramEqualize(imOrig: np.ndarray) ->(np.ndarray, np.ndarray, np.ndarray\n    ):\n    temp = imOrig.copy()\n    if len(imOrig.shape) == 3:\n        imOrig = cv2.normalize(imOrig, None, 0, 255, cv2.NORM_MINMAX)\n        imOrig = np.ceil(imOrig)\n        imOrig = imOrig.astype('uint8')\n        imOrig = cv2.cvtColor(imOrig, cv2.COLOR_BGR2RGB)\n        imOrig = cv2.normalize(imOrig.astype('double'), None, 0.0, 1.0, cv2\n            .NORM_MINMAX)\n        imyiq = transformRGB2YIQ(imOrig)\n        imOrig = imyiq[:, :, 0]\n    imOrig = cv2.normalize(imOrig, None, 0, 255, cv2.NORM_MINMAX)\n    imOrig = np.ceil(imOrig)\n    imOrig = imOrig.astype('uint8')\n    hist, bins = np.histogram(imOrig.flatten(), 256, [0, 256])\n    cdf = hist.cumsum()\n    cdf_normalized = cdf * hist.max() / cdf.max()\n    plt.plot(cdf_normalized, color='b')\n    plt.hist(imOrig.flatten(), 256, [0, 256], color='r')\n    plt.xlim([0, 256])\n    plt.legend(('CDF', 'Histogram'), loc='upper left')\n    cdf_m = np.ma.masked_equal(cdf, 0)\n    cdf_m = (cdf_m - cdf_m.min()) * 255 / (cdf_m.max() - cdf_m.min())\n    cdf = np.ma.filled(cdf_m, 0).astype('uint8')\n    imOrig = imOrig.astype('uint8')\n    img2 = cdf[imOrig]\n    hist, bins = np.histogram(img2.flatten(), 256, [0, 256])\n    cdf_normalized = cdf * hist.max() / cdf.max()\n    hist, bins = np.histogram(img2.flatten(), 256, [0, 256])\n    plt.plot(cdf_normalized, color='b')\n    plt.hist(img2.flatten(), 256, [0, 256], color='g')\n    plt.xlim([0, 256])\n    plt.legend(('CDF', 'Histogram'), loc='upper left')\n    plt.show()\n    if len(temp.shape) == 3:\n        imyiq[:, :, 0] = img2\n        imOrig = transformYIQ2RGB(imyiq)\n        imOrig = cv2.normalize(imOrig, None, 0, 255, cv2.NORM_MINMAX)\n        imOrig = np.ceil(imOrig)\n        imOrig = imOrig.astype('uint8')\n        imOrig = cv2.cvtColor(imOrig, cv2.COLOR_RGB2BGR)\n    return imOrig\n", "import_code": ["import matplotlib.pyplot as plt\n", "import cv2\n", "import numpy as np\n"]}
{"id": "f591bf20-7ec2-3073-b63b-ba418823c805_5", "content": "def documented_func_example(a: int, b: int, *, useless_param=None,\n    _bad_name_param1=None, __bad_name_param2=None, crazy_typing_param:\n    Union[Dict[str, int], List[str]]=None, **kwargs) ->int:\n    \"\"\"\n    Example function that sums to integers. This function is used for pytest.\n    It does not really make sense, this is just for testing docstring rendering.\n\n    Some extended summary.\n\n    Multiline test.\n\n    Custom Section\n    --------------\n    This is for testing that custom sections are not ignored (as opposed to what\n    numpydoc does) and we can fetch the content in the right order.\n\n    Multiline test.\n\n    Notes\n    -----\n    Some notes.\n\n    Multiline test.\n\n    Warnings\n    --------\n    Cautionnary notes for the user.\n    More warning notes.\n\n    Multiline test.\n\n    References\n    ----------\n    [0] Me\n\n    Multiline test.\n\n    See Also\n    --------\n    empty_function, EmptyClass, empty_doc_sections_function, EmptyDocSectionsClass\n        Other useless functions.\n\n        Numpydoc deletes line breaks for section \"See Also\" so no multiline test here...\n        The next function I put in this section will have no description.\n    now_utc\n\n    Parameters\n    ----------\n    a : int\n        First number.\n\n        Multiline test.\n    b : int\n\n    Other Parameters\n    ----------------\n    useless_param\n        Some useless parameter.\n\n        Multiline test.\n\n    _bad_name_param1\n    __bad_name_param2\n    crazy_typing_param\n        Parameter with complex typing\n    optional_type_param\n        Parameter with optional typing\n\n    Examples\n    --------\n    We are going to try to break our library by:\n    1. Putting text before examples or not\n    2. Separating examples by line breaks (also with irrelevant leading spaces) or not\n\n    We'll then check if the library still manages to separate examples properly\n\n    * example 1\n    {{python}}\n    >>> documented_func_example(1, 1)\n    2\n\n    * example 2\n    {{python}}\n    >>> documented_func_example(2, 2)\n    4\n\n    ---\n\n    #### Examples unrelated to the function\n\n    * test rendered markdown\n    {{markdown_rendered}}\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({'A':[1, 2]})\n    >>> print(df.to_markdown())\n    |    |   A |\n    |---:|----:|\n    |  0 |   1 |\n    |  1 |   2 |\n\n    * test raw code block\n    {{raw}}\n    >>> print('Hello world!')\n    Hello world!\n\n    * test non rendered markdown\n    {{markdown}}\n    >>> print('# some markdown string\\\\n* test')\n    # some markdown string\n    * test\n\n    >>> print('test')\n    test\n    >>> print('foo\\\\n')\n    foo\n    <BLANKLINE>\n\n    Returns\n    -------\n    c : int\n        Result of a + b.\n\n        Multiline test.\n\n    Raises\n    ------\n    TypeError\n        If a or b is not a number.\n\n        Multiline test.\n\n    Warns\n    -----\n    UserWarning\n        Never occurs (this is just an example)\n\n        Multiline test.\n    \"\"\"\n    return a + b\n", "import_code": ["from typing import Dict, List, Optional, Union\n", "from typing import Dict, List, Optional, Union\n", "from typing import Dict, List, Optional, Union\n"]}
{"id": "f8fbd423-b6ee-3430-b9f8-13d537e4fb78_0", "content": "def divide_three(img):\n    \"\"\"\n\tThis function takes an image as input and divides each pixel value by 3.\n\t:type img: Image (3D Array)\n\t:return type: 3D Array\n\t\"\"\"\n    for i in range(int(len(img))):\n        row = img[i]\n        for c in range(int(len(row))):\n            row[c] = (row[c] / 3).astype(np.uint8)\n    return img\n", "import_code": ["import numpy as np\n"]}
{"id": "f8fbd423-b6ee-3430-b9f8-13d537e4fb78_1", "content": "def histogram_equalization(img):\n    \"\"\"\n\tThis function takes an image as input and applies histogram equalization to it.\n\t:type img: Image (3D Array)\n\t:return type: 3D Array\n\t\"\"\"\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY).flatten()\n    prob = [(0) for i in range(256)]\n    for i in img:\n        prob[int(i)] += 1\n    cumsum = np.cumsum(prob)\n    cdf_max, cdf_min = max(cumsum), min(cumsum)\n    for i in range(len(prob)):\n        prob[i] = ((cumsum[i] - cdf_min) * 255 / (cdf_max - cdf_min)).astype(np\n            .uint8)\n    for i in range(len(img)):\n        img[i] = prob[img[i]]\n    img = np.reshape(img, (-1, 512))\n    return img\n", "import_code": ["import cv2\n", "import numpy as np\n"]}
{"id": "7190e43b-f59a-3883-b157-ee2fb139c1ff_1", "content": "def hyperbolic_metric(dim):\n    d = np.ones(dim)\n    d[-1] = -1\n    return np.diag(d)\n", "import_code": ["import numpy as np\n"]}
{"id": "7190e43b-f59a-3883-b157-ee2fb139c1ff_2", "content": "def normalize_spherical(p):\n    p = np.array(p)\n    norm = np.sqrt(sum(p ** 2))\n    return p / norm\n", "import_code": ["import numpy as np\n"]}
{"id": "7190e43b-f59a-3883-b157-ee2fb139c1ff_3", "content": "def hyperbolic_inner(p, q):\n    return np.inner(p[0:-1], q[0:-1]) - p[-1] * q[-1]\n", "import_code": ["import numpy as np\n"]}
{"id": "7190e43b-f59a-3883-b157-ee2fb139c1ff_4", "content": "def normalize_hyperbolic(p):\n    p = np.array(p)\n    norm = hyperbolic_inner(p, p)\n    if norm >= 0:\n        raise ValueError('Vector cannot be normalized')\n    return p / -norm\n", "import_code": ["import numpy as np\n"]}
{"id": "7190e43b-f59a-3883-b157-ee2fb139c1ff_5", "content": "def distance_spherical(p, q):\n    return np.arccos(np.inner(p, q))\n", "import_code": ["import numpy as np\n"]}
{"id": "7190e43b-f59a-3883-b157-ee2fb139c1ff_6", "content": "def distance_hyperbolic(p, q):\n    return np.arccosh(-hyperbolic_inner(p, q))\n", "import_code": ["import numpy as np\n"]}
{"id": "fba24880-b312-3ced-b2e2-626e32792e39_0", "content": "def convert2BW(img_in):\n    im_file = Image.open(img_in)\n    im_BW = im_file.convert('L')\n    im_array = np.array(im_BW)\n    return im_array / 255.0\n", "import_code": ["from PIL import Image\n", "import numpy as np\n"]}
{"id": "fba24880-b312-3ced-b2e2-626e32792e39_1", "content": "def GradientSquared(A):\n    n, m = np.shape(A)\n    gradx = np.zeros((n - 2, m - 2))\n    grady = np.zeros((n - 2, m - 2))\n    gradx = A[0:-2, 1:-1] - 2 * A[1:-1, 1:-1] + A[2:, 1:-1]\n    grady = A[1:-1, 0:-2] - 2 * A[1:-1, 1:-1] + A[1:-1, 2:]\n    return gradx ** 2 + grady ** 2\n", "import_code": ["import numpy as np\n"]}
{"id": "fba24880-b312-3ced-b2e2-626e32792e39_2", "content": "def smooth(A, ntimes=1):\n    kernel = np.ones([5, 5]) / 25\n    As = A\n    for i in range(ntimes):\n        As = signal.convolve2d(As, kernel, boundary='symm', mode='same')\n    return As\n", "import_code": ["from scipy import signal\n", "import numpy as np\n"]}
{"id": "fba24880-b312-3ced-b2e2-626e32792e39_3", "content": "def PlotDiagram(vr, colour, saveName='georgytest.png'):\n    fig, ax = plt.subplots()\n    ax.margins(0.1)\n    ax.set_aspect('equal')\n    plt.axis([-0.1 * Nx, 1.1 * Nx, -0.1 * Ny, 1.1 * Ny])\n    for ir, r in enumerate(vr):\n        if not ifix[ir] and ifix[ir] == 0:\n            c = colors[ir]\n            polygon = [vc[i] for i in vr[r]]\n            ax.add_patch(Polygon(np.transpose(zip(*polygon)), closed=True,\n                fill=True, color=c))\n    plt.axis('off')\n    plt.savefig(saveName, bbox_inches='tight')\n    plt.show()\n    return\n", "import_code": ["import numpy as np\n", "from matplotlib.patches import Polygon\n", "import matplotlib.pyplot as plt\n"]}
{"id": "7d34e89c-ab9d-312d-843c-99cdab46681c_0", "content": "def split_rows(df, col1, col2, sep):\n    \"\"\"\n    input:\n    df: dataframe to use\n    col1 and col2 (use col2 to split)\n    return:\n    a df table with col1 as common col and col2 split into multiple rows\n    \"\"\"\n    series = [pd.Series(row[col1], row[col2].split(sep)) for _, row in df.\n        iterrows()]\n    table = pd.concat(series).reset_index()\n    return table\n", "import_code": ["import pandas as pd\n"]}
{"id": "7d34e89c-ab9d-312d-843c-99cdab46681c_1", "content": "def splitgroups(df, colname, names):\n    \"\"\"\n    input:\n    df: dataframe to split the groups \n    colname: name of the column, a string\n    names: criteria a list of string to split column of interest\n    return: \n    separate dfs, num is length of names, used for split into DC, MD, VA\n    \"\"\"\n    dfs = []\n    for i in range(len(names)):\n        mask = df[colname] == names[i]\n        dfs.append(df[mask].reset_index())\n    return dfs\n", "import_code": []}
{"id": "7d34e89c-ab9d-312d-843c-99cdab46681c_2", "content": "def samples(df, colname, criteria, value):\n    \"\"\"\n    input:\n    df: a dataframe\n    colname: colname of interest for split \n    criteria: an int for filter for colname\n    value: colname for actual comparsion \n    return:\n    2 samples of 1 day numpy array, data1 uses mask, data2 complement mask\n    \"\"\"\n    mask = df[colname] >= criteria\n    data1 = df[mask].reset_index()[value]\n    data2 = df[~mask].reset_index()[value]\n    return data1, data2\n", "import_code": []}
{"id": "7d34e89c-ab9d-312d-843c-99cdab46681c_3", "content": "def table_transform(datas, group_names, colname):\n    \"\"\"\n    input:\n    datas: a list of data for comparision \n    group_names: a list of strings with group names, datas order should be same as group_names\n    colname: string, the category to compare \n    return:\n    tukeyhsd table result and stacked table \n    create stacked dataframe for tukey_hsd and welch F test\n    \"\"\"\n    df = pd.DataFrame()\n    for i in range(len(group_names)):\n        df[group_names[i]] = datas[i]\n    stacked_df = df.stack().reset_index()\n    stacked_df = stacked_df.rename(columns={'level_0': 'id', 'level_1':\n        'state', (0): colname})\n    return stacked_df\n", "import_code": ["import pandas as pd\n"]}
{"id": "8c13bb86-b78d-3ed0-be1d-17d6ed2a1055_0", "content": "def getreviews(giveurl):\n    all_reviews = ''\n    url = ''\n    numreviews = 80\n    for num in range(0, numreviews, 20):\n        url = f'{giveurl}?start={num}'\n        response = requests.get(url)\n        html = response.text\n        soup = BeautifulSoup(html, 'lxml')\n        plist = soup.find_all('p', class_=\n            'lemon--p__373c0__3Qnnj text__373c0__2pB8f comment__373c0__3EKjH text-color--normal__373c0__K_MKN text-align--left__373c0__2pnx_'\n            )\n        for p in plist:\n            all_reviews = all_reviews + ' ' + p.text\n        time.sleep(2)\n    return all_reviews\n", "import_code": ["from bs4 import BeautifulSoup\n", "import time\n", "import requests\n"]}
{"id": "8c13bb86-b78d-3ed0-be1d-17d6ed2a1055_1", "content": "def doc_to_frequencies(doc):\n    myDict = {}\n    stop_words = ['iowa', 'city', 'food', 'good', 'great', 'a', 'about',\n        'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and',\n        'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been',\n        'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\",\n        'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does',\n        \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for',\n        'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have',\n        \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here',\n        \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n        \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into',\n        'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more',\n        'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off',\n        'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours',\n        'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\",\n        \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such',\n        'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them',\n        'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\",\n        \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to',\n        'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we',\n        \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what',\n        \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while',\n        'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would',\n        \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your',\n        'yours', 'yourself', 'yourselves']\n    for word in doc:\n        if word in stop_words:\n            continue\n        if word in myDict:\n            myDict[word] += 1\n        else:\n            myDict[word] = 1\n    return myDict\n", "import_code": []}
{"id": "8c13bb86-b78d-3ed0-be1d-17d6ed2a1055_2", "content": "def remove_punc(words):\n    for i in range(len(words)):\n        words[i] = words[i].replace(',', '')\n        words[i] = words[i].replace('.', '')\n        words[i] = words[i].replace(';', '')\n        words[i] = words[i].replace(':', '')\n        words[i] = words[i].replace('(', '')\n        words[i] = words[i].replace(')', '')\n        words[i] = words[i].replace('\"', '')\n    return words\n", "import_code": []}
{"id": "8c13bb86-b78d-3ed0-be1d-17d6ed2a1055_3", "content": "def top_k(freqs, k):\n    top = []\n    for i in range(k):\n        mfreq = 0\n        mkey = 0\n        for key in freqs:\n            if mfreq < freqs[key]:\n                mfreq = freqs[key]\n                mkey = key\n        top.append([mkey, mfreq])\n        del freqs[mkey]\n    return top\n", "import_code": []}
{"id": "080a8f95-74fa-38de-8c41-c57113da840a_0", "content": "def top_loading_words(v, n, i2w):\n    \"\"\"\n    Returns a list of the top words for a loading vector.\n\n    Parameters\n    ----------\n    v: vector of loadings\n    n: number of words to print\n    i2w: index to words\n    \"\"\"\n    return heapq.nlargest(n, zip(abs(v), i2w))\n", "import_code": ["import heapq\n"]}
{"id": "080a8f95-74fa-38de-8c41-c57113da840a_1", "content": "def top_loading_words_df(V, i2w, n, include_coef=False):\n    \"\"\"\n    Creates a data frame of the top words for each loading.\n    Loadings are indexed starting at 1.\n\n    Parameters\n    ----------\n    loading: the matrix of loading vectors as columns\n    iw: index to word\n    n: number of words for each loading\n    include_coef: possibly include the loading coefficient\n\n    Output\n    ------\n    pandas data frame whose\n    \"\"\"\n    top_words_df = pd.DataFrame(index=range(1, n + 1), columns=range(1, V.\n        shape[1] + 1))\n    w2i = {i2w[i]: i for i in range(len(i2w))}\n    for k in range(V.shape[1]):\n        top = top_loading_words(V[:, k], n, i2w)\n        abs_load, words = zip(*top)\n        if include_coef:\n            words = [('%s (%.1E)' % (words[i], V[w2i[words[i]], k])) for i in\n                range(n)]\n        top_words_df[k + 1] = words\n    return top_words_df\n", "import_code": ["import pandas as pd\n"]}
{"id": "014bdc58-c4b9-3a84-b692-6fb3340bbab3_0", "content": "def generateWaveforms(jsonData):\n    dataSize = int(np.floor(jsonData['Rate'] / jsonData['waveFreq']))\n    outputWaveform = []\n    for channel in range(len(jsonData['Channels'])):\n        for wave in jsonData['Waves'][channel]:\n            wave = np.array(list(map(lambda n: waveforms['sine'](n, wave[\n                'freq'], jsonData['Rate']), np.arange(dataSize, dtype=np.\n                complex64))), dtype=np.complex64)\n            outputWaveform.append(wave)\n    return outputWaveform\n", "import_code": ["import numpy as np\n"]}
{"id": "014bdc58-c4b9-3a84-b692-6fb3340bbab3_1", "content": "def generateOutputWaveforms(allWaves, jsonData):\n    waves = []\n    for channel in range(len(jsonData['Channels'])):\n        wave = allWaves[0][0] * 0\n        i = 0\n        for currentWave in jsonData['Waves'][channel]:\n            print(wave)\n            wave = np.add(wave, currentWave['amplitude'] * allWaves[i] * np\n                .exp(currentWave['phase'] * np.pi * 2.0j))\n            i += 1\n        waves.append(wave)\n    return waves\n", "import_code": ["import numpy as np\n"]}
{"id": "83ff29be-5538-3e1c-b818-328690bccc30_0", "content": "def gauss(x, sd, mu):\n    fx = 1 / (sd * math.sqrt(2 * math.pi)) * math.exp(-(x - mu) ** 2 / (2 *\n        sd ** 2))\n    return fx\n", "import_code": ["import math\n"]}
{"id": "83ff29be-5538-3e1c-b818-328690bccc30_1", "content": "def gauss_integrate(x):\n    v = gauss(x, sig, mew)\n    return v\n", "import_code": []}
{"id": "4c472aa1-fe1e-334b-8178-9641fc71f5d9_0", "content": "def string_to_float(currency):\n    currency = currency.replace('$', '')\n    currency = currency.replace(',', '')\n    return float(currency)\n", "import_code": []}
{"id": "9e40d966-13c1-34ce-97b6-b75278ff26ce_2", "content": "def aggregate_over_time_freq(df, group_col='group', dt_col='date', freq='M',\n    value_col='count'):\n    \"\"\"\n    Sum values by group over a given time frequency, e.g. monthly\n    http://pbpython.com/pandas-grouper-agg.html\n    http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n    \"\"\"\n    g = pd.Grouper(key=dt_col, freq=freq)\n    groupby_vars = [g]\n    if group_col:\n        groupby_vars.append(group_col)\n    return df.groupby(groupby_vars)[value_col].sum()\n", "import_code": ["import pandas as pd\n"]}
{"id": "595fea1d-23a9-39ee-bb89-127108f289b5_0", "content": "def key(entry):\n    pid = entry['patient_id']\n    accession_nr = entry['accession_number']\n    series_nr = entry['series_number']\n    return pid + '_' + accession_nr + '_' + series_nr\n", "import_code": []}
{"id": "595fea1d-23a9-39ee-bb89-127108f289b5_1", "content": "def work_items(work_dir):\n    result_files = glob.glob(work_dir + '/*.json')\n    result = []\n    for f in result_files:\n        with open(f, 'r') as f:\n            entry = json.load(f)\n            result.append(entry)\n    return result\n", "import_code": ["import glob\n", "import json\n"]}
{"id": "27920465-f1e5-3e94-b720-da268a2216af_0", "content": "def days_from_present(x):\n    measureDay = pd.Timestamp('20190101')\n    td = pd.to_datetime(x, yearfirst=True) - measureDay\n    return td.days\n", "import_code": ["import pandas as pd\n"]}
{"id": "27920465-f1e5-3e94-b720-da268a2216af_1", "content": "def processing(df):\n    df['date'] = '2000-01-01'\n    if type(df) == type(pd.DataFrame()):\n        df['date'] = df['date'].apply(days_from_present)\n    else:\n        df['date'] = days_from_present(df['date'])\n    if type(df) == type(pd.DataFrame()):\n        types = ['Housing / Retail', 'Light Industrial',\n            'Light Industrial / Office', 'Office', 'Retail']\n        for t in types:\n            df[t] = 0\n            df.loc[df['asset_type'] == t, t] = 1\n        df = df.drop('asset_type', axis=1)\n    else:\n        types = ['Housing / Retail', 'Light Industrial',\n            'Light Industrial / Office', 'Office', 'Retail']\n        for t in types:\n            df[t] = 0\n            if df['asset_type'] == t:\n                df[t] = 1\n        df = df.drop(labels=['asset_type'])\n    return df\n", "import_code": ["import pandas as pd\n"]}
{"id": "51bdfcde-d88a-3aeb-bb22-b80d539e9e1f_2", "content": "def read_file(file_name):\n    f = open(file_name)\n    return f.read()\n", "import_code": []}
{"id": "51bdfcde-d88a-3aeb-bb22-b80d539e9e1f_4", "content": "def send_servers(access_token_path, record_id_path, get_gpu_path, table_id,\n    group_name, new_machine=False):\n    url = 'https://cloud.minapp.com/oserve/v2.4/table/'\n    url += str(table_id) + '/record/'\n    if not os.path.isfile(record_id_path) or new_machine:\n        auth(access_token_path)\n    else:\n        url += read_file(record_id_path) + '/'\n    token = read_file(access_token_path)\n    cron_freq = re.findall('\\\\d+', read_file(sys.path[0] + '/cron'))[0]\n    servers = get_gpus(path=get_gpu_path, cron_freq=cron_freq)\n    param = {'servers': servers, 'update_rate': cron_freq, 'group_name':\n        group_name}\n    headers = {'Authorization': 'Bearer ' + token, 'Content-Type':\n        'application/json', 'charset': 'utf-8'}\n    if not os.path.isfile(record_id_path) or new_machine:\n        res = requests.post(url, json=param, headers=headers)\n    else:\n        res = requests.put(url, json=param, headers=headers)\n    return res\n", "import_code": ["import requests\n", "import re\n", "import sys\n", "import sys\n", "import requests\n", "import os, time, math\n"]}
{"id": "51bdfcde-d88a-3aeb-bb22-b80d539e9e1f_5", "content": "def natural_sort(l):\n    convert = lambda text: int(text) if text.isdigit() else text.lower()\n    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n    return sorted(l, key=alphanum_key)\n", "import_code": ["import requests\n", "import re\n"]}
{"id": "51bdfcde-d88a-3aeb-bb22-b80d539e9e1f_6", "content": "def get_gpus(path=os.getcwd() + '/gpustat', cron_freq=3):\n    servers = []\n    files = os.listdir(path)\n    files = natural_sort(files)\n    for file in files:\n        if not os.path.isdir(file):\n            f = open(path + file)\n            modify_time = os.path.getmtime(path + file)\n            current_time = time.time()\n            if modify_time < current_time - 600 or modify_time > current_time:\n                server_name = f.readline()\n                server = {'hostname': server_name.replace('\\n', ''), 'stat':\n                    'error', 'ram_available': '', 'gpus': [], 'update_rate':\n                    cron_freq}\n                servers.append(server)\n            else:\n                server_data = f.read().splitlines()\n                if len(server_data) != 0:\n                    server = {'stat': 'normal', 'hostname': server_data[0],\n                        'ram_available': server_data[1], 'gpus': [gpu.split\n                        (',') for gpu in server_data[2:]], 'update_rate':\n                        cron_freq}\n                    servers.append(server)\n    return servers\n", "import_code": ["import os, time, math\n", "import os, time, math\n"]}
{"id": "cba9a6d4-a7cd-354d-be95-3dc37efa4cfc_0", "content": "def dataframe_difference(df1: pd.DataFrame, df2: pd.DataFrame, merge_on: list\n    ) ->pd.DataFrame:\n    \"\"\"Find rows which are different between two DataFrames.\"\"\"\n    comparison_df = df1.merge(df2, indicator=True, how='outer', on=merge_on,\n        suffixes=('', '_y'))\n    comparison_df = comparison_df[[c for c in comparison_df.columns if not\n        c.endswith('_y')]]\n    return comparison_df[comparison_df['_merge'] == 'left_only'].drop('_merge',\n        1)\n", "import_code": ["import pandas as pd\n"]}
{"id": "63028cc4-c4e3-3e94-89a2-7b2604cd863f_0", "content": "def getStandardizedEDList(dataframe):\n    \"\"\"\n    Calculate the standardized Euclidean distance for each defective sample\n    :param dataframe: All samples (type:dataframe)\n    :return: 1) k: Number of nearest neighbors (type:int)\n             2) Double_List: List of standardized Euclidean distances for each defective sample (42\u00d7302) (type:2D-list)\n             3) df1: Dataframe of defective samples (type:dataframe)\n             4) df2: Dataframe of non-defective samples (type:dataframe)\n    \"\"\"\n    dataframe1 = dataframe.loc[dataframe['Defective'] == b'Y']\n    dataframe2 = dataframe.loc[dataframe['Defective'] == b'N']\n    df1 = dataframe1.drop(columns='Defective')\n    df2 = dataframe2.drop(columns='Defective')\n    k = len(df2) / len(df1)\n    vec1 = np.array(df1)\n    vec2 = np.array(df2)\n    Double_List = [[(0) for col in range(len(vec2))] for row in range(len(\n        vec1))]\n    for i, v1 in enumerate(vec1):\n        for j, v2 in enumerate(vec2):\n            Vec = np.vstack([v1, v2])\n            sed = pdist(Vec, 'seuclidean')\n            Double_List[i][j] = sed\n    return k, Double_List, df1, df2\n", "import_code": ["from scipy.spatial.distance import pdist\n", "import numpy as np\n"]}
{"id": "63028cc4-c4e3-3e94-89a2-7b2604cd863f_1", "content": "def changeToTupleList(_list):\n    \"\"\"\n    Convert a list to a list of tuples with index and value for sorting purposes\n    :param _list: List to be converted (type:list)\n    :return: List of tuples (type:list)\n    \"\"\"\n    for _index, _value in enumerate(_list):\n        index_value = '({0},{1})'.format(_index, _value)\n        _tuple = eval(index_value, globals)\n        _list[_index] = _tuple\n    return _list\n", "import_code": []}
{"id": "63028cc4-c4e3-3e94-89a2-7b2604cd863f_2", "content": "def ruler(elem):\n    return elem[1]\n", "import_code": []}
{"id": "63028cc4-c4e3-3e94-89a2-7b2604cd863f_3", "content": "def getAdjacentSample(k, Standardized_ED_List):\n    \"\"\"\n    Sort the standardized Euclidean distances for each defective sample in ascending order\n    :param k: Number of nearest neighbors (type:int)\n    :param Standardized_ED_List: List of standardized Euclidean distances (42\u00d7302) (type:2D-list)\n    :return: List of sample ids for each defective sample's nearest neighbors (42\u00d7k) (type:2D-list)\n    \"\"\"\n    adjacent_sample = [[(0) for col in range(int(k))] for row in range(len(\n        Standardized_ED_List))]\n    for sample_id, sample_sed_list in enumerate(Standardized_ED_List):\n        sample_sed_list = changeToTupleList(sample_sed_list)\n        sample_sed_list.sort(key=ruler)\n        for col, item in enumerate(sample_sed_list[:int(k)]):\n            adjacent_sample[sample_id][col] = item[0]\n    return adjacent_sample\n", "import_code": []}
{"id": "63028cc4-c4e3-3e94-89a2-7b2604cd863f_4", "content": "def calculateDiff(aslist, dataframe1, dataframe2):\n    \"\"\"\n    Calculate and update feature weights based on the differences between defective samples and their nearest neighbors\n    :param aslist: List of sample ids for each defective sample's nearest neighbors (42\u00d77) (type:list)\n    :param dataframe1: Dataframe containing all defective samples (42\u00d737) (type:dataframe)\n    :param dataframe2: Dataframe containing all non-defective samples (302\u00d737) (type:dataframe)\n    :return: Feature weight list (1\u00d737)\n    \"\"\"\n    DiffWeight = [(0) for row in range(dataframe1.shape[1])]\n    for i in range(dataframe1.shape[0]):\n        sampleY = dataframe1.iloc[i]\n        adjacentlist = aslist[i]\n        for sample_id in adjacentlist:\n            sampleN = dataframe2.iloc[sample_id]\n            delt = sampleY - sampleN\n            delt = np.abs(delt)\n            diffvaluelist = delt.values.tolist()\n            diffvaluelist = changeToTupleList(diffvaluelist)\n            copylist = diffvaluelist.copy()\n            diffvaluelist.sort(key=ruler)\n            for feature_tuple in copylist:\n                _id = feature_tuple[0]\n                _weight = diffvaluelist.index(feature_tuple) + 1\n                DiffWeight[_id] = DiffWeight[_id] + _weight\n    return DiffWeight\n", "import_code": ["import numpy as np\n"]}
{"id": "63028cc4-c4e3-3e94-89a2-7b2604cd863f_5", "content": "def sortFeature(dataframe1, DiffWeight):\n    \"\"\"\n    Sort features based on their weights (descending order)\n    :param dataframe1: Dataframe containing all defective samples (type:dataframe)\n    :param DiffWeight: Feature weight list (1\u00d737) (type:list)\n    :return: Final sorted list of features\n    \"\"\"\n    DiffWeight = changeToTupleList(DiffWeight)\n    DiffWeight.sort(key=ruler, reverse=True)\n    FeatureList = dataframe1.columns.values.tolist()\n    Result = []\n    for _tuple in DiffWeight:\n        Result.append(FeatureList[_tuple[0]])\n    print(DiffWeight)\n    return Result\n", "import_code": []}
{"id": "63028cc4-c4e3-3e94-89a2-7b2604cd863f_6", "content": "def getLog2dfeature(file):\n    return ['HALSTEAD_DIFFICULTY', 'HALSTEAD_LEVEL', 'HALSTEAD_PROG_TIME',\n        'HALSTEAD_ERROR_EST', 'NUM_OPERANDS']\n", "import_code": []}
{"id": "7231c65f-4c8b-32e7-88fd-09a39f6e67f3_0", "content": "def get_letter_count(file_path):\n    \"\"\"\n    This function reads a text file, counts the frequency of each letter, and returns a dictionary with the frequencies.\n    \"\"\"\n    print(f'Starting {file_path}')\n    infile = open(file_path, 'r')\n    freq = {}\n    count = 0\n    for c in ALPHABET:\n        freq[c] = 0\n    for line in infile:\n        for c in line.lower():\n            if c.isalpha():\n                try:\n                    freq[c] = freq[c] + 1\n                    count += 1\n                except KeyError:\n                    continue\n    for i in ALPHABET:\n        freq[i] = freq[i] / count\n    return freq\n", "import_code": []}
{"id": "7231c65f-4c8b-32e7-88fd-09a39f6e67f3_1", "content": "def get_values_out_of_Dictionary(dictionary):\n    \"\"\"\n    This function takes a dictionary and returns a list of the values.\n    \"\"\"\n    temp_list = []\n    for i in ALPHABET:\n        temp_list.append(dictionary[i])\n    return temp_list\n", "import_code": []}
{"id": "cfbf54da-e41c-334f-a976-5d71d6277763_0", "content": "def digit_factorial_sum(n):\n    return sum(digit_factorials[i] for i in str(n))\n", "import_code": []}
{"id": "fd8645a4-f40e-3f77-a390-378c1d333d68_3", "content": "def get_dtype(size):\n    \"\"\"\n    Since an image can contain only black or white pixels, the size of the integer to be used in the histogram is configured according to the height and width of the image.\n\n    :param size: Image size (number of pixels).\n    :return: An unsigned numpy integer type.\n    \"\"\"\n    if size < 256:\n        return np.uint8\n    if size < 65536:\n        return np.uint16\n    if size < 4294967295:\n        return np.uint32\n    return np.uint64\n", "import_code": ["import numpy as np\n"]}
{"id": "fd8645a4-f40e-3f77-a390-378c1d333d68_4", "content": "def create_gray_histogram(image):\n    \"\"\"\n    Creates a numpy array with dimension (1, 256) to store the histogram of a grayscale image. Each array index (0, 1, 2, ..., 255) represents a pixel value and the value of each position in the array represents the frequency of each pixel in the image.\n\n    :param image: Gray scale image.\n    :return: Numpy array with the frequency of the pixel values in a gray scale image.\n    \"\"\"\n    height, width = image.shape\n    size = height * width\n    dtype = get_dtype(size)\n    histogram = np.zeros(shape=(1, 256), dtype=dtype)\n    flatten_image = image.flatten()\n    for index in flatten_image:\n        histogram[0, index] += 1\n    return histogram\n", "import_code": ["import numpy as np\n"]}
{"id": "fd8645a4-f40e-3f77-a390-378c1d333d68_5", "content": "def create_rgb_histogram(image):\n    \"\"\"\n    Creates a numpy array with dimension (n, 256) (n = number of channels of image) to store the histogram of a multi-channel image. Each array index ([0, 0], [1, 0], [2, 0], ..., [255, 0]; [0, 1], [1, 1], [2, 1] , ..., [255, 1]; [0, n-1], [1, n-1], [2, n-1], ..., [255, n-1]) represents a value pixel for a given color channel and the value of each position of the array represents the frequency of each pixel in the image in its respective channel.\n\n    :param image: Multi-channel image.\n    :return: Numpy array with the frequency of the pixel values in a multi-channel image.\n    \"\"\"\n    height, width, channels = image.shape\n    size = height * width\n    dtype = get_dtype(size)\n    histogram = np.zeros(shape=(channels, 256), dtype=dtype)\n    for h in range(height):\n        for w in range(width):\n            for c in range(channels):\n                index = image[h, w, c]\n                histogram[c, index] += 1\n    return histogram\n", "import_code": ["import numpy as np\n"]}
{"id": "fd8645a4-f40e-3f77-a390-378c1d333d68_6", "content": "def calculate_histogram(image):\n    \"\"\"\n    Load the histogram of an image. Since an image can contain more than one channel, the number of channels in the histogram is configured according to the number of channels in the image.\n\n    :param image Loaded image.\n    :return: a numpy array with the image histogram.\n    \"\"\"\n    channels = 1\n    if len(image.shape) != 2:\n        channels = image.shape[2]\n    if channels == 1:\n        histogram = create_gray_histogram(image)\n    else:\n        histogram = create_rgb_histogram(image)\n    return histogram\n", "import_code": []}
{"id": "fd8645a4-f40e-3f77-a390-378c1d333d68_7", "content": "def calculate_pdf(histogram, image_size):\n    \"\"\"\n    Compute the Probability Density Function (PDF) of a histogram with image_size (number of pixels) positions. The value of each position is divided by image_size.\n\n    :param histogram: Histogram of an image.\n    :param image_size: Number of pixels of an image. Can be obtained by multiplying height x width.\n    :return: A numpy array of size image_size with each histogram value divided by number of pixels.\n    \"\"\"\n    histogram = histogram / image_size\n    return histogram\n", "import_code": []}
{"id": "fd8645a4-f40e-3f77-a390-378c1d333d68_8", "content": "def calculate_distance(pdf1, pdf2, channels):\n    \"\"\"\n    Calculates the Euclidean Distance between two PDFs.\n\n    :param pdf1: PDF of input image.\n    :param pdf2: PDF of the image to be compared.\n    :param channels: Number of channels.\n    :return: A float number that represents the calculated distance.\n    \"\"\"\n    distances = np.zeros(shape=(1, 256 * channels))\n    pdf1_flatten = pdf1.flatten()\n    pdf2_flatten = pdf2.flatten()\n    index = 0\n    for p1, p2 in zip(pdf1_flatten, pdf2_flatten):\n        distances[0, index] = (p1 - p2) ** 2\n        index += 1\n    return np.sum(distances)\n", "import_code": ["import numpy as np\n"]}
{"id": "fd8645a4-f40e-3f77-a390-378c1d333d68_9", "content": "def multichannel2gray(image):\n    \"\"\"\n    Convert multi-channel image to a gray scale image using the pixel average according to the number of channels.\n\n    :param image: A multi-channel image.\n    :return: A gray scale image.\n    \"\"\"\n    height, width, channels = image.shape\n    gray_image = np.zeros(shape=(height, width), dtype=np.uint8)\n    for h in range(height):\n        for w in range(width):\n            gray_image[h, w] = int(np.sum(image[h, w]) / channels)\n    return gray_image\n", "import_code": ["import numpy as np\n"]}
{"id": "01a0d0f5-8a83-3b42-a2df-5b1b04f9deb2_0", "content": "def crossjoin(df1, df2):\n    df1['_tmpcol'] = 1\n    df2['_tmpcol'] = 1\n    result_df = df1.merge(df2, on='_tmpcol')\n    result_df.drop(columns=['_tmpcol'])\n    return result_df\n", "import_code": []}
{"id": "cb059587-7497-3707-a301-1db24bc7dfb4_0", "content": "def parse_html(html_location, path):\n    \"\"\"\n    This function parses HTML files to extract car brand information and saves it in CSV format.\n\n    :param html_location: The directory where the HTML files are located.\n    :param path: The path where the CSV files will be saved.\n    :return: True if the parsing and saving is successful.\n    \"\"\"\n    for hfile in os.listdir(html_location):\n        with open(os.path.join(html_location, hfile), 'r') as f:\n            content = f.read()\n        tree = html.fromstring(content)\n        names = tree.xpath(\"//ul[@id='divModels']//strong/text()\")\n        price_xp = tree.xpath(\n            \"//ul[@id='divModels']//div[@class='font20 margin-top15']/text()\")\n        price_unit = [p[40:-14] for p in price_xp]\n        price_list = [float(p.split(' ')[0]) for p in price_unit]\n        unit_list = [p.split(' ')[1] for p in price_unit]\n        brand = tree.xpath(\"//h1[@class='font30 text-black']/text()\")[0\n            ].replace(' Cars', '')\n        df = pd.DataFrame(np.column_stack([names, price_list, unit_list]),\n            columns=['name', 'price', 'unit'])\n        df['price'] = df['price'].apply(pd.to_numeric, errors='ignore')\n        df['brand'] = brand\n        df['Timestamp'] = dt\n        file_name = os.path.join(path, 'car_brands_{}.csv'.format(dt.date()))\n        logging.info('Writing {} data to {}'.format(brand, file_name))\n        df.to_csv(file_name, mode='a', header=False, index=False)\n        to_mongo(df)\n    return True\n", "import_code": ["import numpy as np\n", "import pandas as pd\n", "from lxml import html\n", "import logging\n", "import os\n"]}
{"id": "cb059587-7497-3707-a301-1db24bc7dfb4_1", "content": "def to_mongo(dataframe):\n    \"\"\"\n    This function inserts a pandas DataFrame into MongoDB.\n\n    :param dataframe: The pandas DataFrame to be inserted.\n    :return: True if the insertion is successful.\n    \"\"\"\n    logging.info('Inserting to Mongo Collection: Carwale')\n    db.Carwale.insert_many(dataframe.to_dict('records'))\n    return True\n", "import_code": ["import logging\n"]}
{"id": "5e02ce31-03f5-3c8c-8602-77eb95c281e0_0", "content": "def calculatePolymerLength(polymer):\n    p = re.compile('[A-Z][a-z]|[a-z][A-Z]')\n    while True:\n        matches = list(filter(lambda x: x[0].lower() == x[1].lower(), p.\n            findall(polymer, overlapped=True)))\n        if len(matches) == 0:\n            break\n        index = polymer.find(matches[0])\n        polymer = polymer[:index] + polymer[index + 2:]\n    return len(polymer)\n", "import_code": ["import regex as re\n"]}
{"id": "a4d9c808-3888-3f64-9238-060376ffbcaa_0", "content": "def consecutive_zeros(string):\n    split_string = string.split('1')\n    return max([len(b) for b in split_string if b != ''])\n", "import_code": []}
{"id": "39cedf66-3a56-356d-896e-324a0c350b8c_0", "content": "def insertAtPos(substr, i, c):\n    return substr[:i] + c + substr[i:]\n", "import_code": []}
{"id": "39cedf66-3a56-356d-896e-324a0c350b8c_1", "content": "def computePermutation(string):\n    substrings = ['']\n    for c in string:\n        tmp = []\n        for ss in substrings:\n            for i in xrange(len(ss) + 1):\n                tmp.append(insertAtPos(ss, i, c))\n        substrings[:] = tmp[:]\n    return substrings\n", "import_code": []}
{"id": "ef41d6d6-b86a-31d1-a79f-46f2b5042bc8_0", "content": "def preprocess_text(text):\n    if type(text) == float:\n        print(text)\n    text = text.lower()\n    text = re.sub('([.,!?])', ' \\\\1 ', text)\n    text = re.sub('[^a-zA-Z.,!?]+', ' ', text)\n    return text\n", "import_code": ["import re\n"]}
{"id": "92ef8c27-ca95-3707-bd53-1fadd039bcb0_0", "content": "def unix2date(unix, fmt='%Y-%m-%d %H:%M:%S'):\n    \"\"\"\n    Convert unix epoch time 1562554800 to\n    datetime with format\n    \"\"\"\n    date = datetime.datetime.utcfromtimestamp(unix)\n    return date.strftime(fmt)\n", "import_code": ["import datetime\n"]}
{"id": "92ef8c27-ca95-3707-bd53-1fadd039bcb0_1", "content": "def date2unix(date, fmt='%Y-%m-%d %H:%M:%S'):\n    \"\"\"\n    Convert datetime with format to \n    unix epoch time 1562554800\n    \"\"\"\n    return int(time.mktime(time.strptime(date, fmt)))\n", "import_code": ["import datetime\n", "import time\n"]}
{"id": "92ef8c27-ca95-3707-bd53-1fadd039bcb0_2", "content": "def cc2bt(df):\n    \"\"\"\n    Convert CryptoCompare data to Backtrader data\n    \"\"\"\n    df['datetime'] = df['time'].apply(unix2date)\n    df.drop(columns=['time'], inplace=True)\n    df.rename(columns={'volumefrom': 'volume', 'volumeto': 'baseVolume'},\n        inplace=True)\n    return df\n", "import_code": []}
{"id": "49e5a5ba-739c-3bf6-a488-e57ac51268d5_1", "content": "def stringify(datetime_object: datetime=now()) ->str:\n    if datetime_object.tzinfo == 'US/Eastern':\n        return datetime_object.isoformat()\n    raise ValueError('Invalid Time Zone')\n", "import_code": ["from datetime import date, datetime, timedelta\n"]}
{"id": "49e5a5ba-739c-3bf6-a488-e57ac51268d5_2", "content": "def str2datetime(dateandtime: str) ->datetime:\n    return datetime.strptime(dateandtime, '%Y-%m-%dT%H:%M:%S%z')\n", "import_code": ["from datetime import date, datetime, timedelta\n"]}
{"id": "49e5a5ba-739c-3bf6-a488-e57ac51268d5_3", "content": "def timediff30(diff: timedelta) ->bool:\n    if diff < timedelta(30):\n        return True\n    else:\n        return False\n", "import_code": ["from datetime import date, datetime, timedelta\n"]}
{"id": "49e5a5ba-739c-3bf6-a488-e57ac51268d5_4", "content": "def get_start_month_date(months: int) ->date:\n    currentDate = datetime.today()\n    firstDayOfMonth = date(currentDate.year, currentDate.month - months, 1)\n    return firstDayOfMonth\n", "import_code": ["from datetime import date, datetime, timedelta\n", "from datetime import date, datetime, timedelta\n"]}
{"id": "49e5a5ba-739c-3bf6-a488-e57ac51268d5_5", "content": "def get_end_month_date(months: int) ->date:\n    currentDate = datetime.today()\n    lastDayOfMonth = date(currentDate.year, currentDate.month - months,\n        calendar.monthrange(currentDate.year, currentDate.month - months)[1])\n    return lastDayOfMonth\n", "import_code": ["from datetime import date, datetime, timedelta\n", "import calendar\n", "from datetime import date, datetime, timedelta\n"]}
{"id": "4705ed59-e270-3d96-97c7-693b2e59296e_1", "content": "def my_solution(year):\n    flattened_year = list(all_days(year))\n    weekday_rates = map(lambda x: flattened_year.count(x), xrange(7))\n    most_frequent_weekdays = [day_name[i] for i, d in enumerate(\n        weekday_rates) if d == max(weekday_rates)]\n    return most_frequent_weekdays\n", "import_code": ["from calendar import day_name\n", "from calendar import day_name, isleap, weekday\n"]}
{"id": "0bf6abbd-9d34-3a6a-9cbd-351df18e81f8_4", "content": "def get_factor_values(context, factor_list, universe):\n    \"\"\"\n    Input: Factors, Stock Universe\n    Return: The factor values of the previous day\n    \"\"\"\n    factor_name = list(factor.name for factor in factor_list)\n    values = calc_factors(universe, factor_list, context.previous_date,\n        context.previous_date)\n    factor_dict = {i: values[i].iloc[0] for i in factor_name}\n    return factor_dict\n", "import_code": []}
{"id": "1a11cec7-62ee-3b5b-b565-de8aad4f737d_1", "content": "def test_train_split(df, train_amount):\n    train_size = int(len(df) * train_amount)\n    train, test = df[0:train_size].reset_index(), df[train_size:len(df)\n        ].reset_index()\n    train = train.set_index(['Date'])\n    test = test.set_index(['Date'])\n    return train, test\n", "import_code": []}
{"id": "1a11cec7-62ee-3b5b-b565-de8aad4f737d_2", "content": "def cluster_split_train_and_test(train, test, target):\n    X_train = train.drop(columns=[target])\n    y_train = train[target]\n    X_test = test.drop(columns=[target])\n    y_test = test[target]\n    return X_train, y_train, X_test, y_test\n", "import_code": []}
{"id": "1a11cec7-62ee-3b5b-b565-de8aad4f737d_3", "content": "def time_split(train, test, target):\n    X_train = train.index\n    y_train = trian[target]\n    X_test = test.index\n    y_test = test[target]\n    return X_train, y_train, X_test, y_test\n", "import_code": []}
{"id": "a2005b2b-9646-35bb-a472-a2bb30209f85_1", "content": "def genrate_password(len_password, num_letters, num_numbers):\n    password_characters = generate_string(string.ascii_letters, num_letters\n        ) + generate_string(string.digits, num_numbers) + string.punctuation\n    return password_characters\n", "import_code": ["import string\n"]}
{"id": "a2005b2b-9646-35bb-a472-a2bb30209f85_2", "content": "def generate_string(typechar, length):\n    generated_string = ''\n    for i in range(length):\n        generated_string += random.choice(typechar)\n    return generated_string\n", "import_code": ["import random\n"]}
{"id": "67ef6fd3-2d66-3732-8106-f15ffb1d23e3_0", "content": "def nearest(array, value):\n    array = np.asarray(array)\n    idx = np.abs(array - value).argmin()\n    return array.flat[idx]\n", "import_code": ["import numpy as np\n"]}
{"id": "718c7e64-7381-35cd-8a61-d3a3e101fc97_0", "content": "def remove_accents(input_str):\n    s1 = (\n        u'\ufffd\ufffd\u00c2\u00c3\u00c8\u00c9\u00ca\u00cc\u00cd\u00d2\u00d3\u00d4\u00d5\u00d9\u00da\u00dd\u00e0\u00e1\u00e2\u00e3\u00e8\u00e9\u00ea\u00ec\u00ed\u00f2\u00f3\u00f4\ufffd\ufffd\ufffd\ufffd\u0102\u0103\u0110\u0111\u0128\u0129\u0168\u0169\u01a0\u01a1\u01af\u01b0\u1ea0\u1ea1\u1ea2\u1ea3\u1ea4\u1ea5\u1ea6\u1ea7\u1ea8\u1ea9\u1eaa\u1eab\u1eac\u1ead\u1eae\u1eaf\u1eb0\u1eb1\u1eb2\u1eb3\u1eb4\u1eb5\u1eb6\u1eb7\u1eb8\u1eb9\u1eba\u1ebb\u1ebc\u1ebd\u1ebe\u1ebf\u1ec0\u1ec1\u1ec2\u1ec3\u1ec4\u1ec5\u1ec6\u1ec7\u1ec8\u1ec9\u1eca\u1ecb\u1ecc\u1ecd\u1ece\u1ecf\u1ed0\u1ed1\u1ed2\u1ed3\u1ed4\u1ed5\u1ed6\u1ed7\u1ed8\u1ed9\u1eda\u1edb\u1edc\u1edd\u1ede\u1edf\u1ee0\u1ee1\u1ee2\u1ee3\u1ee4\u1ee5\u1ee6\u1ee7\u1ee8\u1ee9\u1eea\u1eeb\u1eec\u1eed\u1eee\u1eef\u1ef0\u1ef1\u1ef2\u1ef3\u1ef4\u1ef5\u1ef6\u1ef7\u1ef8\u1ef9\u00df\u00f1\u00e4'\n        )\n    s0 = (\n        u'AAAAEEEIIOOOOUUYaaaaeeeiioooouuyAaDdIiUuOoUuAaAaAaAaAaAaAaAaAaAaAaAaEeEeEeEeEeEeEeEeIiIiOoOoOoOoOoOoOoOoOoOoOoOoUuUuUuUuUuUuUuYyYyYyYybna'\n        )\n    s = ''\n    for c in input_str:\n        if c in s1:\n            s += s0[s1.index(c)]\n        elif ord(c) not in range(760, 900) and ord(c) not in range(161, 191):\n            s += c\n    s = re.sub('\\\\s+', ' ', s)\n    return s.strip().lower()\n", "import_code": ["import re, bson\n"]}
{"id": "718c7e64-7381-35cd-8a61-d3a3e101fc97_1", "content": "def clean(inp):\n    res = remove_accents(inp)\n    res = re.sub('[^A-Za-z]+', ' ', res)\n    while re.findall('\\\\s\\\\s', res):\n        res = re.sub('\\\\s\\\\s', '\\\\s', inp)\n    return res.strip()\n", "import_code": ["import re, bson\n"]}
{"id": "718c7e64-7381-35cd-8a61-d3a3e101fc97_2", "content": "def extract(word):\n    ue = '[ueoaiy]'\n    results = []\n    reg = re.findall(ue, word)\n    if len(reg) > 0:\n        start = word.index(reg[0])\n        end = word.index(reg[-1])\n        results.append(word[:start])\n        results.append(word[start:end + 1])\n        results.append(word[end + 1:])\n    return results\n", "import_code": ["import re, bson\n"]}
{"id": "718c7e64-7381-35cd-8a61-d3a3e101fc97_3", "content": "def collectDict(data):\n    dictionary = set()\n    for d in data.split(' '):\n        for e in extract(d):\n            dictionary.add(e)\n    dictionary.remove('')\n    return dictionary\n", "import_code": []}
{"id": "718c7e64-7381-35cd-8a61-d3a3e101fc97_4", "content": "def splitWord(word):\n    ue = 'ueoaiy'\n    key = word[0] in ue\n    results = []\n    for w in word:\n        if (w in ue) ^ key:\n            results[-1] = results[-1] + w\n        else:\n            key = not key\n            results.append(w)\n    return results\n", "import_code": []}
{"id": "718c7e64-7381-35cd-8a61-d3a3e101fc97_5", "content": "def isVN(word, dic=dictionary):\n    sp = splitWord(word)\n    if len(sp) > 3:\n        return False\n    for s in sp:\n        if s not in dic:\n            return False\n    num = 0\n    for s in sp:\n        if len(re.findall('[ueaoiy]', s)) > 0:\n            num += 1\n    if num != 1:\n        return False\n    if len(sp) == 3 and num == 1 and len(re.findall('[ueiaoy]', sp[1])) == 0:\n        return False\n    return True\n", "import_code": ["import re, bson\n"]}
{"id": "2880a62a-3878-3e5a-b6dc-7652ea12d04e_0", "content": "def _get_available_log_files(log_folder, log_level):\n    \"\"\"\n    Filters the list of possible log files by the log filter.\n    :param log_folder: the folder name where the log files are located\n    :param log_level: the log level, one of the _RequestLevels\n    :return: a list of strings representing the file names of log files matching the log level.\n    \"\"\"\n    result = []\n    for _, _, files in os.walk(log_folder):\n        for f in files:\n            if log_level in f and '.log' in f:\n                result.append(f)\n    return result\n", "import_code": ["import os\n"]}
{"id": "2880a62a-3878-3e5a-b6dc-7652ea12d04e_1", "content": "def _find_log_candidates(available_log_files, request_date, log_file_name):\n    \"\"\"\n    Helper function to find log file candidates by checking which files could cover the requested date interval\n    :param available_log_files: a list of available log files of the requested level\n    :param request_date: the date for which the log file is requested\n    :param log_file_name: the name prefix of the log files (e.g. debug.log, which prefixes e.g. debug.log.2019-12-24)\n    :return: a list of log file names that most likely will contain the requested log date\n    \"\"\"\n    parsed_request_date = datetime.strptime(request_date, _DATE_FORMAT)\n    interval = [parsed_request_date.timestamp(), (parsed_request_date +\n        timedelta(days=1)).timestamp()]\n    candidates = []\n    for log_file in available_log_files:\n        suffix = log_file.replace(log_file_name, '')\n        if suffix == '':\n            end_time = datetime.now().timestamp()\n        else:\n            end_time = datetime.strptime(suffix[1:], _DATE_FORMAT).timestamp()\n        if end_time < interval[0]:\n            continue\n        candidates.append({'end_time': end_time, 'file': log_file})\n    if len(candidates) == 0:\n        return [log_file_name]\n    if len(candidates) == 1:\n        return [candidates[0]['file']]\n    sorted_files = list(sorted(candidates, key=lambda x: x['end_time']))\n    result = [sorted_files[0]['file']]\n    last_seen_date = sorted_files[0]['end_time']\n    index = 1\n    while last_seen_date < interval[1]:\n        result.append(sorted_files[index]['file'])\n        last_seen_date = sorted_files[index]['end_time']\n        index += 1\n    return result\n", "import_code": ["from datetime import datetime, timedelta\n", "from datetime import datetime, timedelta\n"]}
{"id": "2880a62a-3878-3e5a-b6dc-7652ea12d04e_2", "content": "def _parse_date(line):\n    \"\"\"\n    Attempts to parse a date out of a log line. The date format to check for is \"%Y-%m-%d %H:%M:%S\". If no date can be\n    parsed, no date and the message is returned. If a date is found, the date and the message (separately) are returned.\n    :param line: a line from a log file\n    :return: a tuple of datetime and string, where the datetime represents the parsed date (or is None) and the string\n    representing the message (anything after the date).\n    \"\"\"\n    if len(line) < 24:\n        return None, line\n    try:\n        sub = line[0:19]\n        message = line[24:]\n        date = datetime.strptime(sub, '%Y-%m-%d %H:%M:%S')\n        return date, message\n    except ValueError:\n        return None, line\n", "import_code": ["from datetime import datetime, timedelta\n"]}
{"id": "2880a62a-3878-3e5a-b6dc-7652ea12d04e_3", "content": "def _get_log_messages(log_folder, candidates, log_date):\n    \"\"\"\n    Naive function to extract sorted log messages from the provided candidate files for the given date. Any log message\n    in between the log date and the day after the log date are captured from the list of candidate files read from the\n    provided log folder.\n    :param log_folder: a string representing the path to the log folder\n    :param candidates: a list of file names in the log folder that are likely to contain matching log messages for the\n    requested date\n    :param log_date: the log date to filter for\n    :return: a list of dictionaries containing \"timestamp\" and \"message\" keys, where the timestamp is a Unix timestamp\n    of the message and the message is a list of strings representing all the lines of the log messages (or just one, if\n    it's a one liner log message)\n    \"\"\"\n    parsed_start = datetime.strptime(log_date, _DATE_FORMAT)\n    parsed_end = parsed_start + timedelta(days=1)\n    result = []\n    for candidate_file in candidates:\n        fr = open(os.path.join(log_folder, candidate_file), 'r')\n        current_entry = {'timestamp': 0, 'message': []}\n        skip_current = False\n        for line in fr:\n            line = line.replace('\\r\\n', '').replace('\\n', '')\n            parse_date, message = _parse_date(line)\n            if parse_date is not None:\n                if parsed_start.timestamp() > parse_date.timestamp(\n                    ) or parse_date.timestamp() > parsed_end.timestamp():\n                    skip_current = True\n                else:\n                    skip_current = False\n            if skip_current:\n                continue\n            if parse_date is None:\n                if current_entry['timestamp'] != 0:\n                    current_entry['message'].append(message)\n            else:\n                if current_entry['timestamp'] != 0:\n                    result.append(current_entry)\n                current_entry = {'timestamp': round(parse_date.timestamp()),\n                    'message': [message]}\n        fr.close()\n        if current_entry['timestamp'] != 0:\n            result.append(current_entry)\n    return list(sorted(result, key=lambda x: x['timestamp']))\n", "import_code": ["from datetime import datetime, timedelta\n", "from datetime import datetime, timedelta\n", "import os\n"]}
{"id": "3ade6b0f-6deb-3f6f-86eb-25349152888e_2", "content": "def generate_lines(n, abuse_ratio=None):\n    fmt = '[{timestamp}] {ip} {endpoint} {time:.2f}'\n    max_seconds = 100\n    log_length = timedelta(days=30)\n    end = datetime.now()\n    begin = end - log_length\n    output = []\n    bad_ip = random_ip()\n    timestamps = [datetime.fromtimestamp(random.uniform(end.timestamp(),\n        begin.timestamp())) for _ in range(n)]\n    for timestamp in sorted(timestamps):\n        endpoint = get_endpoint()\n        output.append(fmt.format(timestamp=timestamp, ip=random.choice((\n            bad_ip,) + (random_ip(),) * (abuse_ratio - 1)) if abuse_ratio else\n            random_ip(), endpoint=endpoint.url, time=gamma.rvs(endpoint.\n            gamma_shape, scale=max_seconds / 10)))\n    return '\\n'.join(output)\n", "import_code": ["from datetime import datetime, timedelta\n", "from datetime import datetime, timedelta\n", "from scipy.stats import gamma\n", "import random\n"]}
{"id": "dd33e5cd-0b00-3c46-84c1-1654b4312566_0", "content": "def within_schedule(utc, *timezones):\n    \"\"\"\n    Receive a utc datetime and one or more timezones and check if\n    they are all within schedule (MEETING_HOURS)\n    \"\"\"\n    if not all(tz in TIMEZONES for tz in timezones):\n        raise ValueError\n    utc = pytz.timezone('UTC').localize(utc)\n    if all(utc.astimezone(pytz.timezone(time)).hour in MEETING_HOURS for\n        time in timezones):\n        return True\n    else:\n        return False\n", "import_code": ["import pytz\n"]}
{"id": "47f41302-9185-3ca9-a599-eab55943cd60_3", "content": "def process_input(data):\n    \"\"\"\n    :param data: a DataFrame where first column contains date information, second contains response\n    \"\"\"\n    _validate_format(data)\n    data = data.drop_duplicates()\n    data = data.dropna()\n    data = data.reset_index(drop=True)\n    data.columns = ['datetime', 'y']\n    if data['datetime'].dtype == np.int64:\n        data['datetime'] = data['datetime'].astype(str)\n    data['datetime'] = pd.to_datetime(data['datetime'])\n    _validate_dates(data)\n    _validate_response(data)\n    data['datetime'] = data['datetime'].dt.date\n    return data\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "cb85bb89-e514-30ae-93a5-ea3ded049204_0", "content": "def to_excel(df):\n    output = BytesIO()\n    writer = pd.ExcelWriter(output, engine='xlsxwriter')\n    df.to_excel(writer, sheet_name='Sheet1')\n    writer.save()\n    processed_data = output.getvalue()\n    return processed_data\n", "import_code": ["import pandas as pd\n"]}
{"id": "cb85bb89-e514-30ae-93a5-ea3ded049204_1", "content": "def get_table_download_link(df):\n    \"\"\"Generates a link allowing the data in a given panda dataframe to be downloaded\n    in:  dataframe\n    out: href string\n    \"\"\"\n    val = to_excel(df)\n    b64 = base64.b64encode(val)\n    return (\n        f'<a href=\"data:application/octet-stream;base64,{b64.decode()}\" download=\"download.xlsx\">Download excel file</a>'\n        )\n", "import_code": []}
{"id": "84650965-1a32-330f-8587-8225c9d16469_0", "content": "def _pkcs7padding(data):\n    \"\"\"\n    Pad the data to align it with AES block size.\n    This is necessary for AES encryption.\n    \"\"\"\n    size = AES.block_size\n    count = size - len(data) % size\n    if count:\n        data += chr(count) * count\n    return data\n", "import_code": ["from Crypto.Cipher import AES\n"]}
{"id": "a03561b1-e06d-3b13-8600-edf752efdba1_2", "content": "def bencode_file2string(in_filename):\n    be = bEncoder()\n    return be.Encode_File2String(in_filename)\n", "import_code": []}
{"id": "a03561b1-e06d-3b13-8600-edf752efdba1_3", "content": "def bencode_file2string_with_size(in_filename):\n    be = bEncoder()\n    en = be.Encode_File2String(in_filename)\n    sz = be.last_encode_size\n    return en, sz\n", "import_code": []}
{"id": "a03561b1-e06d-3b13-8600-edf752efdba1_5", "content": "def bdecode_file2string(in_filename):\n    be = bDecoder()\n    return be.Decode_File2String(in_filename)\n", "import_code": []}
{"id": "a7069d0d-12aa-3e31-8cd8-be53d6b5280d_0", "content": "def Sin2(angle, mag, xoffset, yoffset):\n    return mag * np.sin(angle * 2 * np.pi / 360 - xoffset) ** 2 + yoffset\n", "import_code": ["import numpy as np\n"]}
{"id": "a7069d0d-12aa-3e31-8cd8-be53d6b5280d_1", "content": "def PowerFit(datax, datay):\n    popt, pcov = curve_fit(Sin2, datax, datay, p0=[1, 1, 1], bounds=([0, 0,\n        0], [1000, 10, 10]))\n    return popt, pcov\n", "import_code": ["from scipy.optimize import curve_fit\n"]}
{"id": "a7069d0d-12aa-3e31-8cd8-be53d6b5280d_2", "content": "def InvSinSqr(y, mag, xoffset, yoffset):\n    return np.mod(360 / (2 * np.pi) * (np.arcsin(np.sqrt(np.abs((y -\n        yoffset) / mag))) + xoffset), 180)\n", "import_code": ["import numpy as np\n"]}
{"id": "a7069d0d-12aa-3e31-8cd8-be53d6b5280d_3", "content": "def PCFit(file):\n    pc = np.load(file, allow_pickle=True)\n    wavelengths = pc[:, 0]\n    PC = []\n    PCcov = []\n    Angles = []\n    xx = np.arange(2, 21, 1)\n    XX = np.linspace(0, 30, 100)\n    for i in range(0, len(pc), 1):\n        params, cov = PowerFit(pc[i, 1][0], pc[i, 1][1])\n        PC.append(params)\n        PCcov.append(cov)\n        analyticsin = InvSinSqr(XX, *params)\n        interpangles = interp1d(XX, analyticsin)\n        angles = interpangles(xx)\n        Angs = dict(zip(xx, angles))\n        Angles.append(Angs)\n    PC = np.asarray(PC)\n    PCcov = np.asarray(PCcov)\n    WavPowAng = dict(zip(wavelengths, Angles))\n    return PC, PCcov, WavPowAng, pc\n", "import_code": ["import numpy as np\n", "from scipy.interpolate import interp1d\n"]}
{"id": "6e53b3b2-cd21-3f62-9d57-0c26f221582f_1", "content": "def read_dfs_report(file_path):\n    report = None\n    with open(file_path) as f:\n        i = 0\n        for line in f:\n            line = line.strip('\\n')\n            if i == 0:\n                report = pd.DataFrame(columns=line.split(','))\n            else:\n                cur_val = ''\n                vals = []\n                for val_ in line.split(','):\n                    if '[' not in val_ and ']' not in val_ and cur_val == '':\n                        vals.append(float(val_))\n                    elif '[' in val_ and ']' in val_:\n                        vals.append([float(re.sub('\\\\[|\\\\]', '', val_))])\n                    elif '[' in val_:\n                        cur_val += val_[1:]\n                    elif ']' in val_:\n                        cur_val += val_[:-1]\n                        cur_val = [float(x) for x in cur_val.split()]\n                        vals.append(cur_val)\n                        cur_val = ''\n                report.loc[i - 1] = vals\n            i += 1\n    return report\n", "import_code": ["import pandas as pd\n", "import re\n"]}
{"id": "6e53b3b2-cd21-3f62-9d57-0c26f221582f_2", "content": "def read_insilico_data(file_path, shuffle=True):\n    df = pd.read_csv(file_path)\n    if shuffle:\n        df = df.sample(frac=1).reset_index(drop=True)\n    x = df[df.columns[df.columns != 'y']].values\n    y = df['y'].values.astype(float).reshape(len(x), 1)\n    return x, y\n", "import_code": ["import pandas as pd\n"]}
{"id": "6e53b3b2-cd21-3f62-9d57-0c26f221582f_3", "content": "def read_json(file_path):\n    with open(file_path) as f:\n        return json.load(f)\n", "import_code": ["import json\n"]}
{"id": "2b1b996f-f6c3-3c1d-b263-77613a552ca0_0", "content": "def luhn_check_dig(num):\n    \"\"\"\n    Function to compute the Luhn Algorithms check sum digit for a given card number\n    Parameters:\n                num (int): A randomly generated 15-digit number\n        Returns:\n                check_sim (int): The associated check sum digit for the given random number\n    \"\"\"\n    str1 = str(num)[::-1][0::2]\n    str2 = str(num)[::-1][1::2]\n    doubles = [(2 * int(x)) for x in str2]\n    res = sum([sum([int(a) for a in str(x)]) for x in doubles]) + sum([int(\n        x) for x in str1])\n    check_sum = 10 - res % 10\n    return check_sum\n", "import_code": []}
{"id": "2b1b996f-f6c3-3c1d-b263-77613a552ca0_2", "content": "def random_date(start, end):\n    \"\"\"\n    Function to generate a random date between two given values\n        Parameters:\n                start (datetime object): start of the given range\n                end (datetime object): end of the given range\n        Returns:\n                start + timedelta(seconds=random_second) (datetime object): A random time within a range\n    \"\"\"\n    delta = end - start\n    int_delta = delta.days * 24 * 60 * 60 + delta.seconds\n    random_second = random.randrange(int_delta)\n    return start + datetime.timedelta(seconds=random_second)\n", "import_code": ["import datetime\n", "import random\n"]}
{"id": "2b1b996f-f6c3-3c1d-b263-77613a552ca0_3", "content": "def generate_trans(card_number, range):\n    \"\"\"\n    Function to generate a semi-realistic distribution of transactions\n        Parameters:\n                card_number (int): A valid card number in plain text format\n                range (tuple): The space in time over which transaction take place\n        Returns:\n                hashed_card.hexdigest() (str): hexadecimal encoding of the md5 hashed card number\n                timestamp (datetime object): Time the transaction occurred\n                amt (float): Amount spent\n    \"\"\"\n    timestamp = random_date(range[0], range[1]).strftime('%Y-%m-%dT%H:%M:%S')\n    mu, sigma = 0, 100.0\n    amt = np.abs(np.random.normal(mu, sigma, 1))[0]\n    hashed_card = hashlib.md5(str(card_number).encode())\n    return hashed_card.hexdigest(), timestamp, amt\n", "import_code": ["import hashlib\n", "import numpy as np\n"]}
{"id": "2b1b996f-f6c3-3c1d-b263-77613a552ca0_4", "content": "def flag_fraudulent_activity(transactions, range, threshold):\n    \"\"\"\n    Function to flag potentially fraudulent transaction activity\n        Parameters:\n                transactions (dict): A dictionary of transaction data\n                    key:\n                        - hashed card number\n                    values:\n                        - transaction timestamp\n                        - transaction amount\n                range (tuple): The space in time over which transaction take place\n                threshold (int): The daily spending threshold above which activity is flagged as fraudulent\n\n        Returns:\n                flagged_cards (list): A list of the card hashes which have been identified as fraudulent\n    \"\"\"\n    fraudulent_cards = []\n    window_centre = range[0]\n    while window_centre < range[1]:\n        window_centre += datetime.timedelta(seconds=3600 * 6)\n        window_start = window_centre + datetime.timedelta(seconds=-3600 * 12)\n        window_end = window_centre + datetime.timedelta(seconds=3600 * 12)\n        for key, value in transactions.items():\n            amt_per_day = 0\n            for transaction in value:\n                time = datetime.datetime.strptime(transaction[0],\n                    '%Y-%m-%dT%H:%M:%S')\n                if window_start <= time <= window_end:\n                    amt_per_day += transaction[1]\n            if amt_per_day > threshold:\n                print('Potential fraudulent activity flagged.')\n                print(f'Card hash {key} \\n in 24 hours around {window_centre}')\n                print(\n                    f'total spend on this day by card {key}: {amt_per_day} \\n')\n                fraudulent_cards.append(key)\n    unique_hashes = list(set(fraudulent_cards))\n    return unique_hashes\n", "import_code": ["import datetime\n", "import time\n", "import datetime\n"]}
{"id": "aa83c99e-3028-387d-b382-469501a8aa2c_0", "content": "def merge_dfs_on_column(dataframes, labels, col):\n    \"\"\"Merge a single column of each dataframe into a new combined dataframe\"\"\"\n    series_dict = {}\n    for index in range(len(dataframes)):\n        series_dict[labels[index]] = dataframes[index][col]\n    return pd.DataFrame(series_dict)\n", "import_code": ["import pandas as pd\n", "from pandas import Series, DataFrame\n"]}
{"id": "9281d458-992d-36d0-8552-60fc94146460_1", "content": "def encrypt_val(passw):\n    SALT = 'vosvjsdabp0909!!'\n    enc_secret = AES.new(SALT[:32])\n    paddout = str(passw) + (AES.block_size - len(str(passw)) % AES.block_size\n        ) * '\\x00'\n    secret_squirrel = base64.b64encode(enc_secret.encrypt(paddout))\n    print('%s encrypted is %s ' % (passw, secret_squirrel))\n    return secret_squirrel\n", "import_code": ["import base64\n", "from Crypto.Cipher import AES\n"]}
{"id": "ee6b4895-f910-34a5-9afa-3d2ba2bf7db2_0", "content": "def rand_number_n_digits(num_digits: int):\n    return random.randrange(10 ** (num_digits - 1), 10 ** num_digits - 1)\n", "import_code": ["import random\n"]}
{"id": "ee6b4895-f910-34a5-9afa-3d2ba2bf7db2_1", "content": "def get_random_string(min_length: int=5, max_length: int=10):\n    length = random.randrange(min_length, max_length)\n    letters = string.ascii_lowercase\n    return ''.join(random.choice(letters) for i in range(length))\n", "import_code": ["import string\n", "import random\n"]}
{"id": "d10354e4-5c10-3009-abb0-0984dfac6880_2", "content": "def create_grid_dict(grid_info: dict):\n    min_long, max_long = grid_info['min_long'], grid_info['max_long']\n    min_lat, max_lat = grid_info['min_lat'], grid_info['max_lat']\n    distance = grid_info['distance']\n    lats = math.ceil((max_lat - min_lat) / distance)\n    longs = math.ceil((max_long - min_long) / distance)\n    d = {}\n    for i in range(lats):\n        for j in range(longs):\n            d[i, j] = []\n    return d\n", "import_code": ["import math\n"]}
{"id": "d10354e4-5c10-3009-abb0-0984dfac6880_3", "content": "def normalise_lat_longs(lat_longs: [(float, float)]) ->[(int, int)]:\n    return list(map(lambda x: (x[0] + 90, x[1] + 180), lat_longs))\n", "import_code": []}
{"id": "d10354e4-5c10-3009-abb0-0984dfac6880_4", "content": "def get_filled_grid_and_filled_squares(grid: dict, grid_info: dict) ->(dict,\n    set):\n    min_long, min_lat = grid_info['min_long'], grid_info['min_lat']\n    distance = grid_info['distance']\n    filled_squares = set()\n    for lat, long in grid_info['lat_longs']:\n        grid_lat = math.floor((lat - min_lat) / distance)\n        grid_long = math.floor((long - min_long) / distance)\n        if (grid_lat, grid_long) in grid:\n            grid[grid_lat, grid_long].append((lat, long))\n            filled_squares.add((grid_lat, grid_long))\n    return grid, filled_squares\n", "import_code": ["import math\n"]}
{"id": "d10354e4-5c10-3009-abb0-0984dfac6880_6", "content": "def create_grid_info_dict(lat_longs: [(float, float)], distance: float):\n    lat, long = zip(*lat_longs)\n    min_long, max_long = min(long), max(long)\n    min_lat, max_lat = min(lat), max(lat)\n    return {'lat_longs': lat_longs, 'min_long': min_long, 'max_long':\n        max_long, 'min_lat': min_lat, 'max_lat': max_lat, 'distance': distance}\n", "import_code": []}
{"id": "7d0d47b5-e7dd-349b-ae06-661afc346135_0", "content": "def requestWikiCategory(categoryName, params=WIKI_CATEGORY_PARAMS):\n    params['cmtitle'] = categoryName\n    categoryMembers = []\n    for request in queryCategory(params):\n        categoryMembers.extend(request['categorymembers'])\n        time.sleep(1)\n    return categoryMembers\n", "import_code": ["import requests, re, time, pickle\n"]}
{"id": "7d0d47b5-e7dd-349b-ae06-661afc346135_2", "content": "def requestFromWiki(request_params, api=WIKIPEDIA_API, base_params=\n    WIKI_REQUEST_PARAMS, header=WIKI_HEADER):\n    request = baseRequestParams\n    if isinstance(pageid, list):\n        request['pageids'] = '|'.join(pageid)\n    elif pagetitle is not None:\n        request['pageids'] = pageid\n    if isinstance(pagetitle, list):\n        request['titles'] = '|'.join(pagetitle)\n    elif pagetitle is not None:\n        request['titles'] = pagetitle\n    if pageid is not None and pagetitle is not None:\n        del request['titles']\n    header = {'User-Agent': userAgent}\n    wikiQuery = requests.get(wikiAPI, params=request, headers=header)\n    if wikiQuery.status_code != requests.codes.OK:\n        raise requests.HTTPError('request for %s caused HTTP status %s: %s' %\n            (pageid, wikiQuery.status, wikiQuery.reason))\n    return wikiQuery.json()\n", "import_code": ["import requests, re, time, pickle\n"]}
{"id": "7d0d47b5-e7dd-349b-ae06-661afc346135_3", "content": "def queryToXML(wikiPage):\n    xmlPages = {}\n    queryPageRoot = wikiPage['query']['pages']\n    for pageID in queryPageRoot:\n        pageTitle = queryPageRoot[pageID]['title']\n        pageXML = ET.fromstring(queryPageRoot[pageID]['revisions'][0][\n            'parsetree'].encode('utf8'))\n        xmlPages[pageTitle] = pageXML\n    return xmlPages\n", "import_code": ["import xml.etree.ElementTree as ET\n"]}
{"id": "7d0d47b5-e7dd-349b-ae06-661afc346135_4", "content": "def xmlToJSON(wikiXML):\n    jsonPages = {}\n    for wikiPageTitle in wikiXML:\n        pageJSON = parsePage(wikiXML[wikiPageTitle])\n        jsonPages[wikiPageTitle] = pageJSON\n    return jsonPages\n", "import_code": []}
{"id": "7d0d47b5-e7dd-349b-ae06-661afc346135_5", "content": "def parsePage(pageRoot):\n    pageDict = {}\n    pageDict['#parentDict'] = pageDict\n    dictDepth = 2\n    currentDict = pageDict\n    headers = [child for child in pageRoot.getchildren() if child.tag == 'h']\n    for header in headers:\n        headerDepth = int(header.attrib['level'])\n        headerName = header.text.strip('=')\n        while headerDepth <= dictDepth:\n            currentDict = currentDict['#parentDict']\n            dictDepth -= 1\n        if headerDepth > dictDepth:\n            dictDepth += 1\n            subDict = {}\n            subDict['#parentDict'] = currentDict\n            subDict['#text'] = header.tail\n            currentDict['%s' % headerName] = subDict\n            currentDict = subDict\n    pageDict = removeParentPointers(pageDict)\n    return pageDict\n", "import_code": []}
{"id": "7d0d47b5-e7dd-349b-ae06-661afc346135_6", "content": "def removeParentPointers(pageDict):\n    try:\n        hereKeys = pageDict.keys()\n    except:\n        return pageDict\n    keysToEmpty = [key for key in hereKeys if not key.startswith('#')]\n    for key in keysToEmpty:\n        pageDict[key] = removeParentPointers(pageDict[key])\n    del pageDict['#parentDict']\n    return pageDict\n", "import_code": []}
{"id": "7d0d47b5-e7dd-349b-ae06-661afc346135_7", "content": "def outputXMLandJSON(pageid, pagetitle):\n    wikiQuery = requestFromWiki(pageid, pagetitle)\n    queryXML = queryToXML(wikiQuery)\n    queryJSON = xmlToJSON(queryXML)\n    return queryXML, queryJSON\n", "import_code": []}
{"id": "ea8814e5-4e06-3341-b389-9504d98f0f56_0", "content": "def jaccard_similarity_small_data(movie_genres):\n    \"\"\"Calculate Jaccard similarity score for a small data one-hot encoded table.\n    Useful to test the multi-processing function for big data that follows.\n\n    Args:\n        movie_genres (pd.DataFrame): One hot-encoded dataframe with rows: movies, binary columns: genres\n\n    Returns:\n        pd.DataFrame: Squareform matrix with rows: movies, colums: movies, values: jaccard similarities\n    \"\"\"\n    assert movie_genres.shape[0] <= 1000\n    jaccard_distances = pdist(movie_genres.values, metric='jaccard')\n    jaccard_similarity_array = 1 - squareform(jaccard_distances)\n    jaccard_similarity_df = pd.DataFrame(jaccard_similarity_array, index=\n        movie_genres.index, columns=movie_genres.index)\n    print(jaccard_similarity_df.head())\n    return jaccard_similarity_df\n", "import_code": ["from scipy.spatial.distance import pdist, squareform\n", "import pandas as pd\n", "from scipy.spatial.distance import pdist, squareform\n", "from scipy.spatial.distance import pdist, squareform\n"]}
{"id": "ea8814e5-4e06-3341-b389-9504d98f0f56_1", "content": "def jaccard_similarity_score_big_data(movie_genres):\n    movie_genres = movie_genres.set_index('title')\n    jaccard_similarity_df = 1 - pairwise_distances(movie_genres, metric=\n        'hamming')\n    jaccard_similarity_df = pd.DataFrame(jaccard_similarity_df, index=\n        movie_genres.index, columns=movie_genres.index)\n    return jaccard_similarity_df\n", "import_code": ["from scipy.spatial.distance import pdist, squareform\n", "import pandas as pd\n", "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity\n"]}
{"id": "ea8814e5-4e06-3341-b389-9504d98f0f56_3", "content": "def calc_cosine_similarity(embeddings):\n    assert embeddings.shape[0] <= 1000\n    cosine_similarity_array = cosine_similarity(embeddings)\n    cosine_similarity_df = pd.DataFrame(cosine_similarity_array, index=\n        embeddings.index, columns=embeddings.index)\n    return cosine_similarity_df\n", "import_code": ["from scipy.spatial.distance import pdist, squareform\n", "import pandas as pd\n", "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity\n"]}
{"id": "ea8814e5-4e06-3341-b389-9504d98f0f56_4", "content": "def cosine_similarity_user_prof(tfidf_movie_plots, user_prof, movies_enjoyed):\n    tfidf_subset_df = tfidf_movie_plots.drop(movies_enjoyed, axis=0)\n    similarity_array = cosine_similarity(user_prof.values.reshape(1, -1),\n        tfidf_subset_df)\n    similarity_df = pd.DataFrame(similarity_array.T, index=tfidf_subset_df.\n        index, columns=['similarity_score'])\n    return similarity_df\n", "import_code": ["from scipy.spatial.distance import pdist, squareform\n", "import pandas as pd\n", "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity\n"]}
{"id": "f2099312-7e3a-34ba-9e8b-87de92bea5fb_0", "content": "def fit_linreg(x: np.ndarray, y: np.ndarray):\n    \"\"\"\n    Fit the first-order regression to the data.\n    Returns m, b, yhat, R2.\n    \"\"\"\n    m, b = np.polyfit(x, y, 1)\n    yhat = m * x + b\n    ei = y - yhat\n    ybar = np.mean(y)\n    SSR = sum((yhat - ybar) ** 2)\n    SSE = sum(ei ** 2)\n    SST = SSR + SSE\n    R2 = SSR / SST\n    return m, b, yhat, R2\n", "import_code": ["import numpy as np\n"]}
{"id": "a95ae4e1-767a-3db2-8ef5-cf5c6aba144f_0", "content": "def f2cat(filename: str) ->str:\n    return filename.split('.')[0]\n", "import_code": []}
{"id": "dfc7307c-482c-3df2-83ce-a8300cb23207_0", "content": "def _display_period_parser(obj):\n    logger = logging.getLogger('display_period_parser')\n    required_fields = ['start', 'end', 'time_zone']\n    if not all([(k in obj.keys()) for k in required_fields]):\n        logger.error('Missing required fields [%s] in [%s]',\n            required_fields, obj)\n        return False, None\n    time_zone = obj['time_zone']\n    if not time_zone in pytz.all_timezones:\n        logger.error('Unknown timezone: [%s].', time_zone)\n        return False, None\n    start_time = convert_to_timestamp(obj['start'], time_zone)\n    if start_time is None:\n        logger.error('Failed to parse time: [%s %s]', obj['start'], time_zone)\n        return False, None\n    end_time = convert_to_timestamp(obj['end'], time_zone)\n    if end_time is None:\n        logging.error('Failed to parse end time: [%s %s]', obj['end'])\n        return False, None\n    return True, {'start_time': start_time, 'end_time': end_time}\n", "import_code": ["import pytz\n", "import logging\n"]}
{"id": "dfc7307c-482c-3df2-83ce-a8300cb23207_1", "content": "def convert_to_timestamp(time_str, time_zone):\n    \"\"\"\n    Converts the given string representation to number of seconds since the Epoch\n    :param time_str: The time string in the format 'YYYYMMDD:HH:MM:SS'\n    :param time_zone: The time zone to use for the datetime object\n    :return: The number of seconds since the EPOCH or None if the time string could not be parsed\n    \"\"\"\n    try:\n        dt = datetime.strptime(time_str, '%Y%m%d:%H:%M:%S')\n        tz = pytz.timezone(time_zone)\n        dt_with_tz = tz.localize(dt)\n        ts = int((dt_with_tz - EPOCH).total_seconds())\n        return ts\n    except ValueError:\n        return None\n", "import_code": ["import pytz\n", "from datetime import datetime\n"]}
{"id": "67356396-c03a-3d50-86e3-bc27b3f768ec_0", "content": "def sorted_str(params, key, null=False):\n    \"\"\"\n    This function sorts the keys in the params dictionary and joins them into a string.\n    If null is True, it includes keys with empty values. Otherwise, it excludes them.\n    \"\"\"\n    if null:\n        s = '&'.join(str(k) + '=' + str(params[k]) for k in sorted(params))\n    else:\n        s = '&'.join(str(k) + '=' + str(params[k]) for k in sorted(params) if\n            params[k])\n    return s + '&key={}'.format(key)\n", "import_code": []}
{"id": "67356396-c03a-3d50-86e3-bc27b3f768ec_1", "content": "def sign_md5(s, upper=True):\n    \"\"\"\n    This function computes the MD5 hash of the input string.\n    If upper is True, it returns the hash in uppercase. Otherwise, it returns it in lowercase.\n    \"\"\"\n    if upper:\n        return hashlib.md5(s.encode('utf-8')).hexdigest().upper()\n    else:\n        return hashlib.md5(s.encode('utf-8')).hexdigest()\n", "import_code": ["import hashlib\n"]}
{"id": "67356396-c03a-3d50-86e3-bc27b3f768ec_2", "content": "def to_xml(params, cdata=True, encoding='utf-8'):\n    \"\"\"\n    This function converts a dictionary into an XML string.\n    If cdata is True, it wraps the values in <![CDATA[ and ]]> tags.\n    \"\"\"\n    tag = '<{0}><![CDATA[{1}]]></{0}>' if cdata else '<{0}>{1}</{0}>'\n    s = ''.join(tag.format(k, v) for k, v in params.items())\n    return '<xml>{}</xml>'.format(s).encode(encoding)\n", "import_code": []}
{"id": "67356396-c03a-3d50-86e3-bc27b3f768ec_3", "content": "def to_dict(content):\n    \"\"\"\n    This function converts an XML string into a dictionary.\n    \"\"\"\n    data = xmltodict.parse(content).get('xml')\n    if '#text' in data:\n        del data['#text']\n    return data\n", "import_code": ["import xmltodict\n"]}
{"id": "67356396-c03a-3d50-86e3-bc27b3f768ec_4", "content": "def random_str(length, upper=True):\n    \"\"\"\n    This function generates a random string of the specified length.\n    If upper is True, it returns the string in uppercase. Otherwise, it returns it in lowercase.\n    \"\"\"\n    sample = 'abcdefghijklmnopqrstuvwxyz'\n    sample += sample.upper()\n    sample += '1234567890'\n    result = ''.join(random.sample(sample, length))\n    return result.upper() if upper else result\n", "import_code": ["import random\n"]}
{"id": "4651a25e-eabb-3746-9528-16faae61a0da_1", "content": "def mse(ys, ys_predicted):\n    \"\"\"Calculate the mean squared error.\"\"\"\n    sum_ = 0.0\n    for y, pred in zip(ys, ys_predicted):\n        sum_ += (y - pred) ** 2\n    return sum_ / float(len(ys))\n", "import_code": []}
{"id": "4651a25e-eabb-3746-9528-16faae61a0da_2", "content": "def last_diff(ys, ys_predicted):\n    \"\"\"Return the difference of the last element and the prediction of it.\"\"\"\n    return ys[-1] - ys_predicted[-1]\n", "import_code": []}
{"id": "4651a25e-eabb-3746-9528-16faae61a0da_3", "content": "def _get_nonexistant_path(csv_filepath):\n    csv_filepath_abs = os.path.abspath(csv_filepath)\n    directory = os.path.dirname(csv_filepath_abs)\n    filename_ext = os.path.basename(csv_filepath_abs)\n    filename = os.path.splitext(filename_ext)[0]\n    i = 1\n    new_name = os.path.join(directory, '%s-%i.csv' % (filename, i))\n    while os.path.isfile(new_name):\n        i += 1\n        new_name = os.path.join(directory, '%s-%i.csv' % (filename, i))\n    return new_name\n", "import_code": ["import os\n"]}
{"id": "4651a25e-eabb-3746-9528-16faae61a0da_4", "content": "def _extract(csv_filepath):\n    \"\"\"Extract all normalized curves from csv.\"\"\"\n    curve_train = []\n    curve_test = []\n    with open(csv_filepath, 'r') as fp:\n        spamreader = csv.reader(fp, delimiter=';', quotechar='\"')\n        for epoch, train, test in spamreader:\n            curve_train.append((epoch, train))\n            curve_test.append((epoch, test))\n    for data in [curve_train, curve_test]:\n        filename = _get_nonexistant_path(csv_filepath)\n        with open(filename, 'w') as fp:\n            a = csv.writer(fp, delimiter=',')\n            data = [('epoch', 'accuracy')] + data\n            a.writerows(data)\n    return [curve_train, curve_test]\n", "import_code": ["import csv\n"]}
{"id": "e07bff85-e663-3e7e-9a88-9719fbf3a8fa_7", "content": "def compare_data(data0, data1):\n    \"\"\"\n    This function compares two data dictionaries and prints the differences.\n    \"\"\"\n    kong = []\n    booL = []\n    num = []\n    typo = []\n    for key in data1:\n        try:\n            compare_kong(data0, data1, kong, key)\n            compare_bool(data0, data1, booL, key)\n            compare_num(data0, data1, num, key)\n            compare_typo(data0, data1, typo, key)\n        except KeyError:\n            pass\n        compare_plus(data0, data1, key)\n    for key in data0:\n        compare_less(data0, data1, key)\n    if len(kong) > 0:\n        print('\u4e0d\u76f8\u540c\u7684\u7a7a\u5b57\u6bb5:' + str(kong))\n    if len(booL) > 0:\n        print('\u4e0d\u76f8\u540c\u7684\u5e03\u5c14\u578b\u5b57\u6bb5:' + str(booL))\n    if len(num) > 0:\n        print('\u6240\u6709\u4e0d\u76f8\u540c\u7684\u5b57\u6bb5:' + str(num))\n    if len(typo) > 0:\n        print('\u7c7b\u578b\u4e0d\u540c\u7684\u5b57\u6bb5:' + str(typo))\n    L = kong + booL + num + typo\n    return L\n", "import_code": []}
{"id": "280ea57c-2f95-3e3e-98e0-f084322e984e_0", "content": "def studnet_ID(x, y):\n    for z in range(len(y)):\n        if x == y[z][1]:\n            return True\n    return False\n", "import_code": []}
{"id": "280ea57c-2f95-3e3e-98e0-f084322e984e_1", "content": "def class_avg(x):\n    average = 0\n    for i in range(len(x)):\n        for number in range(2, len(x[i])):\n            average += x[i][number]\n    average = average / len(x)\n    return average\n", "import_code": []}
{"id": "280ea57c-2f95-3e3e-98e0-f084322e984e_2", "content": "def letter_grade(z):\n    if z >= 87:\n        return 'A'\n    elif 75 <= z <= 86:\n        return 'B'\n    elif 65 <= z <= 74:\n        return 'C'\n    else:\n        return 'F'\n", "import_code": []}
{"id": "280ea57c-2f95-3e3e-98e0-f084322e984e_3", "content": "def lo_row(x, y):\n    for z in range(len(y)):\n        if x == y[z][1]:\n            return z\n", "import_code": []}
{"id": "0b66ce48-7799-3562-85bd-180c22ba6837_1", "content": "def convert_base(num, to_base=10, from_base=10):\n    if isinstance(num, str):\n        n = int(num, from_base)\n    else:\n        n = int(num)\n    alphabet = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if n < to_base:\n        return alphabet[n]\n    else:\n        return convert_base(n // to_base, to_base) + alphabet[n % to_base]\n", "import_code": []}
{"id": "0b66ce48-7799-3562-85bd-180c22ba6837_2", "content": "def _len(num):\n    return len(str(num))\n", "import_code": []}
{"id": "8fc9a6ce-ca1d-3a6d-9596-ed217dd203e1_0", "content": "def walk_modules(path):\n    \"\"\"\n    This function loads a module and all its submodules from the given module path and\n    returns them. If *any* module throws an exception while importing, that\n    exception is thrown back.\n\n    :param path: The path to the module to be loaded.\n    :return: A list of modules loaded from the given path.\n    \"\"\"\n    mods = []\n    mod = import_module(path)\n    mods.append(mod)\n    if hasattr(mod, '__path__'):\n        for _, sub_path, is_pkg in iter_modules(mod.__path__):\n            full_path = path + '.' + sub_path\n            if is_pkg:\n                mods += walk_modules(full_path)\n            else:\n                sub_mod = import_module(full_path)\n                mods.append(sub_mod)\n    return mods\n", "import_code": ["from importlib import import_module\n", "from pkgutil import iter_modules\n"]}
{"id": "10511acd-e39e-3c9a-9ab2-e4f5a32d0aff_0", "content": "def execute_sublist(list):\n    \"\"\"\n    This function removes the last 4 samples from the feature and the first 4 samples from long and lat.\n    It returns the modified feature list, lat list and long list.\n    \"\"\"\n    feature = []\n    long = []\n    lat = []\n    for element in list:\n        long.append(element[-3])\n        lat.append(element[-4])\n        del element[1]\n        del element[1]\n        feature.append(element[:])\n    for i in range(4):\n        del feature[-1]\n        del long[0]\n        del lat[0]\n    return feature, lat, long\n", "import_code": []}
{"id": "10511acd-e39e-3c9a-9ab2-e4f5a32d0aff_1", "content": "def load_data_as_ndarray(filepath):\n    \"\"\"\n    This function reads data from a file and processes it into three numpy arrays: features array,\n    longitude targets array, and latitude targets array.\n    \"\"\"\n    feature_list = []\n    latitude_targets_list = []\n    longitude_targets_list = []\n    if not os.path.exists(filepath):\n        print('Invalid file path!')\n        return\n    for dir, sub_dir, files in os.walk(filepath, topdown=False):\n        for file in files:\n            print('Executing file:{0}'.format(os.path.basename(file)))\n            temp_list = []\n            for line in open(os.path.join(filepath, file), 'r', encoding=\n                'utf-8').readlines():\n                if line.strip().split()[0] == '66666':\n                    if len(temp_list) < 5:\n                        continue\n                    else:\n                        temp_fea, temp_lat, temp_long = execute_sublist(\n                            temp_list)\n                        feature_list.extend(temp_fea)\n                        latitude_targets_list.extend(temp_lat)\n                        longitude_targets_list.extend(temp_long)\n                        temp_list = []\n                        continue\n                list = line.strip().split()[-5:]\n                temp_list.append(list)\n    print('Executing finished!')\n    feature_array = np.array(feature_list)\n    latitude_array = np.array(latitude_targets_list)\n    longitude_array = np.array(longitude_targets_list)\n    return feature_array, latitude_array, longitude_array\n", "import_code": ["import numpy as np\n", "import os\n"]}
{"id": "10511acd-e39e-3c9a-9ab2-e4f5a32d0aff_2", "content": "def euclidean_distance(lat1, long1, lat2, long2):\n    \"\"\"\n    This function calculates the Euclidean distance between two points given their latitudes and longitudes.\n    \"\"\"\n    eu_dist = 0.0\n    latitude1 = math.pi / 180.0 * lat1\n    latitude2 = math.pi / 180.0 * lat2\n    longitude1 = math.pi / 180.0 * long1\n    longitude2 = math.pi / 180.0 * long2\n    R = 6378.1\n    temp = math.sin(latitude1) * math.sin(latitude2) + math.cos(latitude1\n        ) * math.cos(latitude2) * math.cos(longitude2 - longitude1)\n    if float(repr(temp)) > 1.0:\n        temp = 1.0\n    eu_dist = math.acos(temp) * R\n    return eu_dist\n", "import_code": ["import math\n"]}
{"id": "bc7aab92-e079-3aa6-9954-fb367bc7606c_1", "content": "def process_check(pid, argfile):\n    if not psutil.pid_exists(pid):\n        return None\n    f = open(argfile, 'w')\n    f.close()\n    return True\n", "import_code": ["import psutil\n"]}
{"id": "bc7aab92-e079-3aa6-9954-fb367bc7606c_2", "content": "def process_cpu(argv):\n    pid = int(argv[0])\n    argfile = argv[1]\n    interval = int(argv[2])\n    if not process_check(pid, argfile):\n        return\n    while True:\n        if not psutil.pid_exists(pid):\n            print('pid is not exist', pid)\n            return\n        tmp = psutil.Process(pid)\n        cpu = tmp.cpu_percent(interval)\n        value = '%12d %12f\\n' % (int(time.time()), cpu)\n        write_file(argfile, value)\n", "import_code": ["import psutil\n", "import time\n"]}
{"id": "bc7aab92-e079-3aa6-9954-fb367bc7606c_3", "content": "def process_mem(argv):\n    pid = int(argv[0])\n    argfile = argv[1]\n    interval = int(argv[2])\n    if not process_check(pid, argfile):\n        print('check process failed')\n        return\n    while True:\n        if not psutil.pid_exists(pid):\n            print('pid is not exist', pid)\n            return\n        tmp = psutil.Process(pid)\n        mem = tmp.memory_info()\n        value = '%12d %12d\\n' % (int(time.time()), mem.vms)\n        write_file(argfile, value)\n        time.sleep(interval)\n", "import_code": ["import psutil\n", "import time\n"]}
{"id": "bc7aab92-e079-3aa6-9954-fb367bc7606c_4", "content": "def process_dbd(argv):\n    pid = int(argv[0])\n    argfile = argv[1]\n    interval = int(argv[2])\n    if not process_check(pid, argfile):\n        return\n    sent = 0\n    received = 0\n    f = open(argfile, 'w')\n    value = 'mongoc sent received\\n'\n    f.write(value)\n    f.close\n    while True:\n        if not psutil.pid_exists(pid):\n            print('pid is not exist', pid)\n            return\n        command = 'mongoc-stat %d' % pid\n        retval = os.popen(command).read()\n        reg = (\n            '.*Streams : Egress Bytes             : The number of bytes sent. .*: ([0-9]+).*\\\\n.*Streams : Ingress Bytes            : The number of bytes received.*: ([0-9]+).*'\n            )\n        matchObj = re.search(reg, retval)\n        if matchObj:\n            tmpSent = int(matchObj.group(1))\n            tmpReceived = int(matchObj.group(2))\n            value = '%12d %12d %12d\\n' % (int(time.time()), tmpSent - sent,\n                tmpReceived - received)\n            write_file(argfile, value)\n            sent = tmpSent\n            received = tmpReceived\n        else:\n            print('not match')\n        time.sleep(interval)\n", "import_code": ["import re\n", "import psutil\n", "import time\n", "import os\n"]}
{"id": "223f0b24-ff39-38cb-b6e9-a85dafee478e_0", "content": "def category_t(x):\n    x = x.replace(' \u5929 ', '/')\n    x = x.replace(' \u5730 ', '/')\n    x = x.replace(' \u7384 ', '/')\n    x = x.replace(' \u9ec4 ', '/')\n    x = x.replace(';', '/')\n    x_list = x.split('/')\n    return x_list[0]\n", "import_code": []}
{"id": "223f0b24-ff39-38cb-b6e9-a85dafee478e_1", "content": "def industry_t(x):\n    x = x.replace(' \u5929 ', '/')\n    x = x.replace(' \u5730 ', '/')\n    x = x.replace(' \u7384 ', '/')\n    x = x.replace(' \u9ec4 ', '/')\n    x = x.replace(';', '/')\n    x_list = x.split('/')\n    return x_list[1]\n", "import_code": []}
{"id": "223f0b24-ff39-38cb-b6e9-a85dafee478e_2", "content": "def product_t(x):\n    x = x.replace(' \u5929 ', '/')\n    x = x.replace(' \u5730 ', '/')\n    x = x.replace(' \u7384 ', '/')\n    x = x.replace(' \u9ec4 ', '/')\n    x = x.replace(';', '/')\n    x_list = x.split('/')\n    return x_list[2]\n", "import_code": []}
{"id": "223f0b24-ff39-38cb-b6e9-a85dafee478e_3", "content": "def advertiser_t(x):\n    x = x.replace(' \u5929 ', '/')\n    x = x.replace(' \u5730 ', '/')\n    x = x.replace(' \u7384 ', '/')\n    x = x.replace(' \u9ec4 ', '/')\n    x = x.replace(';', '/')\n    x_list = x.split('/')\n    return x_list[3]\n", "import_code": []}
{"id": "223f0b24-ff39-38cb-b6e9-a85dafee478e_4", "content": "def creative_t(x):\n    x = x.replace(' \u5929 ', '/')\n    x = x.replace('  \u5730  ', '/')\n    x = x.replace(' \u7384 ', '/')\n    x = x.replace(' \u9ec4 ', '/')\n    x = x.replace(';', '/')\n    x_list = x.split('/')\n    return x_list[4]\n", "import_code": []}
{"id": "223f0b24-ff39-38cb-b6e9-a85dafee478e_5", "content": "def trian_save_word2vec(docs, window, embed_size=256, save_name='w2v.txt',\n    split_char=' '):\n    \"\"\"\n    This function takes in a list of documents, trains a Word2Vec model on them,\n    and saves the model in the specified format.\n\n    Parameters:\n    docs (list): A list of documents (strings) to be used for training the model.\n    embed_size (int): The size of the embeddings to be used in the model.\n    save_name (str): The name of the file to save the model in.\n    split_char (str): The character to split the documents on.\n\n    Returns:\n    w2v: The trained Word2Vec model.\n    \"\"\"\n    input_docs = []\n    for i in docs:\n        input_docs.append(i.split(split_char))\n    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s',\n        level=logging.INFO)\n    w2v = Word2Vec(input_docs, size=embed_size, sg=1, window=window, seed=\n        2020, workers=32, min_count=1, iter=10)\n    w2v.wv.save_word2vec_format(save_name)\n    print('w2v model done')\n    return w2v\n", "import_code": ["import logging\n", "from gensim.models import Word2Vec\n"]}
{"id": "4e95fb95-d8ce-385b-a369-e1670e4ee694_0", "content": "def clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n    \"\"\"\n    string = re.sub(\"[^A-Za-z0-9(),!?\\\\'\\\\`]\", ' ', string)\n    string = re.sub(\"\\\\'s\", \" 's\", string)\n    string = re.sub(\"\\\\'ve\", \" 've\", string)\n    string = re.sub(\"n\\\\'t\", \" n't\", string)\n    string = re.sub(\"\\\\'re\", \" 're\", string)\n    string = re.sub(\"\\\\'d\", \" 'd\", string)\n    string = re.sub(\"\\\\'ll\", \" 'll\", string)\n    string = re.sub(',', ' , ', string)\n    string = re.sub('!', ' ! ', string)\n    string = re.sub('\\\\(', ' \\\\( ', string)\n    string = re.sub('\\\\)', ' \\\\) ', string)\n    string = re.sub('\\\\?', ' \\\\? ', string)\n    string = re.sub('\\\\s{2,}', ' ', string)\n    return string.strip().lower()\n", "import_code": ["import re\n"]}
{"id": "4e95fb95-d8ce-385b-a369-e1670e4ee694_1", "content": "def load_data(file_path, sw_path=None, min_frequency=0, max_length=0,\n    vocab_processor=None, shuffle=True):\n    \"\"\"\n    Build dataset for mini-batch iterator\n    :param file_path: Data file path\n    :param sw_path: Stop word file path\n    :param min_frequency: the minimal frequency of words to keep\n    :param max_length: the max document length\n    :param vocab_processor: the predefined vocabulary processor\n    :param shuffle: whether to shuffle the data\n    :return data, labels, lengths, vocabulary processor\n    \"\"\"\n    print('Building dataset ...')\n    start = time.time()\n    labels = []\n    sentences = []\n    if sw_path is not None:\n        sw = _stop_words(sw_path)\n    else:\n        sw = None\n    labelsMapping = {'Non-KejadianPenting': 0, 'Jalan-Rusak': 1, 'Internet':\n        2, 'PDAM': 3, 'Mati-Listrik': 4}\n    lines = [line.strip() for line in open(file_path)]\n    for idx in range(0, len(lines), 3):\n        sent = lines[idx]\n        sent = sent.lower()\n        sent = clean_str(sent)\n        sentences.append(sent)\n        labels.append(labelsMapping[lines[idx + 1]])\n    labels = np.array(labels)\n    lengths = np.array(list(map(len, [sent.strip().split(' ') for sent in\n        sentences])))\n    if max_length == 0:\n        max_length = max(lengths)\n    if vocab_processor is None:\n        vocab_processor = learn.preprocessing.VocabularyProcessor(max_length,\n            min_frequency=min_frequency)\n        data = np.array(list(vocab_processor.fit_transform(sentences)))\n    else:\n        data = np.array(list(vocab_processor.transform(sentences)))\n    data_size = len(data)\n    if shuffle:\n        shuffle_indices = np.random.permutation(np.arange(data_size))\n        data = data[shuffle_indices]\n        labels = labels[shuffle_indices]\n        lengths = lengths[shuffle_indices]\n    end = time.time()\n    print('Dataset has been built successfully.')\n    print('Run time: {}'.format(end - start))\n    print('Number of sentences: {}'.format(len(data)))\n    print('Vocabulary size: {}'.format(len(vocab_processor.vocabulary_.\n        _mapping)))\n    print('Max document length: {}\\n'.format(vocab_processor.\n        max_document_length))\n    return data, labels, lengths, vocab_processor\n", "import_code": ["import numpy as np\n", "from tensorflow.contrib import learn\n", "import time\n"]}
{"id": "4e95fb95-d8ce-385b-a369-e1670e4ee694_3", "content": "def _stop_words(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        sw = list()\n        for line in f:\n            sw.append(line.strip())\n    return set(sw)\n", "import_code": []}
{"id": "cd14d498-fafe-3bd2-8a21-05d265505137_2", "content": "def at_bat(batter, starting_pitcher, relief_pitcher, inning):\n    \"\"\"\n    takes as input a batter (batter Player), starting pitcher\n    (starting_pitcher, Pitcher), relief pitcher (relief_pitcher Pitcher), and\n    the current inning in the simulated game (inning int)\n    simulates a single at bat for a single player/pitcher interaction and\n    prints some of the information to the screen\n    \"\"\"\n    print('batter: ', batter)\n    if inning < 8:\n        print('pitcher:', starting_pitcher)\n        p = Player.status(batter, starting_pitcher)\n    else:\n        print('pitcher:', relief_pitcher)\n        p = Player.status(batter, relief_pitcher)\n    return p\n", "import_code": []}
{"id": "cd14d498-fafe-3bd2-8a21-05d265505137_3", "content": "def one_game(team1, team2, s_pitcher1, r_pitcher1, s_pitcher2, r_pitcher2):\n    inning = 1\n    outs = 0\n    score = [0, 0]\n    game = True\n    ondeck = [0, 0]\n    bases1 = [False, False, False, False, False]\n    bases2 = [False, False, False, False, False]\n    teams = [team1, team2]\n    bases = [bases1, bases2]\n    s_pitcher = [s_pitcher1, s_pitcher2]\n    r_pitcher = [r_pitcher1, r_pitcher2]\n    while game:\n        if inning % 1 == 0:\n            ups = 0\n            downs = 1\n        else:\n            ups = 1\n            downs = 0\n        x = at_bat(teams[ups].lineup[ondeck[ups]], teams[downs].pitchers[\n            s_pitcher[downs] - 1], teams[downs].pitchers[r_pitcher[downs] -\n            1], inning)\n        teams[ups].lineup[ondeck[ups]].status_list.append(x)\n        if teams[ups].lineup[ondeck[ups]].status_list[-1] == 0:\n            print('Out')\n        if teams[ups].lineup[ondeck[ups]].status_list[-1] == 1:\n            print('Single')\n        if teams[ups].lineup[ondeck[ups]].status_list[-1] == 2:\n            print('Double')\n        if teams[ups].lineup[ondeck[ups]].status_list[-1] == 3:\n            print('Triple')\n        if teams[ups].lineup[ondeck[ups]].status_list[-1] == 4:\n            print('HOME RUN!')\n        for i in range(3, 0, -1):\n            if bases[ups][i]:\n                bases[ups][i] = False\n                if i + x < 4:\n                    bases[ups][i + x] = True\n                else:\n                    score[ups] += 1\n        bases[ups][x] = True\n        ondeck[ups] += 1\n        if bases[ups][0]:\n            outs += 1\n            bases[ups][0] = False\n        if bases[ups][4]:\n            score[0] += 1\n        print(teams[ups])\n        print('inning:', inning, ' outs:', outs, ' score:', score, sep='')\n        print(bases[ups])\n        bases[ups][4] = False\n        print()\n        if outs == 3:\n            inning += 0.5\n            bases[ups] = [False, False, False, False, False]\n        if ondeck[ups] == len(teams[ups].lineup):\n            ondeck[ups] = 0\n        if (outs == 3) & (inning >= 10) and score[0] != score[1\n            ] and inning % 1 == 0:\n            game = False\n        if outs == 3:\n            outs = 0\n    return score\n", "import_code": []}
{"id": "cd14d498-fafe-3bd2-8a21-05d265505137_4", "content": "def results(n, score, team1, team2, win_count1, win_count2):\n    print('Final Score', score)\n    if float(score[0]) > float(score[1]):\n        print(team1, 'win!')\n        win_count1 += 1\n    elif float(score[1]) > float(score[0]):\n        print(team2, 'win!')\n        win_count2 += 1\n    else:\n        breakpoint()\n    if n == win_count1 + win_count2:\n        print()\n        print(team1, ':', win_count1, sep='')\n        print(team2, ':', win_count2, sep='')\n        percentage_won1 = win_count1 / (win_count1 + win_count2) * 100\n        percentage_won2 = win_count2 / (win_count1 + win_count2) * 100\n        print(team1, ' have a ', percentage_won1, '%', ' chance of winning',\n            sep='')\n        print(team2, ' have a ', percentage_won2, '%', ' chance of winning',\n            sep='')\n    return win_count1, win_count2\n", "import_code": []}
{"id": "cdba51a3-8e0c-3edf-aca4-41631f613849_0", "content": "def scrape_pubmed(pmid):\n    base_url = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n    params = 'db=pubmed&retmode=xml&tool=PMA&id=' + pmid\n    response = requests.get(base_url + '?' + params)\n    content = response.content\n    return content\n", "import_code": ["import requests\n"]}
{"id": "cdba51a3-8e0c-3edf-aca4-41631f613849_1", "content": "def get_xml_attrib(content, attrib):\n    tree = etree.fromstring(content)\n    for i in tree.iter(attrib):\n        return i.text\n", "import_code": ["from lxml import etree\n"]}
{"id": "cdba51a3-8e0c-3edf-aca4-41631f613849_2", "content": "def reach_api(text, format):\n    base_url = 'http://agathon.sista.arizona.edu:8080/odinweb/api/text'\n    params = {'text': text, 'output': format}\n    response = requests.post(base_url, params=params)\n    return response.json()\n", "import_code": ["import requests\n"]}
{"id": "f0bd6f18-e010-3d41-b3f5-3c8d34069fc1_0", "content": "def pkg_config(name, arg, strip_prefix=0):\n    args = ['pkg-config', name, arg]\n    process = subprocess.Popen(args, stdout=subprocess.PIPE,\n        universal_newlines=True)\n    stdout, stderr = process.communicate()\n    exitcode = process.wait()\n    if exitcode:\n        sys.exit(exitcode)\n    args = stdout.strip().split()\n    if strip_prefix:\n        args = [item[strip_prefix:] for item in args]\n    return args\n", "import_code": ["import sys\n", "import subprocess\n"]}
{"id": "76ae208d-97de-3f0b-bd0d-4d1e28085f5a_1", "content": "def bp_model(db_in, file_out, name_flag='g_wally', flag=True):\n    \"\"\"Building Permit Model for UFDA Project.\n    Args:\n        db_in (str): File path to SQLite database\n        file_out (str): File path to ...\n    \"\"\"\n    print((db_in, file_out))\n    print('Analyzing...')\n    return\n", "import_code": []}
{"id": "76ae208d-97de-3f0b-bd0d-4d1e28085f5a_2", "content": "def get_pos_params(func):\n    spec = inspect.getargspec(func)\n    try:\n        optionals = zip(spec.args[-len(spec.defaults):], spec.defaults)\n        optionals = [(o[0].replace('_', '-'), o[1]) for o in optionals]\n    except TypeError:\n        optionals = [[None, None]]\n    pos = ['<{}>'.format(a.replace('_', '-')) for a in spec.args if a.\n        replace('_', '-') not in [o[0] for o in optionals]]\n    return pos\n", "import_code": ["import inspect\n"]}
{"id": "76ae208d-97de-3f0b-bd0d-4d1e28085f5a_3", "content": "def pos_args(func):\n    return [d[arg] for arg in get_pos_params(func)]\n", "import_code": []}
{"id": "76ae208d-97de-3f0b-bd0d-4d1e28085f5a_5", "content": "def opt_args(func):\n    return {opt[0].replace('--', '').replace('-', '_'): d[opt[0]] for opt in\n        get_opt_params(func)}\n", "import_code": []}
{"id": "dddebfcb-51f2-33d2-b340-d0fb2d15c6bd_0", "content": "def count_letter(content, letter):\n    \"\"\"Count the number of times `letter` appears in `content`.\n\n    Args:\n      content (str): The string to search.\n      letter (str): The letter to search for.\n\n    Returns:\n      int: The number of times `letter` appears in `content`.\n\n    Raises:\n      ValueError: If `letter` is not a one-character string.\n    \"\"\"\n    if not isinstance(letter, str) or len(letter) != 1:\n        raise ValueError('`letter` must be a single character string.')\n    return len([char for char in content if char == letter])\n", "import_code": []}
{"id": "dddebfcb-51f2-33d2-b340-d0fb2d15c6bd_4", "content": "def standardize(column):\n    \"\"\"Standardize the values in a column.\n\n    Args:\n      column (pandas Series): The data to standardize.\n\n    Returns:\n      pandas Series: the values as z-scores\n    \"\"\"\n    z_score = (column - column.mean()) / column.std()\n    return z_score\n", "import_code": []}
{"id": "dddebfcb-51f2-33d2-b340-d0fb2d15c6bd_6", "content": "def mean(values):\n    \"\"\"Get the mean of a sorted list of values\n\n    Args:\n      values (iterable of float): A list of numbers\n\n    Returns:\n      float: The mean of the list of values.\n    \"\"\"\n    mean = sum(values) / len(values)\n    return mean\n", "import_code": []}
{"id": "dddebfcb-51f2-33d2-b340-d0fb2d15c6bd_7", "content": "def median(values):\n    \"\"\"Get the median of a sorted list of values\n\n    Args:\n      values (iterable of float): A list of numbers\n\n    Returns:\n      float: The median of the list of values.\n    \"\"\"\n    midpoint = int(len(values) / 2)\n    if len(values) % 2 == 0:\n        median = (values[midpoint - 1] + values[midpoint]) / 2\n    else:\n        median = values[midpoint]\n    return median\n", "import_code": []}
{"id": "dddebfcb-51f2-33d2-b340-d0fb2d15c6bd_9", "content": "def better_add_column(values, df=None):\n    \"\"\"Add a column of `values` to a DataFrame `df`.\n    The column will be named \"col_<n>\" where \"n\" is\n    the numerical index of the column.\n\n    Args:\n      values (iterable): The values of the new column\n      df (DataFrame, optional): The DataFrame to update.\n        If no DataFrame is passed, one is created by default.\n\n    Returns:\n      DataFrame: The updated DataFrame.\n    \"\"\"\n    if df is None:\n        df = pd.DataFrame()\n    df['col_{}'.format(len(df.columns))] = values\n    return df\n", "import_code": ["import pandas as pd\n"]}
{"id": "155d41c4-499e-30ce-bc8a-9870e2bf7c2f_0", "content": "def compile_code(input_file):\n    \"\"\"\n    This function compiles the C++ code using g++ compiler.\n\n    Parameters:\n    input_file (str): The name of the C++ source file without extension.\n\n    Returns:\n    dict: A dictionary containing the compilation status, standard error (stderr), and standard output (stdout).\n    \"\"\"\n    ret = {'compile_error': False, 'stderr': None, 'stdout': None}\n    compile_result = subprocess.run(['g++', '-std=c++11', input_file +\n        '.cpp', '-o', input_file + '.exe'], universal_newlines=True, stdout\n        =subprocess.PIPE, stderr=subprocess.PIPE)\n    if compile_result.returncode != 0:\n        ret['compile_error'] = True\n    ret['stderr'] = compile_result.stderr\n    ret['stdout'] = compile_result.stdout\n    return ret\n", "import_code": ["import subprocess\n"]}
{"id": "155d41c4-499e-30ce-bc8a-9870e2bf7c2f_1", "content": "def execute_code(input_file, input_string=None):\n    \"\"\"\n    This function executes the compiled C++ code.\n\n    Parameters:\n    input_file (str): The name of the compiled executable file without extension.\n    input_string (str, optional): The input to be provided to the program during execution. Defaults to None.\n\n    Returns:\n    dict: A dictionary containing the runtime status, time limit exceeded (tle) flag, standard error (stderr), and standard output (stdout).\n    \"\"\"\n    ret = {'runtime_error': False, 'tle': False, 'stderr': None, 'stdout': None\n        }\n    try:\n        execution_result = subprocess.run([input_file + '.exe'],\n            universal_newlines=True, stdout=subprocess.PIPE, stderr=\n            subprocess.PIPE, input=input_string, timeout=2)\n        if execution_result.returncode != 0:\n            ret['runtime_error'] = True\n        ret['stderr'] = execution_result.stderr\n        ret['stdout'] = execution_result.stdout\n    except:\n        ret['tle'] = True\n    return ret\n", "import_code": ["import subprocess\n"]}
{"id": "d59ce55a-65c2-35f7-a74b-7f6b3c792b81_0", "content": "def create_matrix(num_books):\n    \"\"\"Construct Massey's matrix\n\n    Parameters\n    ----------\n    num_books : integer\n        number of books in the dataset\n\n    Returns\n    -------\n    game_matrix : Numpy Array\n        constructed 2D numpy array with Massey's matrix\n\n    Examples\n    --------\n    >>> create_matrix(3)\n    [[2, -1, -1],[-1, 2, -1],[-1,-1,2]]\n    \"\"\"\n    game_mat = np.full([num_books, num_books], -1)\n    np.fill_diagonal(game_mat, num_books - 1)\n    game_mat[num_books - 1] = np.ones(num_books)\n    return game_mat\n", "import_code": ["import numpy as np\n"]}
{"id": "d59ce55a-65c2-35f7-a74b-7f6b3c792b81_1", "content": "def compute_differentials_single(df, feature_list, var_diff):\n    \"\"\"Compute overall margin of victory for each book\n\n    Parameters\n    ----------\n    df : pandas dataframe\n        dataset to be used for computation\n    feature_list: list\n        features relevant to include in computation\n    var_diff:\n        variable for differentials to be calculated\n\n    Returns\n    -------\n    None\n    \"\"\"\n    game_scores = np.array(df[feature_list])\n    df[var_diff] = df[[feature_list]].apply(lambda x: len(df) * x - sum(np.\n        array(df[feature_list])), axis=1)\n    return None\n", "import_code": ["import numpy as np\n"]}
{"id": "d59ce55a-65c2-35f7-a74b-7f6b3c792b81_2", "content": "def solve_lin_system(df, feature_list, var_diff, rank_name='Massey_rank'):\n    \"\"\"Compute the ranking vector and add it into the\n        dataframe\n\n    Parameters\n    ----------\n    df : pandas dataframe\n        dataset to be used for computation\n    feature_list: list\n        features relevant to include in computation\n    var_diff: string\n        variable for differentials to be calculated\n    rank_name: string\n        name of ranking vector to be added\n\n    Returns\n    -------\n    None\n    \"\"\"\n    M = create_matrix(len(df))\n    compute_differentials(df, feature_list, var_diff)\n    y = np.array(df[var_diff])\n    y[len(y) - 1] = 0\n    df[rank_name] = np.linalg.solve(M, y)\n    df.pop(var_diff)\n    return None\n", "import_code": ["import numpy as np\n"]}
{"id": "36e3dd46-d355-31a6-8ecd-f69d1c46a1d7_1", "content": "def PCA(data):\n    meaned = np.mean(data.T, axis=1)\n    centered = data - meaned\n    V = np.cov(centered.T)\n    values, vectors = eig(V)\n    P = vectors.T.dot(centered.T)\n    return P.T\n", "import_code": ["from numpy.linalg import eig\n", "import numpy as np\n", "from numpy.linalg import eig\n"]}
{"id": "f9387e2b-8d1e-3afa-8eec-12f6640de2a8_0", "content": "def getFromSina(symbol='sh600000', scale=5, datalen=100):\n    klinePattern = re.compile(\n        '\\\\{day:\\\\\"([\\\\d-]+)\\\\\",open:\\\\\"([\\\\d.]+)\\\\\",high:\\\\\"([\\\\d.]+)\\\\\",low:\\\\\"([\\\\d.]+)\\\\\",close:\\\\\"([\\\\d.]+)\\\\\",volume:\\\\\"(\\\\d+)\\\\\"\\\\}'\n        )\n    params = {'symbol': symbol, 'scale': scale, 'datalen': datalen}\n    url = (\n        'http://money.finance.sina.com.cn/quotes_service/api/json_v2.php/CN_MarketData.getKLineData'\n        )\n    r = requests.get(url=url, params=params)\n    list = []\n    match = klinePattern.search(r.content)\n    while match:\n        trans = Trans(match.group(1), match.group(2), match.group(3), match\n            .group(4))\n        if trans.price > 0 and trans.volume > 0:\n            transList.insert(0, trans)\n        match = transPattern.search(content, match.end() + 1)\n    print(r.content)\n    df = pd.DataFrame()\n    return df\n", "import_code": ["import requests\n", "import re\n", "import pandas as pd\n", "import requests\n"]}
{"id": "7508187c-9c65-3798-a136-6285f7f69a2a_0", "content": "def hidden_to_pandas(model, y_pred, y_true):\n    \"\"\"\n    Join hidden state with predicted and true y label\n    model: ontoencoder.TopoNet, trained model to provide hidden state\n    y_pred: predicted class (not binarized)\n    y_true: real class\n\n    return pandas.DataFrame, columns = all hidden GO terms and y_pred, y_true, rows = test sample\n    \"\"\"\n    df = pd.DataFrame(model.hidden.data.numpy(), columns=model.\n        term_name_to_id.index)\n    df['y_pred'] = y_pred\n    df['y_true'] = y_true\n    return df\n", "import_code": ["import pandas as pd\n"]}
{"id": "7508187c-9c65-3798-a136-6285f7f69a2a_1", "content": "def differential_hidden_state(df, cell_group=['gluta', 'GABA'], alpha=0.05,\n    corr_method='fdr_bh'):\n    \"\"\"\n    Use Welch's t-test to find hidden state that are key to distinguish cell type\n    df: pandas.DataFrame produced by `hidden_to_pandas`\n    cell_group = list of length 2, cell type that your are interested in seeing differential states\n\n    return pandas.DataFrame storing result\n    \"\"\"\n    group1 = df.loc[df['y_pred'] == cell_group[0]]\n    group2 = df.loc[df['y_pred'] == cell_group[1]]\n    stat = [stats.ttest_ind(group1[go_term], group2[go_term], equal_var=\n        False) for go_term in df.columns[:-2]]\n    rejected, p_corrected, alpha_Sidak, alpha_bonf = multipletests(pvals=[s\n        [1] for s in stat], alpha=alpha, method=corr_method, is_sorted=\n        False, returnsorted=False)\n    stat_df = pd.DataFrame(columns=['t', 'p-unadjusted', 'reject'])\n    stat_df['t'] = [s[0] for s in stat]\n    stat_df['p-unadjusted'] = [s[1] for s in stat]\n    stat_df['reject'] = rejected\n    stat_df.index = df.columns[:-2]\n    stat_df['name'] = term_entropy.loc[stat_df.index, 'name']\n    stat_df['t_abs'] = stat_df['t'].abs()\n    return stat_df\n", "import_code": ["import pandas as pd\n", "from statsmodels.stats.multitest import multipletests\n", "from scipy import stats\n", "from statsmodels.stats.multitest import multipletests\n"]}
{"id": "979c8b4f-178c-349e-a949-258091f9d0fe_0", "content": "def _get_repo_data(repo):\n    data = {'id': repo.id, 'name': repo.name, 'description': repo.\n        description, 'private': repo.private, 'owner': repo.owner.login,\n        'state': 'present'}\n    return data\n", "import_code": []}
{"id": "8030ea38-97e9-30a4-b284-8e0912dbba06_0", "content": "def plot_fct(ax, data, y_label, x_label='Time in hours', grid='both'):\n    \"\"\"\n    Plots the given data.\n\n    Arguments:\n    ax -- subplot to plot the data on.\n    data -- list, data to be plotted.\n    y_label -- str, label for the y axis.\n    x_label -- str, label for the x axis (default Time in hours).\n    grid -- str, {'major', 'minor', 'both'} grid lines to show\n    (default both).\n\n    Returns:\n    figure_handle -- handle for the current subplot.\n    \"\"\"\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n    ax.grid(True, which=grid)\n    return ax.plot(np.arange(len(data)), data)\n", "import_code": ["import numpy as np\n"]}
{"id": "cadca81e-ecb1-3792-ac32-1414ab4d629c_0", "content": "def preprocessing(s):\n    \"\"\"\n    Helper function.  Takes a dataframe and drops nans, cleans up col names.\n\n    Import stuff is with the stack.  This is to get a nice index for merge later.\n    \"\"\"\n    s[np.isinf(s)] = np.nan\n    s = s.dropna()\n    letter = s.columns[0][0]\n    s.columns = [int(y.split('_')[1][-4:]) for y in s.columns]\n    s = s.stack()\n    s.name = letter\n    s.index.names[-1] = 'year'\n    s = s.sortlevel('year')\n    s = s.reorder_levels((2, 0, 1))\n    return s\n", "import_code": ["import numpy as np\n"]}
{"id": "cadca81e-ecb1-3792-ac32-1414ab4d629c_1", "content": "def process_and_merge(s):\n    \"\"\"\n    Where the action is at.  Is what assembles stacked Dataframes (series)\n    into merged dataframe.\n    \"\"\"\n    l = [preprocessing(df) for df in s]\n    d = {x.name: x for x in l}\n    df = pd.DataFrame(d)\n    df.index.names = [x.lower() for x in df.index.names]\n    return pd.DataFrame(d)\n", "import_code": ["import pandas as pd\n"]}
{"id": "d8c009bc-3d15-383b-abf5-230deafbeb35_0", "content": "def word_tf_idf(documento):\n    \"\"\"\n    This function calculates the TF-IDF (Term Frequency-Inverse Document Frequency) for each word in a document.\n    TF-IDF is a measure of the importance of a word in a document relative to the documents in a corpus.\n    It is commonly used in information retrieval systems and text mining applications.\n\n    Args:\n    documento (list): A list of documents (strings)\n\n    Returns:\n    words (list): A list of words (strings) sorted alphabetically\n    words_tot (dict): A dictionary mapping each word to its TF-IDF value\n    median (float): The median TF-IDF value across all documents\n    df_pattern (DataFrame): A DataFrame containing the TF-IDF values for each word in each document\n    df_suma (DataFrame): A DataFrame containing the sum of TF-IDF values for each document\n    \"\"\"\n    words = []\n    word_list = []\n    df_pattern = pd.DataFrame()\n    i = 0\n    for utterance in documento:\n        w = re.findall('\\\\w+', utterance.lower(), flags=re.UNICODE)\n        words = w\n        words = [word for word in words if word not in stopwords.words(\n            'english')]\n        words = [word for word in words if not word.isdigit()]\n        words = [stemmer.stem(w) for w in words]\n        pattern_words = words\n        df = pd.DataFrame(pattern_words)\n        df['ocurrencias'] = 1\n        df.columns = ['palabras', 'ocurrencias']\n        df = df.groupby(['palabras'])['ocurrencias'].sum()\n        df = pd.DataFrame(df)\n        df['n_documento'] = i\n        i += 1\n        df_pattern = df_pattern.append(df)\n        word_list.extend(words)\n    n = i\n    df_pattern = df_pattern.reset_index()\n    df_pattern['doc'] = 1\n    df = df_pattern.groupby(['palabras', 'n_documento'])['doc'].mean()\n    df = df.reset_index()\n    df = df_pattern.groupby(['palabras'])['doc'].sum()\n    df = df.reset_index()\n    df_pattern = df_pattern.drop(['doc'], axis=1)\n    df_pattern = df_pattern.merge(df, on='palabras', how='left')\n    df_pattern['tf-idf'] = df_pattern.apply(lambda x: np.log(n / x['doc']) *\n        x['ocurrencias'], axis=1)\n    df_aux = df_pattern.groupby(['palabras'])['ocurrencias'].sum()\n    df_aux = df_aux.reset_index()\n    words_tot = df_pattern.to_dict()\n    words_tot['palabras'].update({'UNK': 0})\n    words = sorted(list(set(word_list)))\n    words.append('UNK')\n    df_suma = df_pattern.groupby(['n_documento'])['tf-idf'].sum()\n    df_suma = df_suma.reset_index()\n    median = df_suma['tf-idf'].median()\n    return words, words_tot, median, df_pattern, df_suma\n", "import_code": ["import numpy as np\n", "import re\n", "import pandas as pd\n", "from nltk.corpus import stopwords\n"]}
{"id": "daa377b9-3fb9-32da-a510-18a2c089efe1_0", "content": "def porc(nomfic):\n    \"\"\"\n    Returns the winning percentages for a file (and thus, a year)\n    Requires a file named Resultat as input\n    \"\"\"\n    df4 = pd.read_excel(nomfic)\n    nbNul = []\n    nbDom = []\n    nbExt = []\n    nbAutres = []\n    for i in range(len(df4.columns)):\n        if i > 1:\n            nbNul.append(len(df4[df4[i].isin(['Nul'])]))\n            nbDom.append(len(df4[df4[i].isin(['Domicile'])]))\n            nbExt.append(len(df4[df4[i].isin(['Ext\u00e9rieur'])]))\n            nbAutres.append(len(df4[~df4[i].isin(['Nul', 'Domicile',\n                'Ext\u00e9rieur'])]))\n    totNul = 0\n    totDom = 0\n    totExt = 0\n    totAutres = 0\n    for i in range(len(nbNul)):\n        totNul += nbNul[i]\n        totDom += nbDom[i]\n        totExt += nbExt[i]\n        totAutres += nbAutres[i]\n    tot = totNul + totDom + totExt\n    porcDom = round(totDom / tot * 100, 2)\n    porcNul = round(totNul / tot * 100, 2)\n    porcExt = round(totExt / tot * 100, 2)\n    porcAutres = totAutres / tot * 100\n    return porcDom, porcNul, porcExt, porcAutres\n", "import_code": ["import pandas as pd\n"]}
{"id": "daa377b9-3fb9-32da-a510-18a2c089efe1_3", "content": "def nbButs(nomfic):\n    \"\"\"\n    Requires a file named Score as input.\n    Returns the average number of goals per match (domicile/exterieur/totaux) and the number of matches.\n    \"\"\"\n    df = pd.read_excel(nomfic)\n    df = df.apply(pd.to_numeric, errors='coerce')\n    nbButsDom = 0\n    nbButsExt = 0\n    nbButsTot = 0\n    nbMatchs = 0\n    for i in range(len(df.columns)):\n        if i > 1:\n            nbButsTot += np.sum(df[i])\n            if i % 2 == 0:\n                nbButsDom += np.sum(df[i])\n                nbMatchs += len(df[i].dropna())\n            else:\n                nbButsExt += np.sum(df[i])\n    nbBMDom = nbButsDom / nbMatchs\n    nbBMExt = nbButsExt / nbMatchs\n    nbBMTot = nbButsTot / nbMatchs\n    return nbBMDom, nbBMExt, nbBMTot, nbMatchs\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "daa377b9-3fb9-32da-a510-18a2c089efe1_4", "content": "def ecart(df):\n    \"\"\"\n    Plots a histogram representing the goal difference between the home and away teams, per season.\n    \"\"\"\n    df2 = pd.DataFrame()\n    df3 = pd.DataFrame()\n    for i in range(len(df.columns) - 1):\n        if i > 1:\n            df2[i] = np.zeros(len(df))\n            df2[i] = df[i] - df[i + 1]\n    liste = []\n    for i in range(20):\n        liste.append(i - 9)\n    l2 = np.zeros(len(liste))\n    df3 = pd.DataFrame({'l': liste, 'l2': l2})\n    df3 = df3.set_index('l')\n    for j in df2.columns:\n        for i in df2.groupby(j).size().index:\n            df3.iloc[i + 9] = df3.iloc[i + 9] + df2.groupby(j).size()[i]\n    df3.plot(kind='bar')\n    return df2\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "76b01c64-2a74-3356-8160-9dd2f54db6f5_0", "content": "def filtering_sentence(text):\n    stop_words = stopwords.words('arabic')\n    filtered_sentence = []\n    w = text.translate(str.maketrans('', '', string.punctuation))\n    splitted_sentence = w.split()\n    for word in splitted_sentence:\n        if word not in stop_words:\n            filtered_sentence.append(word)\n    final_string = str(filtered_sentence)\n    return final_string\n", "import_code": ["from nltk.corpus import stopwords\n", "from nltk.corpus import stopwords\n", "import string\n"]}
{"id": "d91b1195-cb89-3124-b3c8-4f1c152b0c04_0", "content": "def check_position_on_ladder_or_snake(position):\n    \"\"\"\n    Checks if the player has landed on a ladder or snake\n    and returns the new position accordingly.\n\n    Parameters\n    ----------\n    position : int\n       Players position on game board\n\n    Returns\n    -------\n    position : int\n        Players new position\n    \"\"\"\n    ladders_and_snakes = {(1): 40, (8): 10, (36): 52, (43): 62, (49): 79, (\n        65): 82, (68): 85, (24): 5, (33): 3, (42): 30, (56): 37, (64): 27,\n        (74): 12, (87): 70}\n    if position in ladders_and_snakes.keys():\n        return ladders_and_snakes[position]\n    else:\n        return position\n", "import_code": []}
{"id": "d91b1195-cb89-3124-b3c8-4f1c152b0c04_1", "content": "def player_make_one_move(position):\n    \"\"\"\n    Simulates die throw and adds it to\n    the current player position. Then uses the\n    check_position_on_ladder_or_snake function.\n\n    Parameters\n    ----------\n    position: int\n        Players position\n\n    Returns\n    -------\n        Players new position\n    \"\"\"\n    position += rd.randint(1, 6)\n    return check_position_on_ladder_or_snake(position)\n", "import_code": ["import random as rd\n"]}
{"id": "d91b1195-cb89-3124-b3c8-4f1c152b0c04_3", "content": "def single_game(num_players):\n    \"\"\"\n    Returns duration of single game.\n\n    Arguments\n    ---------\n    num_players : int\n        Number of players in the game\n\n    Returns\n    -------\n    num_moves : int\n        Number of moves the winning player needed to reach the goal\n    \"\"\"\n    moves_per_player = []\n    for player in range(num_players):\n        num_moves = one_player_game()\n        moves_per_player.append(num_moves)\n    num_moves = min(moves_per_player)\n    return num_moves\n", "import_code": []}
{"id": "d91b1195-cb89-3124-b3c8-4f1c152b0c04_4", "content": "def multiple_games(num_games, num_players):\n    \"\"\"\n    Returns durations of a number of games.\n\n    Arguments\n    ---------\n    num_games : int\n        Number of games to play\n    num_players : int\n        Number of players in the game\n\n    Returns\n    -------\n    num_moves : list\n        List with the numbers of moves needed in each game.\n    \"\"\"\n    num_moves = []\n    for game in range(num_games):\n        num_moves.append(single_game(num_players))\n    return num_moves\n", "import_code": []}
{"id": "d91b1195-cb89-3124-b3c8-4f1c152b0c04_5", "content": "def multi_game_experiment(num_games, num_players, seed):\n    \"\"\"\n    Returns durations of a number of games when playing with given seed.\n\n    Arguments\n    ---------\n    num_games : int\n        Number of games to play\n    num_players : int\n        Number of players in the game\n    seed : int\n        Seed used to initialise the random number generator\n\n    Returns\n    -------\n    num_moves : list\n        List with the number of moves needed in each game.\n    \"\"\"\n    rd.seed(seed)\n    num_moves = multiple_games(num_games, num_players)\n    return num_moves\n", "import_code": ["import random as rd\n"]}
{"id": "87ee580f-0f4a-3b26-b869-6b2c42327a07_1", "content": "def make_filename(__url: str, __path: str):\n    return __path + '/' + __url.split('/')[-1]\n", "import_code": []}
{"id": "ef2c4feb-b22c-3003-9a48-d014dd5fafba_1", "content": "def crossValidation(model, folds, training_data, training_label):\n    accuracy = cross_val_score(model, training_data, training_label, cv=int\n        (folds))\n    return accuracy\n", "import_code": ["from sklearn.model_selection import cross_val_score\n"]}
{"id": "3b5101e3-013d-3bb9-af9b-f36fa94e48df_0", "content": "def get_sentiments(text):\n    analyzer = SentimentIntensityAnalyzer()\n    return analyzer.polarity_scores(text)\n", "import_code": ["from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"]}
{"id": "3b5101e3-013d-3bb9-af9b-f36fa94e48df_1", "content": "def split_sentiments(sentiments):\n    xs = [sent['neg'] for sent in sentiments]\n    ys = [sent['neu'] for sent in sentiments]\n    zs = [sent['pos'] for sent in sentiments]\n    return xs, ys, zs\n", "import_code": []}
{"id": "3b5101e3-013d-3bb9-af9b-f36fa94e48df_2", "content": "def lexical_diversity(text):\n    tokens = text.lower().split()\n    return len(set(tokens)) / len(tokens)\n", "import_code": []}
{"id": "e7d91098-c8b1-32d1-8d54-c90934b0bdba_0", "content": "def txt2Csv(filename):\n    \"\"\"\n    This function converts a text file to a CSV file.\n    \"\"\"\n    txtFile = []\n    with open(filename, 'rb') as infile:\n        for line in infile:\n            if line == '\\r\\n' or line == '\\n':\n                print('Found Empty line and removed it !!')\n                continue\n            txtFile.append(line.rstrip('\\r\\n').split('\\t'))\n    with open('../referencesCSV/' + filename + '.csv', 'wb') as outfile:\n        writer = csv.writer(outfile)\n        writer.writerows(txtFile)\n        del writer\n        print('Done!')\n    return\n", "import_code": ["import csv\n"]}
{"id": "994b53f6-dc09-3aa9-a3a2-b6f6efe17dea_0", "content": "def localize_timestamp(naive_ts, utc_offset=0):\n    \"\"\"Imbue a naive timestamp with a timezone\n\n    Python has two kinds of timestamps: naive (just a datetime, no\n    time zone) and aware (time plus time zone).  Mixing the two is\n    awkward.  Thie routine will assign a time zone to a datetime (UTC\n    by default) for consistency throughout Tracktable.\n\n    Note: You can change the default timezone by setting the\n    module-level variable DEFAULT_TIMEZONE.  This is not recommended.\n\n    Args:\n      naive_ts (datetime): Timestamp to localize\n      utc_offset (integer): Number of hours offset from UTC.  You can also specify a fraction of an hour as '0530', meaning 5 hours 30 minutes.\n\n    Returns:\n      A new datetime imbued with the desired time zone\n\n    \"\"\"\n    absolute_offset = abs(utc_offset)\n    if absolute_offset > 100:\n        hours = int(absolute_offset / 100)\n        minutes = absolute_offset % 100\n    else:\n        hours = absolute_offset\n        minutes = 0\n    if utc_offset >= 0:\n        utc_time = naive_ts - datetime.timedelta(hours=hours, minutes=minutes)\n    else:\n        utc_time = naive_ts + datetime.timedelta(hours=hours, minutes=minutes)\n    return DEFAULT_TIMEZONE.localize(utc_time)\n", "import_code": ["import datetime\n"]}
{"id": "994b53f6-dc09-3aa9-a3a2-b6f6efe17dea_1", "content": "def _fastparse(text):\n    \"\"\"INTERNAL METHOD\n\n    Because of the string processing we have to do, methods like\n    strptime are relatively slow.  We can go a lot faster if we\n    know exactly which characters in a substring correspond\n    to different parts of a timestamp.  This method is for that\n    case.\n\n    We assume that the timestamp is in the format 'YYYY-MM-DD\n    HH:MM:SS' with an optional addendum of '+XX' or '-XX' for\n    a UTC offset.\n\n    We deliberately don't trap any exceptions here.  If anything\n    goes wrong, the caller will find out about it and fall back\n    to a slower but more robust method.\n\n    Args:\n      text (string): String representation of timestamp\n\n    Returns:\n      Timezone-aware datetime object\n    \"\"\"\n    textlen = len(text)\n    if len(text) == 19:\n        year = int(text[0:4])\n        month = int(text[5:7])\n        day = int(text[8:10])\n        hour = int(text[11:13])\n        minute = int(text[14:16])\n        second = int(text[17:19])\n        return datetime.datetime(year=year, month=month, day=day, hour=hour,\n            minute=minute, second=second), 0\n    elif len(text) > 19 and (text[19] == '+' or text[19] == '-'):\n        year = int(text[0:4])\n        month = int(text[5:7])\n        day = int(text[8:10])\n        hour = int(text[11:13])\n        minute = int(text[14:16])\n        second = int(text[17:19])\n        offset = int(text[19:22])\n        return datetime.datetime(year=year, month=month, day=day, hour=hour,\n            minute=minute, second=second), offset\n", "import_code": ["import datetime\n"]}
{"id": "8133afea-9a8d-3741-aeaf-b4b6414bfa14_0", "content": "def convertToRow(arr):\n    \"\"\"\n    This function takes a numpy array as input and reshapes it to return a 1D array.\n    \"\"\"\n    x_shape = arr.shape\n    arr = np.reshape(arr, (x_shape[1], x_shape[0]))\n    x_shape = arr.shape\n    arr = arr[0]\n    return arr\n", "import_code": ["import numpy as np\n"]}
{"id": "8133afea-9a8d-3741-aeaf-b4b6414bfa14_3", "content": "def giniIndex(column, dataset, value, numberOfClasses):\n    \"\"\"\n    This function calculates the Gini index for a split in the dataset.\n    \"\"\"\n    giniMatrix = np.zeros([2, numberOfClasses])\n    for row in dataset:\n        if row[column] < value:\n            classNo = row[-1]\n            giniMatrix[0][int(classNo)] += 1\n        else:\n            classNo = row[-1]\n            giniMatrix[1][int(classNo)] += 1\n    totalLeft = np.sum(giniMatrix[0])\n    totalRight = np.sum(giniMatrix[1])\n    leftRatio = list()\n    rightRatio = list()\n    for j in range(numberOfClasses):\n        if totalLeft != 0:\n            ratio = giniMatrix[0][j] / totalLeft\n            leftRatio.append(ratio)\n        else:\n            leftRatio.append(0.0)\n        if totalRight != 0:\n            ratio = giniMatrix[1][j] / totalRight\n            rightRatio.append(ratio)\n        else:\n            rightRatio.append(0.0)\n    index = 0\n    for j in range(numberOfClasses):\n        index += leftRatio[j] * (1 - leftRatio[j])\n        index += rightRatio[j] * (1 - rightRatio[j])\n    return index\n", "import_code": ["import numpy as np\n"]}
{"id": "ee32781a-9774-3a9e-b0c5-ed50c225ea20_0", "content": "def validate_players_table(df):\n    if df.columns.tolist() == ['Player', 'From', 'To', 'Pos', 'Ht', 'Wt',\n        'Birth Date', 'College']:\n        return True\n    else:\n        return False\n", "import_code": []}
{"id": "ee32781a-9774-3a9e-b0c5-ed50c225ea20_1", "content": "def player_id_creator(player_fullname):\n    \"\"\"\n    Guess what player IDs should look like based on BasketBall References format\n    ID format seems to use:\n    {first five letters of the surname}\n    + {first two letters of forename}\n    + {counter}\n    \"\"\"\n    if len(player_fullname.split(' ')) < 2:\n        return None\n    else:\n        return (player_fullname.split(' ')[1][:5] + player_fullname.split(\n            ' ')[0][:2] + '01').lower()\n", "import_code": []}
{"id": "ee32781a-9774-3a9e-b0c5-ed50c225ea20_2", "content": "def check_player_ids_look_ok(df):\n    \"\"\"\n    Check to see a good proportion of player IDs match\n    what we'd expect them to be. Basketball Reference's player\n    If 50% of calculated player_ids match, it's probably ok\n    \"\"\"\n    correctness_threshold = 0.5\n    check_vector = df.apply(lambda x: x['player_id'] == player_id_creator(x\n        ['Player']), axis=1)\n    return check_vector.value_counts().to_dict()[True] > len(check_vector\n        ) * correctness_threshold\n", "import_code": []}
{"id": "ee32781a-9774-3a9e-b0c5-ed50c225ea20_3", "content": "def get_players_table(letter):\n    \"\"\"\n    Return a BS Tag of all playes whose surname begins with letter\n    \"\"\"\n    req = requests.get(players_url.format(firstletter=letter))\n    soup = BeautifulSoup(req.text, 'html.parser')\n    tables = soup.find_all('table')\n    table_lst = [x for x in tables if x.attrs.get('id') == 'players']\n    if len(table_lst) == 1:\n        return table_lst[0]\n    else:\n        print('Error: Unable to find player list for letter: %s' % letter)\n        return None\n", "import_code": ["from bs4 import BeautifulSoup\n", "import requests\n"]}
{"id": "ee32781a-9774-3a9e-b0c5-ed50c225ea20_4", "content": "def convert_HTML_players_table_to_df(t):\n    \"\"\"\n    Returns a Pandas DataFrame from BS4 player table input from BREF\n    \"\"\"\n    \"\"\"\n    Create base table as seen on the site\n    \"\"\"\n    df = pandas.read_html(str(t))[0]\n    assert validate_players_table(df\n        ), 'Player table format unexpected. Either table download failed or BR has changed its format'\n    \"\"\"\n    Enrichment past the basic table provided by Basketball Ref\n    \"\"\"\n    \"\"\"\n    1. Identify Hall of Famers and remove the asterisk from their Names!\n    \"\"\"\n    df['hall_of_fame'] = df.Player.map(lambda x: '*' in x)\n    df.Player = df.Player.map(lambda x: x.replace('*', ''))\n    \"\"\"\n    2. Grab Player IDs\n    \"\"\"\n    player_id_series = []\n    for row in t.find_all('tr'):\n        if row.th.attrs.get('data-append-csv'):\n            player_id_series.append(row.th.attrs.get('data-append-csv'))\n    if len(player_id_series) == len(df):\n        df['player_id'] = player_id_series\n        if check_player_ids_look_ok(df):\n            return df\n        else:\n            print('Unable to parse Player IDs')\n            df.drop('player_id', inplace=True)\n            return df\n", "import_code": ["import pandas\n"]}
{"id": "ee32781a-9774-3a9e-b0c5-ed50c225ea20_5", "content": "def get_players_dataframe(letter):\n    t = get_players_table(letter)\n    df = convert_HTML_players_table_to_df(t)\n    \"\"\"\n    Rename BRef's columns to friendlier DB ones\n    \"\"\"\n    df.rename(columns={'player_id': 'id', 'Player': 'player_name', 'From':\n        'year_from', 'To': 'year_to', 'Pos': 'position', 'Ht': 'height',\n        'Wt': 'weight', 'Birth Date': 'birth_date', 'College': 'college',\n        'hall_of_fame': 'hall_of_fame'}, inplace=True)\n    df.birth_date = df.birth_date.map(bref_date_transformation)\n    return df.where(pandas.notnull, None)\n", "import_code": ["import pandas\n"]}
{"id": "0eb1ff0a-f0b2-3192-883e-66de9b514877_0", "content": "def apply_dependencies(state: pd.DataFrame, dependencies: Dict) ->pd.DataFrame:\n    \"\"\"\n    This function applies the dependencies to the state dataframe.\n    If the dependencies dictionary contains a 'filters' key, it will filter the state dataframe.\n    The 'filters' value should be a dictionary where the keys are column names and the values are the values to filter by.\n    The function will return the filtered dataframe.\n    \"\"\"\n    if 'filters' in dependencies:\n        df = state\n        for key in dependencies['filters'].keys():\n            df = df[df[key]]\n        return df\n", "import_code": ["from typing import Dict\n", "import pandas as pd\n"]}
{"id": "56eb8b2e-e410-3afe-8a42-e3cd651e6258_0", "content": "def Sort(sub_li):\n    return sorted(sub_li, key=lambda x: x[1])\n", "import_code": []}
{"id": "a21a348f-134d-3254-bbc1-cef0dd898a95_1", "content": "def json_to_pandas(path, timestamp):\n    list = []\n    files = os.listdir(path)\n    for file in files:\n        if str(timestamp) in file:\n            list.append(pd.read_json(path + file))\n    return list\n", "import_code": ["import pandas as pd\n", "import os\n"]}
{"id": "a21a348f-134d-3254-bbc1-cef0dd898a95_2", "content": "def split_dataframe(df, chunk_size=1000):\n    list_of_dfs = [df[i:i + chunk_size] for i in range(0, df.shape[0],\n        chunk_size)]\n    return list_of_dfs\n", "import_code": []}
{"id": "59779329-35a9-36e0-8712-4b4b94e9bbd9_1", "content": "def create_model(num_teams):\n    model = Sequential()\n    model.add(Embedding(input_dim=num_teams, output_dim=50, input_length=2,\n        init='uniform', trainable=True))\n    model.add(Flatten())\n    model.add(Dense(100))\n    model.add(Dropout(0.5))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    return model\n", "import_code": ["from keras.layers import Input, Dense, Dropout, Flatten, Embedding, Activation\n", "from keras.models import Sequential\n", "from keras.layers import Input, Dense, Dropout, Flatten, Embedding, Activation\n", "from keras.layers import Input, Dense, Dropout, Flatten, Embedding, Activation\n", "from keras.layers import Input, Dense, Dropout, Flatten, Embedding, Activation\n", "from keras.layers import Input, Dense, Dropout, Flatten, Embedding, Activation\n"]}
{"id": "5fa3561d-f90b-32e1-8a0e-e17ca6fc146d_0", "content": "def get_wiki_url(wikidata_id, lang='en', debug=False):\n    url = (\n        f'https://www.wikidata.org/w/api.php?action=wbgetentities&props=sitelinks/urls&ids={wikidata_id}&format=json'\n        )\n    json_response = requests.get(url).json()\n    if debug:\n        print(wikidata_id, url, json_response)\n    entities = json_response.get('entities')\n    if entities:\n        entity = entities.get(wikidata_id)\n        if entity:\n            sitelinks = entity.get('sitelinks')\n            if sitelinks:\n                if lang:\n                    sitelink = sitelinks.get(f'{lang}wiki')\n                    if sitelink:\n                        wiki_url = sitelink.get('url')\n                        if wiki_url:\n                            return requests.utils.unquote(wiki_url)\n                else:\n                    wiki_urls = {}\n                    for key, sitelink in sitelinks.items():\n                        wiki_url = sitelink.get('url')\n                        if wiki_url:\n                            wiki_urls[key] = requests.utils.unquote(wiki_url)\n                    return wiki_urls\n    return None\n", "import_code": []}
{"id": "5fa3561d-f90b-32e1-8a0e-e17ca6fc146d_1", "content": "def page_open_body(name):\n    resp = req.get(name)\n    soup = BeautifulSoup(resp.text, 'lxml')\n    main_part = str(soup.body)\n    return main_part\n", "import_code": ["from bs4 import BeautifulSoup\n", "import requests as req\n"]}
{"id": "5fa3561d-f90b-32e1-8a0e-e17ca6fc146d_2", "content": "def get_dates(page, pattern):\n    new = re.findall(pattern, str(page))\n    date = ''\n    date_list = []\n    state = False\n    for j in new:\n        for i in j:\n            if i == '<':\n                state = True\n            if i == '>':\n                state = False\n            if i == '[':\n                state = True\n            if i == ']':\n                state = False\n            if i == '(':\n                state = True\n            if i == ')':\n                state = False\n            if not state:\n                if i != ']' and i != '>' and i != ')':\n                    date += i\n        date_list.append(date)\n        date = ''\n    data_list_new = []\n    for i in date_list:\n        new_elem = i.replace(u'\\xa0', u' ')\n        data_list_new.append(new_elem)\n    return data_list_new\n", "import_code": ["import requests as req\n", "import re\n"]}
{"id": "713eb219-7b3d-3292-9827-999fbd4ce9c6_0", "content": "def read_shadow_beam(beam, x_column_index=1, y_column_index=3, nbins_x=100,\n    nbins_y=100, nolost=1, ref=23, zeroPadding=0, gaussian_filter=0):\n    \"\"\"\n    This function reads Shadow beam data and returns a 2D histogram matrix.\n\n    Parameters\n    ----------\n    beam : ShadowBeam()\n        General Shadow beam object.\n    x_column_index : int\n        Shadow column number for x axis. The default is 1.\n    y_column_index : int\n        Shadow column number for y axis. The default is 3.\n    nbins_x : int\n        Number of bins for x axis. The default is 100.\n    nbins_y : int\n        Number of bins for y axis. The default is 100.\n    nolost : int\n        1 to use only good rays; 0 to use good and lost rays. The default is 1.\n    ref : TYPE, optional\n        Shadow column used as weights. The default is 23 (intensity).\n    zeroPadding : float\n        Range factor for inserting zeros in the beam matrix. The default is 0.\n    gaussian_filter : float\n        A float larger than 0 to apply gaussian filter. The default is 0.\n\n    Returns\n    -------\n    XY : float array\n        returns a 2D numpy array where first row is x coordinates, first column\n        is y coordinates, [0,0] is not used, and [1:1:] is the 2D histogram.\n\n    \"\"\"\n    histo2D = beam.histo2(col_h=x_column_index, col_v=y_column_index,\n        nbins_h=nbins_x, nbins_v=nbins_y, nolost=nolost, ref=ref)\n    x_axis = histo2D['bin_h_center']\n    y_axis = histo2D['bin_v_center']\n    xy = histo2D['histogram']\n    XY = np.zeros((nbins_y + 1, nbins_x + 1))\n    XY[1:, 0] = y_axis\n    XY[0, 1:] = x_axis\n    XY[1:, 1:] = np.array(xy).transpose()\n    if gaussian_filter != 0:\n        XY[1:, 1:] = ndimage.gaussian_filter(np.array(xy).transpose(),\n            gaussian_filter)\n    return XY\n", "import_code": ["import numpy as np\n", "from scipy import ndimage\n"]}
{"id": "713eb219-7b3d-3292-9827-999fbd4ce9c6_1", "content": "def read_spectra_brilliance_file(filename):\n    \"\"\"\n    This function reads spectra brilliance data from a JSON file and returns it as a dictionary.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the JSON file containing the spectra brilliance data.\n\n    Returns\n    -------\n    data : dict\n        A dictionary containing the spectra brilliance data.\n\n    \"\"\"\n    data = {}\n    with open(filename) as f:\n        filedata = json.load(f)\n    data['filename'] = filename\n    data['period'] = filedata['Input']['Light Source'][\n        '&lambda;<sub>u</sub> (mm)']\n    data['nk'] = filedata['Input']['Configurations']['Points (K)']\n    hmin = filedata['Input']['Configurations']['Harmonic Range'][0]\n    hmax = filedata['Input']['Configurations']['Harmonic Range'][1]\n    harmonics = []\n    harmonics.append(hmin)\n    addHarmonic = True\n    while addHarmonic:\n        harmonics.append(harmonics[-1] + 2)\n        if harmonics[-1] + 2 > hmax:\n            addHarmonic = False\n    data['harmonics'] = np.array(harmonics)\n    data['nh'] = len(harmonics)\n    data['energy'] = np.array(filedata['Output']['data'][0][0][::-1])\n    data['k'] = np.array(filedata['Output']['data'][0][1][::-1])\n    brilliance = []\n    for j in range(data['nh']):\n        brilliance.append(filedata['Output']['data'][0][2][::-1])\n    data['brilliance'] = np.array(brilliance)\n    return data\n", "import_code": ["import numpy as np\n", "import json\n"]}
{"id": "713eb219-7b3d-3292-9827-999fbd4ce9c6_2", "content": "def read_spectra_xyz(filename):\n    \"\"\"\n    This function reads spectra xyz data from a file and returns it as a 2D numpy array.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file containing the spectra xyz data.\n\n    Returns\n    -------\n    beam : float array\n        Returns a 2D numpy array where first row is x coordinates, first column\n        is y coordinates, [0,0] is not used, and [1:1:] is the z axis.\n\n    \"\"\"\n    data = np.genfromtxt(filename, skip_header=2)\n    X = data[:, 0]\n    Y = data[:, 1]\n    I = data[:, 2]\n    for nx in range(len(X)):\n        if X[nx + 1] == X[0]:\n            nx += 1\n            break\n    ny = int(len(Y) / nx)\n    print(nx, ny)\n    I_mtx = I.reshape((ny, nx))\n    beam = np.zeros((ny + 1, nx + 1))\n    beam[1:, 0] = Y[0::nx]\n    beam[0, 1:] = X[:nx]\n    beam[1:, 1:] = I_mtx\n    return beam\n", "import_code": ["import numpy as np\n"]}
{"id": "713eb219-7b3d-3292-9827-999fbd4ce9c6_4", "content": "def read_srw_int(filename):\n    \"\"\"\n    This function reads SRW intensity data from a file and returns it as a 3D numpy array.\n\n    Parameters\n    ----------\n    filename : str\n        Path to the file containing the SRW intensity data.\n\n    Returns\n    -------\n    mtx : float array\n        Returns a 3D numpy array where the first dimension is the energy axis, the second row is x coordinates,\n        the second column is y coordinates, and [0,0,0] is not used, and [1:1:1:] is the z axis.\n\n    \"\"\"\n    with open(filename, 'r') as infile:\n        data = infile.readlines()\n    infile.close()\n    ei = float(data[1].split('#')[1])\n    ef = float(data[2].split('#')[1])\n    en = int(data[3].split('#')[1])\n    xi = float(data[4].split('#')[1])\n    xf = float(data[5].split('#')[1])\n    xn = int(data[6].split('#')[1])\n    yi = float(data[7].split('#')[1])\n    yf = float(data[8].split('#')[1])\n    yn = int(data[9].split('#')[1])\n    nheaders = 11\n    if not data[10][0] == '#':\n        nheaders = 10\n    if 0:\n        intensity = np.zeros((en, yn, xn))\n        count = 0\n        for i in range(yn):\n            for j in range(xn):\n                for k in range(en):\n                    intensity[k, i, j] = data[count + nheaders]\n                    count += 1\n    if 1:\n        intensity = np.array(data[nheaders:], dtype='float').reshape((en,\n            yn, xn))\n    e_pts = np.linspace(ei, ef, en)\n    mtx = np.zeros((en, yn + 1, xn + 1))\n    for i in range(en):\n        mtx[i][0, 0] = e_pts[i]\n        mtx[i][0, 1:] = np.linspace(xi, xf, xn) * 1000.0\n        mtx[i][1:, 0] = np.linspace(yi, yf, yn) * 1000.0\n        mtx[i][1:, 1:] = intensity[i]\n    return mtx\n", "import_code": ["import numpy as np\n"]}
{"id": "eaa62d9f-5da5-3247-8936-4c9ab8b16934_0", "content": "def scrabble_score(word):\n    total = 0\n    for letter in word:\n        total = total + letter_score[letter.upper()]\n    return total\n", "import_code": []}
{"id": "cdbac127-af7d-394a-90fd-ebbdae49a2ec_0", "content": "def get_rnd_strng(size=10):\n    \"\"\"get_rnd_strng() generates a random string for creating temp files.\n\n    Args:\n        size (int) - Size of the string. (Default 10).\n    \"\"\"\n    return ''.join(random.choice(string.ascii_lowercase) for i in range(size))\n", "import_code": ["import random\n", "import string\n"]}
{"id": "cdbac127-af7d-394a-90fd-ebbdae49a2ec_1", "content": "def add_ext(name, ext, seq=-1):\n    \"\"\"add_ext constructs file names with a name, sequence number, and \n    extension.\n\n    Args:\n        name (string) - the filename\n        ext (string) - the file extension\n        seq (int) - the number in a sequence with other files \n                    (Default -1) means that it is a standalone \n                    file\n    \"\"\"\n    if '.' not in ext[0]:\n        ext = '.' + ext\n    if seq != -1:\n        ext = str(seq) + ext\n    return name + ext\n", "import_code": []}
{"id": "cdbac127-af7d-394a-90fd-ebbdae49a2ec_2", "content": "def write_block(data, path, name=None):\n    \"\"\"Writes a dictionary of serializable data to a file.\n\n    Args:  \n        data (dict) - dictionary\n        path (string)- a temp directory path to write to\n        name (string) - optional write to a specific file\n    \"\"\"\n    if name == None:\n        r_name = get_rnd_strng()\n        file_name = os.path.join(path, r_name)\n    else:\n        file_name = name\n    f = open(file_name, 'wb')\n    pickle.dump(data, f)\n    f.close()\n    return file_name\n", "import_code": ["import pickle\n", "import os\n"]}
{"id": "cdbac127-af7d-394a-90fd-ebbdae49a2ec_3", "content": "def read_block(file):\n    \"\"\"Reads a block from a specified data path.\n\n    Args: \n        file (string) - a file name of a block to read\n    \"\"\"\n    f = open(file, 'rb')\n    return pickle.load(f)\n", "import_code": ["import pickle\n"]}
{"id": "cdbac127-af7d-394a-90fd-ebbdae49a2ec_4", "content": "def stack_block(files, sfile, compression=RAW):\n    \"\"\"Given a list of files builds a contiguous block\n\n    Args:  \n        files- a collection of files\n        sfile- stacked file name to create\n        compression- a meta compression scheme over the blocks\n    \"\"\"\n    if len(files) == 1 and compression == RAW:\n        os.rename(files[0], sfile)\n    else:\n        tf = tarfile.open(sfile, mode=compression)\n        for file in files:\n            tf.add(file, arcname=os.path.basename(file))\n        tf.close()\n    return sfile\n", "import_code": ["import tarfile\n", "import os\n"]}
{"id": "cdbac127-af7d-394a-90fd-ebbdae49a2ec_5", "content": "def unstack_block(sfile, path, compression_hint=RAW):\n    \"\"\"Given a stacked file returns pointers to all of the constituent\n       extracted and decompressed files.\n\n       Args: \n       \tsfile- stacked file name to extract\n       \tpath- directory to extract all to\n    \"\"\"\n    if compression_hint == RAW:\n        return [sfile]\n    else:\n        tf = tarfile.open(sfile)\n        tf.extractall(path)\n        return [os.path.join(path, n) for n in tf.getnames()]\n", "import_code": ["import tarfile\n", "import os\n"]}
{"id": "cdbac127-af7d-394a-90fd-ebbdae49a2ec_6", "content": "def ncpy_stack_block(files, sfile):\n    os.mkdir(sfile)\n    for file in files:\n        os.rename(file, os.path.join(sfile, os.path.basename(file)))\n    return sfile\n", "import_code": ["import os\n"]}
{"id": "cdbac127-af7d-394a-90fd-ebbdae49a2ec_7", "content": "def ncpy_unstack_block(sfile):\n    return [os.path.join(sfile, n) for n in os.listdir(sfile)]\n", "import_code": ["import os\n"]}
{"id": "cdbac127-af7d-394a-90fd-ebbdae49a2ec_8", "content": "def build_fmt_file(header_data, video, path, output, header_cmp, meta_cmp,\n    header_name=None):\n    \"\"\"Helper method to write the archive file with the headers and the data.\n\n    Args:\n        header_data - header data in dict form that will describe what is \n                      in a video clip.\n        video - video file on disk\n        path  - scratch or temp space to store the data\n        output - output file name\n        header_cmp - how to compress the header\n        meta_cmp - how to compress the whole archive (header + video)\n        header_name - how to name the header\n    \"\"\"\n    file = write_block(header_data, path)\n    if header_name == None:\n        header_name = get_rnd_strng()\n    header = stack_block([file], add_ext(header_name, '.head'), compression\n        =header_cmp)\n    ncpy_stack_block([video, header], output)\n    return output\n", "import_code": []}
{"id": "d122bf2b-6016-3cd8-964f-c31604bcaeca_0", "content": "def read_halo_catalog(catalog_loc):\n    \"\"\"Read in a halo catalog produced by gbpCode.\n\n    *Args*:\n        catalog_loc : str\n            Full path to input catalog file or directory\n\n    *Returns*:\n        halo : array\n            The catalog of halos\n    \"\"\"\n    if type(catalog_loc) is str:\n        catalog_loc = [catalog_loc]\n    if path.isdir(catalog_loc[0]):\n        dirname = catalog_loc[0]\n        catalog_loc = listdir(catalog_loc[0])\n        file_num = [int(f.rsplit('.')[-1]) for f in catalog_loc]\n        sort_index = np.argsort(file_num)\n        catalog_loc = [catalog_loc[i] for i in sort_index]\n        catalog_loc = [path.join(dirname, f) for f in catalog_loc]\n    n_halos = np.fromfile(catalog_loc[0], catalog_header_dtype, 1)[0][\n        'N_halos_total']\n    halo = np.empty(n_halos, dtype=catalog_halo_dtype)\n    print('Reading in {:d} halos...'.format(n_halos))\n    n_halos = 0\n    for f in tqdm(catalog_loc):\n        with open(f, 'rb') as fd:\n            n_halos_file = np.fromfile(fd, catalog_header_dtype, 1)[0][\n                'N_halos_file']\n            halo[n_halos:n_halos + n_halos_file] = np.fromfile(fd,\n                catalog_halo_dtype, n_halos_file)\n        n_halos += n_halos_file\n    return halo[list(catalog_halo_dtype.names[:-1])]\n", "import_code": ["from os import listdir\n", "from os import path\n"]}
{"id": "d861e9b2-cfe4-3c5b-a846-fff19126a780_0", "content": "def updateBoard(guess, actualPhraseLetters, secretPhraseLetters):\n    guess = guess.lower()\n    correctGuess = False\n    for i in range(0, len(secretPhraseLetters)):\n        currWordList = secretPhraseLetters[i]\n        for j in range(0, len(currWordList)):\n            currLetter = currWordList[j]\n            if currLetter == guess:\n                actualPhraseLetters[i][j] = guess\n                correctGuess = True\n    return actualPhraseLetters, correctGuess\n", "import_code": []}
{"id": "d861e9b2-cfe4-3c5b-a846-fff19126a780_1", "content": "def calculateProbabilityMap(wordNum, actualPhraseLetters, alreadyGuessed):\n    currWord = actualPhraseLetters[wordNum]\n    dic = [dictionary[i] for i in range(0, len(dictionary)) if len(\n        dictionary[i]) == len(currWord)]\n    for i in range(0, len(currWord)):\n        currLetter = currWord[i]\n        if currLetter != WILD_CHARACTER:\n            dic = [dic[j] for j in range(0, len(dic)) if dic[j][i] ==\n                currLetter]\n    occurences = [(0) for i in range(0, 26)]\n    for entry in dic:\n        for i in range(0, len(entry)):\n            if not entry[i] in currWord and not entry[i] in alreadyGuessed:\n                occurences[ord(entry[i]) - ord('a')] += 1\n    return [(float(occurence) / sum(occurences)) for occurence in occurences]\n", "import_code": []}
{"id": "d861e9b2-cfe4-3c5b-a846-fff19126a780_2", "content": "def stringFromCharArray(arr):\n    return ' '.join([''.join(word) for word in arr])\n", "import_code": []}
{"id": "0a690d4c-e325-30b8-8f76-8b4c52826e8e_0", "content": "def IsHeaderFile(filename):\n    extension = os.path.splitext(filename)[1]\n    return extension in ['.h', '.hxx', '.hpp', '.hh']\n", "import_code": ["import os\n"]}
{"id": "0a690d4c-e325-30b8-8f76-8b4c52826e8e_1", "content": "def FindCorrespondingSourceFile(filename):\n    if IsHeaderFile(filename):\n        basename = os.path.splitext(filename)[0]\n        for extension in SOURCE_EXTENSIONS:\n            replacement_file = basename + extension\n            if os.path.exists(replacement_file):\n                return replacement_file\n        return filename\n", "import_code": ["import os\n"]}
{"id": "1f16ae16-7e38-33a9-8b66-83ab6f8635f6_0", "content": "def get_assignments(my_seed):\n    data = pd.read_csv('world_cup_sweepstake.csv')\n    people = data.names.values\n    good_teams = data.top_teams.values\n    bad_teams = data.bottom_teams.values\n    np.random.seed(my_seed)\n    np.random.shuffle(people)\n    np.random.shuffle(bad_teams)\n    np.random.shuffle(good_teams)\n    assigned_bad = zip(people, bad_teams)\n    assigned_good = zip(people, good_teams)\n    return assigned_bad, assigned_good\n", "import_code": ["import pandas as pd\n", "import numpy as np\n"]}
{"id": "908c47f0-265d-32b6-90f0-56e2f7bf2a94_0", "content": "def return_top(group):\n    return group[:1]\n", "import_code": []}
{"id": "42fdf3c1-806d-37f8-b421-a378d8407a4b_0", "content": "def read_json(path: str) ->Union[list, dict]:\n    \"\"\" Read a json file \"\"\"\n    with open(path, 'r') as infile:\n        x = json.load(infile)\n    return x\n", "import_code": ["from typing import Union\n", "import json\n"]}
{"id": "42fdf3c1-806d-37f8-b421-a378d8407a4b_2", "content": "def to_DataFrame(x: Union[list, dict]) ->pd.DataFrame:\n    \"\"\" Convert a list (with row-wise dicts) or dict to a pandas DataFrame \"\"\"\n    if isinstance(x, pd.DataFrame):\n        df = x\n    elif isinstance(x, dict):\n        df = pd.DataFrame(x, index=[0])\n    elif isinstance(x, list):\n        df = pd.DataFrame(x)\n    else:\n        raise NotImplementedError('Input type {} not supported'.format(type(x))\n            )\n    return df\n", "import_code": ["import pandas as pd\n", "from typing import Union\n"]}
{"id": "42fdf3c1-806d-37f8-b421-a378d8407a4b_3", "content": "def to_dict(x: pd.DataFrame, drop_na: bool=True) ->Union[list, dict]:\n    \"\"\" Convert a pandas DataFrame to a row-wise list of dictionaries \"\"\"\n    if isinstance(x, dict):\n        pass\n    if isinstance(x, pd.DataFrame):\n        x = x.to_dict(orient='records')\n    if drop_na:\n        x = [{k: v for k, v in x.items() if not pd.isna(v)} for x in x]\n    return x\n", "import_code": ["import pandas as pd\n", "from typing import Union\n"]}
